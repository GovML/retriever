,title,title_abs,publish_year,keyword,emb1,emb2,year_cat,year_enc,color_code
0,Earthquake networks based on similar activity patterns,"Earthquake networks based on similar activity patterns  Earthquakes are a complex spatiotemporal phenomenon, the underlying mechanism for which is still not fully understood despite decades of research and analysis. We propose and develop a network approach to earthquake events. In this network, a node represents a spatial location while a link between two nodes represents similar activity patterns in the two different locations. The strength of a link is proportional to the strength of the cross-correlation in activities of two nodes joined by the link. We apply our network approach to a Japanese earthquake catalog spanning the 14-year period 1985-1998. We find strong links representing large correlations between patterns in locations separated by more than 1000 km, corroborating prior observations that earthquake interactions have no characteristic length scale. We find network characteristics not attributable to chance alone, including a large number of network links, high node assortativity, and strong stability over time. ",2011,earthquake,3.4518623,6.6949534,2011-2015,4,#9100CF
1,A Prospect of Earthquake Prediction Research,"A Prospect of Earthquake Prediction Research  Earthquakes occur because of abrupt slips on faults due to accumulated stress in the Earth's crust. Because most of these faults and their mechanisms are not readily apparent, deterministic earthquake prediction is difficult. For effective prediction, complex conditions and uncertain elements must be considered, which necessitates stochastic prediction. In particular, a large amount of uncertainty lies in identifying whether abnormal phenomena are precursors to large earthquakes, as well as in assigning urgency to the earthquake. Any discovery of potentially useful information for earthquake prediction is incomplete unless quantitative modeling of risk is considered. Therefore, this manuscript describes the prospect of earthquake predictability research to realize practical operational forecasting in the near future. ",2013,earthquake,5.2595787,6.385388,2011-2015,4,#9100CF
2,Triggering of large earthquakes is driven by their twins,"Triggering of large earthquakes is driven by their twins  Fundamentally related to the UV divergence problem in Physics, conventional wisdom in seismology is that the smallest earthquakes, which are numerous and often go undetected, dominate the triggering of major earthquakes, making the prediction of the latter difficult if not inherently impossible. By developing a rigorous validation procedure, we show that, in fact, large earthquakes (above magnitude 6.3 in California) are preferentially triggered by large events. Because of the magnitude correlations intrinsic in the validated model, we further rationalize the existence of earthquake doublets. These findings have far-reaching implications for short-term and medium-term seismic risk assessment, as well as for the development of a deeper theory without UV cut-off that is locally self-similar. ",2021,earthquake,5.377548,6.293648,2020-,6,#5E0897
3,"Tectonic ""short circuit"" of sub-horizontal fluid-saturated bodies as a  possible mechanism of the earthquake","Tectonic ""short circuit"" of sub-horizontal fluid-saturated bodies as a   possible mechanism of the earthquake  An alternative earthquake mechanism is proposed. The traditional stress mechanism of fracture formation assigned a support role. As a proximate cause of the earthquake the destruction of the roofs of sub-horizontal fluid-saturated bodies (SHFB) is considered. This collapse may occur due to redistribution of fluid pressure within the system of SHFB connected by cracks (tectonic or other nature). It can cause both shifts of rock blocks contributing to seismic shocks and various effects characteristic of foreshocks and aftershocks. ",2013,earthquake,6.2041283,5.831665,2011-2015,4,#9100CF
4,Physics Models of Earthquake,"Physics Models of Earthquake  Since long back, scientists have been putting enormous effort to understand earthquake dynamics -the goal is to develop a successful prediction scheme which can provide reliable alarm that an earthquake is imminent. Model studies sometimes help to understand in some extend the basic dynamics of the real systems and therefore is an important part of earthquake research. In this report, we review several physics models which capture some essential features of earthquake phenomenon and also suggest methods to predict catastrophic events being within the range of model parameters. ",2007,earthquake,4.9923854,6.1713066,2006-2010,3,#FEF65C
5,Earthquake Impact Analysis Based on Text Mining and Social Media  Analytics,"Earthquake Impact Analysis Based on Text Mining and Social Media   Analytics  Earthquakes have a deep impact on wide areas, and emergency rescue operations may benefit from social media information about the scope and extent of the disaster. Therefore, this work presents a text miningbased approach to collect and analyze social media data for early earthquake impact analysis. First, disasterrelated microblogs are collected from the Sina microblog based on crawler technology. Then, after data cleaning a series of analyses are conducted including (1) the hot words analysis, (2) the trend of the number of microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and rule-based text classification for earthquake impact analysis. Finally, two recent earthquakes with the same magnitude and focal depth in China are analyzed to compare their impacts. The results show that the public opinion trend analysis and the trend of public opinion sentiment can estimate the earthquake's social impact at an early stage, which will be helpful to decision-making and rescue management. ",2022,earthquake,3.1900325,6.8911777,2020-,6,#5E0897
6,Spatiotemporal Correlations of Earthquakes,"Spatiotemporal Correlations of Earthquakes  Statistical properties of earthquakes are studied both by the analysis of real earthquake catalog of Japan and by numerical computer simulations of the spring-block model in both one and two dimensions. Particular attention is paid to the spatiotemporal correlations of earthquakes, e.g., the recurrence-time distribution or the time evolution before and after the mainshock of seismic distribution functions, including the magnitude distribution and the spatial seismic distribution. Certain eminent features of the spatiotemporal correlations, e.g., foreshocks, aftershocks, swarms and doughnut-like seismic pattern, are discussed. ",2006,earthquake,5.3540034,6.3527,2006-2010,3,#FEF65C
7,Anomalous diffusion of epicentres,Anomalous diffusion of epicentres  The classification of earthquakes in main shocks and aftershocks by a method recently proposed by M. Baiesi and M. Paczuski allows to the generation of a complex network composed of clusters that group the most correlated events. The spatial distribution of epicentres inside these structures corresponding to the catalogue of earthquakes in the eastern region of Cuba shows anomalous anti-diffusive behaviour evidencing the attractive nature of the main shock and the possible description in terms of fractional kinetics. ,2007,earthquake,5.3281693,6.273119,2006-2010,3,#FEF65C
8,The Olami-Feder-Christensen earthquake model in one dimension,"The Olami-Feder-Christensen earthquake model in one dimension  We study the earthquake model by Olami, Feder and Christensen in one dimension. While the size distribution of earthquakes resembles a power law for small system sizes, it splits for larger system sizes into two parts, one comprising small avalanches and showing a size independent cutoff, and the other comprising avalanches of the order of the system size. We identify four different types of attractors of the dynamics of the system which already exist for very small systems. For larger system sizes, these attractors contain large synchronized regions. ",2004,earthquake,5.2625318,5.763434,2001-2005,2,#FFE135
9,Earthquake forecasting: Statistics and Information,"Earthquake forecasting: Statistics and Information  We present an axiomatic approach to earthquake forecasting in terms of multi-component random fields on a lattice. This approach provides a method for constructing point estimates and confidence intervals for conditional probabilities of strong earthquakes under conditions on the levels of precursors. Also, it provides an approach for setting multilevel alarm system and hypothesis testing for binary alarms. We use a method of comparison for different earthquake forecasts in terms of the increase of Shannon information. 'Forecasting' and 'prediction' of earthquakes are equivalent in this approach. ",2013,earthquake,5.203241,6.41517,2011-2015,4,#9100CF
10,Forecasting in multivariate irregularly sampled time series with missing  values,"Forecasting in multivariate irregularly sampled time series with missing   values  Sparse and irregularly sampled multivariate time series are common in clinical, climate, financial and many other domains. Most recent approaches focus on classification, regression or forecasting tasks on such data. In forecasting, it is necessary to not only forecast the right value but also to forecast when that value will occur in the irregular time series. In this work, we present an approach to forecast not only the values but also the time at which they are expected to occur. ",2020,forecasting,3.9039388,6.511547,2020-,6,#5E0897
11,Forecasting with Feedback,"Forecasting with Feedback  Systematically biased forecasts are typically interpreted as evidence of forecasters' irrationality and/or asymmetric loss. In this paper we propose an alternative explanation: when forecasts inform economic policy decisions, and the resulting actions affect the realization of the forecast target itself, forecasts may be optimally biased even under quadratic loss. The result arises in environments in which the forecaster is uncertain about the decision maker's reaction to the forecast, which is presumably the case in most applications. We illustrate the empirical relevance of our theory by reviewing some stylized properties of Green Book inflation forecasts and relating them to the predictions from our model. Our results point out that the presence of policy feedback poses a challenge to traditional tests of forecast rationality. ",2023,forecasting,3.7884583,6.479365,2020-,6,#5E0897
12,A Strong Baseline for Weekly Time Series Forecasting,"A Strong Baseline for Weekly Time Series Forecasting  Many businesses and industries require accurate forecasts for weekly time series nowadays. The forecasting literature however does not currently provide easy-to-use, automatic, reproducible and accurate approaches dedicated to this task. We propose a forecasting method that can be used as a strong baseline in this domain, leveraging state-of-the-art forecasting techniques, forecast combination, and global modelling. Our approach uses four base forecasting models specifically suitable for forecasting weekly data: a global Recurrent Neural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS), and Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally combined using a lasso regression stacking approach. We evaluate the performance of our method against a set of state-of-the-art weekly forecasting models on six datasets. Across four evaluation metrics, we show that our method consistently outperforms the benchmark methods by a considerable margin with statistical significance. In particular, our model can produce the most accurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset. ",2020,forecasting,3.6693296,6.7600384,2020-,6,#5E0897
13,Forecasting Framework for Open Access Time Series in Energy,"Forecasting Framework for Open Access Time Series in Energy  In this paper we propose a framework for automated forecasting of energy-related time series using open access data from European Network of Transmission System Operators for Electricity (ENTSO-E). The framework provides forecasts for various European countries using publicly available historical data only. Our solution was benchmarked using the actual load data and the country provided estimates (where available). We conclude that the proposed system can produce timely forecasts with comparable prediction accuracy in a number of cases. We also investigate the probabilistic case of forecasting - that is, providing a probability distribution rather than a simple point forecast - and incorporate it into a web based API that provides quick and easy access to reliable forecasts. ",2016,forecasting,3.7625835,6.813959,2016-2020,5,#7402B1
14,Forecasting with a Panel Tobit Model,"Forecasting with a Panel Tobit Model  We use a dynamic panel Tobit model with heteroskedasticity to generate forecasts for a large cross-section of short time series of censored observations. Our fully Bayesian approach allows us to flexibly estimate the cross-sectional distribution of heterogeneous coefficients and then implicitly use this distribution as prior to construct Bayes forecasts for the individual time series. In addition to density forecasts, we construct set forecasts that explicitly target the average coverage probability for the cross-section. We present a novel application in which we forecast bank-level loan charge-off rates for small banks. ",2021,forecasting,4.1229157,6.2391777,2020-,6,#5E0897
15,Improving forecasting by subsampling seasonal time series,"Improving forecasting by subsampling seasonal time series  Time series forecasting plays an increasingly important role in modern business decisions. In today's data-rich environment, people often aim to choose the optimal forecasting model for their data. However, identifying the optimal model requires professional knowledge and experience, making accurate forecasting a challenging task. To mitigate the importance of model selection, we propose a simple and reliable algorithm to improve the forecasting performance. Specifically, we construct multiple time series with different sub-seasons from the original time series. These derived series highlight different sub-seasonal patterns of the original series, making it possible for the forecasting methods to capture diverse patterns and components of the data. Subsequently, we produce forecasts for these multiple series separately with classical statistical models (ETS or ARIMA). Finally, the forecasts are combined. We evaluate our approach on widely-used forecasting competition data sets (M1, M3, and M4) in terms of both point forecasts and prediction intervals. We observe performance improvements compared with the benchmarks. Our approach is particularly suitable and robust for the data with higher frequency. To demonstrate the practical value of our proposition, we showcase the performance improvements from our approach on hourly load data that exhibit multiple seasonal patterns. ",2021,forecasting,3.5719602,6.7192984,2020-,6,#5E0897
16,Hurricane Forecasting: A Novel Multimodal Machine Learning Framework,"Hurricane Forecasting: A Novel Multimodal Machine Learning Framework  This paper describes a novel machine learning (ML) framework for tropical cyclone intensity and track forecasting, combining multiple ML techniques and utilizing diverse data sources. Our multimodal framework, called Hurricast, efficiently combines spatial-temporal data with statistical data by extracting features with deep-learning encoder-decoder architectures and predicting with gradient-boosted trees. We evaluate our models in the North Atlantic and Eastern Pacific basins on 2016-2019 for 24-hour lead time track and intensity forecasts and show they achieve comparable mean absolute error and skill to current operational forecast models while computing in seconds. Furthermore, the inclusion of Hurricast into an operational forecast consensus model could improve over the National Hurricane Center's official forecast, thus highlighting the complementary properties with existing approaches. In summary, our work demonstrates that utilizing machine learning techniques to combine different data sources can lead to new opportunities in tropical cyclone forecasting. ",2020,forecasting,3.7960331,6.7082553,2020-,6,#5E0897
17,Visual Time Series Forecasting: An Image-driven Approach,"Visual Time Series Forecasting: An Image-driven Approach  In this work, we address time-series forecasting as a computer vision task. We capture input data as an image and train a model to produce the subsequent image. This approach results in predicting distributions as opposed to pointwise values. To assess the robustness and quality of our approach, we examine various datasets and multiple evaluation metrics. Our experiments show that our forecasting tool is effective for cyclic data but somewhat less for irregular data such as stock prices. Importantly, when using image-based evaluation metrics, we find our method to outperform various baselines, including ARIMA, and a numerical variation of our deep learning approach. ",2021,forecasting,3.696875,6.7006764,2020-,6,#5E0897
18,"Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of  Anchoring ""Bias""","Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of   Anchoring ""Bias""  In an online prediction market, forecasters who could not see the current state of the market until they made their own separate estimates moved their estimates closer to the market forecast when the current state of the market became known. Their first edits to the market forecast were very similar to the first edits of forecasters who could always see the current state of the market, and forecasters in both conditions had similar accuracy. These results suggest that our more elaborate forecast elicitation method might not improve forecasts and that any anchoring on the state of the market does not constitute an error in judgment. ",2014,forecasting,3.9902723,6.421084,2011-2015,4,#9100CF
19,Large Scale Automated Forecasting for Monitoring Network Safety and  Security,"Large Scale Automated Forecasting for Monitoring Network Safety and   Security  Real time large scale streaming data pose major challenges to forecasting, in particular defying the presence of human experts to perform the corresponding analysis. We present here a class of models and methods used to develop an automated, scalable and versatile system for large scale forecasting oriented towards safety and security monitoring. Our system provides short and long term forecasts and uses them to detect safety and security issues in relation with multiple internet connected devices well in advance they might take place. ",2018,forecasting,3.720016,6.9323564,2016-2020,5,#7402B1
20,Volcano transition in populations of phase oscillators with random  nonreciprocal interactions,"Volcano transition in populations of phase oscillators with random   nonreciprocal interactions  Populations of heterogeneous phase oscillators with frustrated random interactions exhibit a quasi-glassy state in which the distribution of local fields is volcano-shaped. In a recent work [Phys. Rev. Lett. 120, 264102 (2018)] the volcano transition was replicated in a solvable model using a low-rank, random coupling matrix $\mathbf M$. We extend here that model including tunable nonreciprocal interactions, i.e. ${\mathbf M}^T\ne \mathbf M$. More specifically, we formulate two different solvable models. In both of them the volcano transition persists if matrix elements $M_{jk}$ and $M_{kj}$ are enough correlated. Our numerical simulations fully confirm the analytical results. To put our work in a wider context, we also investigate numerically the volcano transition in the analogous model with a full-rank random coupling matrix. ",2023,volcano,6.2246995,5.050803,2020-,6,#5E0897
21,"A Simple Volcano Potential with an Analytic, Zero-Energy, Ground State","A Simple Volcano Potential with an Analytic, Zero-Energy, Ground State  We describe a simple volcano potential, which is supersymmetric and has an analytic, zero-energy, ground state. (The KK modes are also analytic.) It is an interior harmonic oscillator potential properly matched to an exterior angular momentum-like tail. Special cases are given to elucidate the physics, which may be intuitively useful in studies of higher-dimensional gravity. ",2000,volcano,7.454048,2.6597364,1996-2000,1,#B2AB10
22,Predictability of Volcano Eruption: lessons from a basaltic effusive  volcano,"Predictability of Volcano Eruption: lessons from a basaltic effusive   volcano  Volcano eruption forecast remains a challenging and controversial problem despite the fact that data from volcano monitoring significantly increased in quantity and quality during the last decades.This study uses pattern recognition techniques to quantify the predictability of the 15 Piton de la Fournaise (PdlF) eruptions in the 1988-2001 period using increase of the daily seismicity rate as a precursor. Lead time of this prediction is a few days to weeks. Using the daily seismicity rate, we formulate a simple prediction rule, use it for retrospective prediction of the 15 eruptions,and test the prediction quality with error diagrams. The best prediction performance corresponds to averaging the daily seismicity rate over 5 days and issuing a prediction alarm for 5 days. 65% of the eruptions are predicted for an alarm duration less than 20% of the time considered. Even though this result is concomitant of a large number of false alarms, it is obtained with a crude counting of daily events that are available from most volcano observatories ",2003,volcano,4.8440638,6.661766,2001-2005,2,#FFE135
23,Classification of volcanic ash particles using a convolutional neural  network and probability,"Classification of volcanic ash particles using a convolutional neural   network and probability  Analyses of volcanic ash are typically performed either by qualitatively classifying ash particles by eye or by quantitatively parameterizing its shape and texture. While complex shapes can be classified through qualitative analyses, the results are subjective due to the difficulty of categorizing complex shapes into a single class. Although quantitative analyses are objective, selection of shape parameters is required. Here, we applied a convolutional neural network (CNN) for the classification of volcanic ash. First, we defined four basal particle shapes (blocky, vesicular, elongated, rounded) generated by different eruption mechanisms (e.g., brittle fragmentation), and then trained the CNN using particles composed of only one basal shape. The CNN could recognize the basal shapes with over 90% accuracy. Using the trained network, we classified ash particles composed of multiple basal shapes based on the output of the network, which can be interpreted as a mixing ratio of the four basal shapes. Clustering of samples by the averaged probabilities and the intensity is consistent with the eruption type. The mixing ratio output by the CNN can be used to quantitatively classify complex shapes in nature without categorizing forcibly and without the need for shape parameters, which may lead to a new taxonomy. ",2018,volcano,4.0856524,8.25117,2016-2020,5,#7402B1
24,About the Mechanism of Volcanic Eruptions,"About the Mechanism of Volcanic Eruptions  A new approach to the volcanic eruption theory is proposed. It is based on a simple physical mechanism of the imbalance in the system ""magma-crust-fluid"". This mechanism helps to explain from unified positions the different types of volcanic eruptions. A criterion of imbalance and magma eruption is derived. Stratovolcano and caldera formation is analyzed. High explosive eruptions of the silicic magma is discussed ",2012,volcano,5.614642,5.311607,2011-2015,4,#9100CF
25,"Methods for analyzing surface texture effects of volcanoes with Plinian  and subplinian eruptions types: Cases of study Lascar (23 S) and Chaiten (42  S), Chile","Methods for analyzing surface texture effects of volcanoes with Plinian   and subplinian eruptions types: Cases of study Lascar (23 S) and Chaiten (42   S), Chile  This paper presents a new methodology that provides the analysis of surface texture changes in areas adjacent to the volcano and its impact product of volcanic activity. To do this, algorithms from digital image processing such as the co-occurrence matrix and the wavelet transform are used. These methods are working on images taken by the Landsat satellite platform sensor 5 TM and Landsat 7 ETM + sensor, and implemented with the purpose of evaluating superficial changes that can warn of surface movements of the volcano. The results were evaluated by similarity metrics for grayscale images, and validated in two different scenarios that have the same type of eruption, but differ, essentially, in climate and vegetation. Finally, the proposed algorithm is presented, setting the parameters and constraints for implementation and use. ",2016,volcano,4.693839,7.5651984,2016-2020,5,#7402B1
26,Detecting Volcano Deformation in InSAR using Deep learning,"Detecting Volcano Deformation in InSAR using Deep learning  Globally 800 million people live within 100 km of a volcano and currently 1500 volcanoes are considered active, but half of these have no ground-based monitoring. Alternatively, satellite radar (InSAR) can be employed to observe volcanic ground deformation, which has shown a significant statistical link to eruptions. Modern satellites provide large coverage with high resolution signals, leading to huge amounts of data. The explosion in data has brought major challenges associated with timely dissemination of information and distinguishing volcano deformation patterns from noise, which currently relies on manual inspection. Moreover, volcano observatories still lack expertise to exploit satellite datasets, particularly in developing countries. This paper presents a novel approach to detect volcanic ground deformation automatically from wrapped-phase InSAR images. Convolutional neural networks (CNN) are employed to detect unusual patterns within the radar data. ",2018,volcano,3.68431,8.030829,2016-2020,5,#7402B1
27,Toward Forecasting Volcanic Eruptions using Seismic Noise,"Toward Forecasting Volcanic Eruptions using Seismic Noise  During inter-eruption periods, magma pressurization yields subtle changes of the elastic properties of volcanic edifices. We use the reproducibility properties of the ambient seismic noise recorded on the Piton de la Fournaise volcano to measure relative seismic velocity variations of less than 0.1 % with a temporal resolution of one day. Our results show that five studied volcanic eruptions were preceded by clearly detectable seismic velocity decreases within the zone of magma injection. These precursors reflect the edifice dilatation induced by magma pressurization and can be useful indicators to improve the forecasting of volcanic eruptions. ",2007,volcano,5.334355,6.573278,2006-2010,3,#FEF65C
28,Volcano transition in a solvable model of oscillator glass,"Volcano transition in a solvable model of oscillator glass  In 1992 a puzzling transition was discovered in simulations of randomly coupled limit-cycle oscillators. This so-called volcano transition has resisted analysis ever since. It was originally conjectured to mark the emergence of an oscillator glass, but here we show it need not. We introduce and solve a simpler model with a qualitatively identical volcano transition and find, unexpectedly, that its supercritical state is not glassy. We discuss the implications for the original model and suggest experimental systems in which a volcano transition and oscillator glass may appear. ",2017,volcano,6.8827047,5.0139313,2016-2020,5,#7402B1
29,From periodically driven double wells to volcano potentials: Quantum  dynamics,"From periodically driven double wells to volcano potentials: Quantum   dynamics  We consider the dynamics of a particle confined in a double well potential which is subjected to a periodic drive. In the case of deep and well separated wells, we find that by adjusting the parameters of the drive we can generate, to a very good approximation, a volcano potential. The quantum dynamics in this volcano potential is studied by a variation of what can be called a generalized Ehrenfest's theorem. We find that the coupling of the mean position and the width of the wave packet in this dynamics causes the particle to escape from the central well in accordance with the fact that the volcano potential only supports resonance states. ",2018,volcano,7.047133,4.4308605,2016-2020,5,#7402B1
30,Peranso - Light Curve and Period Analysis Software,"Peranso - Light Curve and Period Analysis Software  A time series is a sample of observations of well-defined data points obtained through repeated measurements over a certain time range. The analysis of such data samples has become increasingly important not only in natural science but also in many other fields of research. Peranso offers a complete set of powerful light curve and period analysis functions to work with large, astronomical data sets. Substantial attention has been given to ease-of-use and data accuracy, making it one of the most productive time series analysis software available. In this paper, we give an introduction to Peranso and its functionality. ",2016,time series,3.681695,6.906394,2016-2020,5,#7402B1
31,Early Classification of Time Series is Meaningful,"Early Classification of Time Series is Meaningful  Many approaches have been proposed for early classification of time series in light of its significance in a wide range of applications including healthcare, transportation and finance. However, recently a preprint saved on Arxiv claim that all research done for almost 20 years now on the Early Classification of Time Series is useless, or, at the very least, ill-oriented because severely lacking a strong ground. In this paper, we answer in detail the main issues and misunderstandings raised by the authors of the preprint, and propose directions to further expand the fields of application of early classification of time series. ",2021,time series,3.5879204,7.380817,2020-,6,#5E0897
32,Time series classification for varying length series,"Time series classification for varying length series  Research into time series classification has tended to focus on the case of series of uniform length. However, it is common for real-world time series data to have unequal lengths. Differing time series lengths may arise from a number of fundamentally different mechanisms. In this work, we identify and evaluate two classes of such mechanisms -- variations in sampling rate relative to the relevant signal and variations between the start and end points of one time series relative to one another. We investigate how time series generated by each of these classes of mechanism are best addressed for time series classification. We perform extensive experiments and provide practical recommendations on how variations in length should be handled in time series classification. ",2019,time series,3.560385,7.210984,2016-2020,5,#7402B1
33,Time-warping invariants of multidimensional time series,"Time-warping invariants of multidimensional time series  In data science, one is often confronted with a time series representing measurements of some quantity of interest. Usually, as a first step, features of the time series need to be extracted. These are numerical quantities that aim to succinctly describe the data and to dampen the influence of noise. In some applications, these features are also required to satisfy some invariance properties. In this paper, we concentrate on time-warping invariants. We show that these correspond to a certain family of iterated sums of the increments of the time series, known as quasisymmetric functions in the mathematics literature. We present these invariant features in an algebraic framework, and we develop some of their basic properties. ",2019,time series,4.9350815,4.694975,2016-2020,5,#7402B1
34,GluonTS: Probabilistic Time Series Models in Python,"GluonTS: Probabilistic Time Series Models in Python  We introduce Gluon Time Series (GluonTS, available at https://gluon-ts.mxnet.io), a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy. ",2019,time series,3.5441816,7.149748,2016-2020,5,#7402B1
35,Two-dimensional cellular automata and the analysis of correlated time  series,"Two-dimensional cellular automata and the analysis of correlated time   series  Correlated time series are time series that, by virtue of the underlying process to which they refer, are expected to influence each other strongly. We introduce a novel approach to handle such time series, one that models their interaction as a two-dimensional cellular automaton and therefore allows them to be treated as a single entity. We apply our approach to the problems of filling gaps and predicting values in rainfall time series. Computational results show that the new approach compares favorably to Kalman smoothing and filtering. ",2005,time series,2.822063,4.638055,2006-2010,3,#FEF65C
36,Chaotic time series in financial processes consisting of savings with  piecewise constant monthly contributions,Chaotic time series in financial processes consisting of savings with   piecewise constant monthly contributions  We investigate the time series generated by an elementary and deterministic financial process that consists in making monthly contributions to a savings account subjected to the devaluation by a monthly negative real interest rate. The monthly contribution is a piecewise constant function of the account balance. We show that a dichotomy holds for such a financial time series: either the financial time series are asymptotic to finitely many periodic sequences or the financial time series have an uncountable (Cantor) set of {\omega}-limit points. We also provide explicit parameters for which the financial process is chaotic in the sense that the financial time series have sensitive dependence on initial conditions at points of a Cantor attractor. ,2022,time series,5.720203,4.3277907,2020-,6,#5E0897
37,GRATIS: GeneRAting TIme Series with diverse and controllable  characteristics,"GRATIS: GeneRAting TIme Series with diverse and controllable   characteristics  The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application. ",2019,time series,3.7483523,6.6882596,2016-2020,5,#7402B1
38,Forecasting with Deep Learning,"Forecasting with Deep Learning  This paper presents a method for time series forecasting with deep learning and its assessment on two datasets. The method starts with data preparation, followed by model training and evaluation. The final step is a visual inspection. Experimental work demonstrates that a single time series can be used to train deep learning networks if time series in a dataset contain patterns that repeat even with a certain variation. However, for less structured time series such as stock market closing prices, the networks perform just like a baseline that repeats the last observed value. The implementation of the method as well as the experiments are open-source. ",2023,time series,3.5958073,6.939287,2020-,6,#5E0897
39,Feature-based time-series analysis,"Feature-based time-series analysis  This work presents an introduction to feature-based time-series analysis. The time series as a data type is first described, along with an overview of the interdisciplinary time-series analysis literature. I then summarize the range of feature-based representations for time series that have been developed to aid interpretable insights into time-series structure. Particular emphasis is given to emerging research that facilitates wide comparison of feature-based representations that allow us to understand the properties of a time-series dataset that make it suited to a particular feature-based representation or analysis algorithm. The future of time-series analysis is likely to embrace approaches that exploit machine learning methods to partially automate human learning to aid understanding of the complex dynamical patterns in the time series we measure from the world. ",2017,time series,3.6153743,7.1455564,2016-2020,5,#7402B1
40,Earthquakes in cities revisited,"Earthquakes in cities revisited  During the last twenty years, a number of publications of theoretical-numerical nature have appeared which come to the apparently-reassuring conclusion that seismic motion on the ground in cities is smaller than what this motion would be in the absence of the buildings (but for the same underground and seismic load). Other than the fact that this finding tells nothing about the motion within the buildings, it must be confronted with the overwhelming empirical evidence (e.g, earthquakes in Sendai (2011), Kathmandu (2015), Tainan City (2016), etc.) that shaking within buildings of a city is often large enough to damage or even destroy these structures. I show, on several examples, that theory can be reconciled with empirical evidence, and suggest that the crucial subject of seismic response in cities is in need of more thorough research. ",2016,earthquakes in remote places,5.279921,6.365577,2016-2020,5,#7402B1
41,Seismology with optical links: enabling a global network for submarine  earthquake monitoring,"Seismology with optical links: enabling a global network for submarine   earthquake monitoring  Earthquake monitoring across the globe is currently achieved with networks of seismic stations. The data from these networks have been instrumental in advancing our understanding of the Earth's interior structure and dynamic behaviour. However, almost all seismic stations are located on land and earthquakes of magnitude smaller than 4 at the bottom of the oceans remain largely undetected. Here we show that ordinary telecommunication optical fibre links can detect seismic events when combined with state-of-the-art frequency metrology techniques. We have detected earthquakes over terrestrial and submarine optical fibre links with length ranging from 75 to 535 km and a geographical distance from the earthquake's epicentre ranging from 25 to 18,500 km. In contrast to existing commercial reflectometry-based acoustic sensing methods used widely in the oil and gas industry, which are limited to only a few tens of kilometres, the technique presented here can be extended over thousands of kilometres, paving the way for detection of remote underwater earthquakes. By using the proposed technique on the existing extensive submarine optical fibre infrastructure, which already criss-crosses the seas and oceans, a global seismic network for real-time detection of underwater earthquakes could be implemented. The ability to detect off-shore earthquakes closer to the source could also enable a cost-effective solution for early detection of tsunamis. ",2017,earthquakes in remote places,5.358485,6.4701543,2016-2020,5,#7402B1
42,Global Earthquake Prediction Systems,"Global Earthquake Prediction Systems  Terra Seismic can predict most major earthquakes (M6.2 or greater) at least 2 - 5 months before they will strike. Global earthquake prediction is based on determinations of the stressed areas that will start to behave abnormally before major earthquakes. The size of the observed stressed areas roughly corresponds to estimates calculated from Dobrovolskys formula. To identify abnormalities and make predictions, Terra Seismic applies various methodologies, including satellite remote sensing methods and data from ground-based instruments. We currently process terabytes of information daily, and use more than 80 different multiparameter prediction systems. Alerts are issued if the abnormalities are confirmed by at least five different systems. We observed that geophysical patterns of earthquake development and stress accumulation are generally the same for all key seismic regions. Thus, the same earthquake prediction methodologies and systems can be applied successfully worldwide. Our technology has been used to retrospectively test data gathered since 1970 and it successfully detected about 90 percent of all significant quakes over the last 50 years. ",2020,earthquakes in remote places,5.3322253,6.468651,2020-,6,#5E0897
43,Locating earthquakes with a network of seismic stations via a deep  learning method,"Locating earthquakes with a network of seismic stations via a deep   learning method  The accurate and automated determination of earthquake locations is still a challenging endeavor. However, such information is critical for monitoring seismic activity and assessing potential hazards in real time. Recently, a convolutional neural network was applied to detect earthquakes from single-station waveforms and approximately map events across several large surface areas. In this study, we locate 194 earthquakes induced during oil and gas operations in Oklahoma, USA, within an error range of approximately 4.9 km on average to the epicenter and 1.0 km to the depth in catalogs with data from 30 network stations by applying the fully convolutional network. The network is trained by 1,013 historic events, and the output is a 3D volume of the event location probability in the Earth. The trained system requires approximately one hundredth of a second to locate an event without the need for any velocity model or human interference. ",2018,earthquakes in remote places,5.3018227,6.4881587,2016-2020,5,#7402B1
44,Earthquake networks based on similar activity patterns,"Earthquake networks based on similar activity patterns  Earthquakes are a complex spatiotemporal phenomenon, the underlying mechanism for which is still not fully understood despite decades of research and analysis. We propose and develop a network approach to earthquake events. In this network, a node represents a spatial location while a link between two nodes represents similar activity patterns in the two different locations. The strength of a link is proportional to the strength of the cross-correlation in activities of two nodes joined by the link. We apply our network approach to a Japanese earthquake catalog spanning the 14-year period 1985-1998. We find strong links representing large correlations between patterns in locations separated by more than 1000 km, corroborating prior observations that earthquake interactions have no characteristic length scale. We find network characteristics not attributable to chance alone, including a large number of network links, high node assortativity, and strong stability over time. ",2011,earthquakes in remote places,3.4518623,6.6949534,2011-2015,4,#9100CF
45,Building Damage Detection in Satellite Imagery Using Convolutional  Neural Networks,"Building Damage Detection in Satellite Imagery Using Convolutional   Neural Networks  In all types of disasters, from earthquakes to armed conflicts, aid workers need accurate and timely data such as damage to buildings and population displacement to mount an effective response. Remote sensing provides this data at an unprecedented scale, but extracting operationalizable information from satellite images is slow and labor-intensive. In this work, we use machine learning to automate the detection of building damage in satellite imagery. We compare the performance of four different convolutional neural network models in detecting damaged buildings in the 2010 Haiti earthquake. We also quantify how well the models will generalize to future disasters by training and testing models on different disaster events. ",2019,earthquakes in remote places,3.6114414,7.764975,2016-2020,5,#7402B1
46,Earthquake detection at the edge: IoT crowdsensing network,"Earthquake detection at the edge: IoT crowdsensing network  Earthquake Early Warning state of the art systems rely on a network of sensors connected to a fusion center in a client-server paradigm. Instead, we propose moving computation to the edge, with detector nodes that probe the environment and process information from nearby probes to detect earthquakes locally. Our approach tolerates multiple node faults and partial network disruption and keeps all data locally, enhancing privacy. This paper describes our proposal's rationale and explains its architecture. We then present an implementation using Raspberry, NodeMCU, and the Crowdquake machine learning model. ",2021,earthquakes in remote places,1.8483872,6.7381697,2020-,6,#5E0897
47,The shock-acoustic waves generated by earthquakes,"The shock-acoustic waves generated by earthquakes  We investigate the form and dynamics of shock-acoustic waves generated by earthquakes. We use the method for detecting and locating the sources of ionospheric impulsive disturbances, based on using data from a global network of receivers of the GPS navigation system and requiring no a priori information about the place and time of associated effects. The practical implementation of the method is illustrated by a case study of earthquake effects in Turkey (August 17, and November 12, 1999), in Southern Sumatera (June 4, 2000), and off the coast of Central America (January 13, 2001). It was found that in all instances the time period of the ionospheric response is 180-390 s, and the amplitude exceeds by a factor of two as a minimum the standard deviation of background fluctuations in total electron content in this range of periods under quiet and moderate geomagnetic conditions. The elevation of the wave vector varies through a range of 20-44 degree, and the phase velocity (1100-1300 m/s) approaches the sound velocity at the heights of the ionospheric F-region maximum. The calculated (by neglecting refraction corrections) location of the source roughly corresponds to the earthquake epicenter. Our data are consistent with the present views that shock-acoustic waves are caused by a piston-like movement of the Earth surface in the zone of an earthquake epicenter. ",2000,earthquakes in remote places,5.446524,6.4625173,1996-2000,1,#B2AB10
48,Survivability Improvement Against Earthquakes in Backbone Optical  Networks Using Actual Seismic Zone Information,"Survivability Improvement Against Earthquakes in Backbone Optical   Networks Using Actual Seismic Zone Information  Optical backbone networks carry a huge amount of bandwidth and serve as a key enabling technology to provide telecommunication connectivity across the world. Hence, in events of network component (node/link) failures, communication networks may suffer from huge amount of bandwidth loss and service disruptions. Natural disasters such as earthquakes, hurricanes, tornadoes, etc., occur at different places around the world, causing severe communication service disruptions due to network component failures. Most of the previous works on optical network survivability assume that the failures are going to occur in future, and the network is made survivable to ensure connectivity in events of failures. With the advancements in seismology, the predictions of earthquakes are becoming more accurate. Earthquakes have been a major cause of telecommunication service disruption in the past. Hence, the information provided by the meteorological departments and other similar agencies of different countries may be helpful in designing networks that are more robust against earthquakes. In this work, we consider the actual information provided by the Indian meteorological department (IMD) on seismic zones, and earthquakes occurred in the past in India, and propose a scheme to improve the survivability of the existing Indian optical network through minute changes in network topology. Simulations show significant improvement in the network survivability can be achieved using the proposed scheme in events of earthquakes. ",2017,earthquakes in remote places,1.5188193,6.308049,2016-2020,5,#7402B1
49,Anomalous diffusion of epicentres,Anomalous diffusion of epicentres  The classification of earthquakes in main shocks and aftershocks by a method recently proposed by M. Baiesi and M. Paczuski allows to the generation of a complex network composed of clusters that group the most correlated events. The spatial distribution of epicentres inside these structures corresponding to the catalogue of earthquakes in the eastern region of Cuba shows anomalous anti-diffusive behaviour evidencing the attractive nature of the main shock and the possible description in terms of fractional kinetics. ,2007,earthquakes in remote places,5.3281693,6.273119,2006-2010,3,#FEF65C
50,Detecting Volcano Deformation in InSAR using Deep learning,"Detecting Volcano Deformation in InSAR using Deep learning  Globally 800 million people live within 100 km of a volcano and currently 1500 volcanoes are considered active, but half of these have no ground-based monitoring. Alternatively, satellite radar (InSAR) can be employed to observe volcanic ground deformation, which has shown a significant statistical link to eruptions. Modern satellites provide large coverage with high resolution signals, leading to huge amounts of data. The explosion in data has brought major challenges associated with timely dissemination of information and distinguishing volcano deformation patterns from noise, which currently relies on manual inspection. Moreover, volcano observatories still lack expertise to exploit satellite datasets, particularly in developing countries. This paper presents a novel approach to detect volcanic ground deformation automatically from wrapped-phase InSAR images. Convolutional neural networks (CNN) are employed to detect unusual patterns within the radar data. ",2018,volcanoes in remote places,3.68431,8.030829,2016-2020,5,#7402B1
51,Abrupt changes of hydrothermal activity in a lava dome detected by  combined seismic and muon monitoring,"Abrupt changes of hydrothermal activity in a lava dome detected by   combined seismic and muon monitoring  The recent 2014 eruption of the Ontake volcano in Japan recalled that hydrothermal fields of moderately active volcanoes have an unpredictable and hazardous behavior that may endanger human beings. Steam blasts can expel devastating ejecta and create craters of several tens of meters. The management of such hydrothermal hazards in populated areas is problematic because of their very short time constants. At present no precursory signal is clearly identified as a potential warning of imminent danger. Here we show how the combination of seismic noise monitoring and muon density tomography allows to detect, with an unprecedented space and time resolution, the increase of activity of a hydrothermal focus located 50 to 100 m below the summit of an active volcano, the La Soufri\`ere of Guadeloupe, in the Lesser Antilles. The present study deals with hydrothermal activity events at timescales of few hours to few days. We show how the combination of those two methods improves the risk evaluation of short-term hazards and the localization of the involved volumes in the volcano. We anticipate that the deployment of networks of various sensors including temperature probes, seismic antennas and cosmic muon telescopes around such volcanoes could valuably contribute to early warning decisions. ",2018,volcanoes in remote places,5.8566003,6.527841,2016-2020,5,#7402B1
52,A multi-decadal view of the heat and mass budget of a volcano in unrest:  La Soufri\`ere de Guadeloupe (French West Indies),"A multi-decadal view of the heat and mass budget of a volcano in unrest:   La Soufri\`ere de Guadeloupe (French West Indies)  Particularly in the presence of a hydrothermal system, many volcanoes output large quantities of heat through the transport of water from deep within the edifice to the surface. Thus, heat flux is a prime tool for evaluating volcanic activity and unrest. We review the volcanic unrest at La Soufri\`ere de Guadeloupe (French West Indies) using an airborne thermal camera survey, and in-situ measurements of temperature and flow rate through temperature probes, Pitot-tube and MultiGAS measurements. We deduce mass and heat fluxes for the fumarolic, ground and thermal spring outputs and follow these over a period spanning 2000--2020. Our results are compared with published data and we performed a retrospective analysis of the temporal variations in heat flux over this period using the literature data.   We find that the heat emitted by the volcano is 36.5 +/- 7.9 MW, of which the fumarolic heat flux is dominant at 28.3 +/- 6.8 MW. Given a total heated area of 26780 m2, this equates to a heat flux density of 627 +/- 94 W/m2, which is amongst the highest established for worldwide volcanoes with hydrothermal systems, particularly for dome volcanoes. A major change at La Soufri\`ere de Guadeloupe, however, is the development of a widespread region of ground heating at the summit where heat output has increased from 0.2 +/- 0.1 MW in 2010 to 5.7 +/- 0.9 MW in 2020. This change is concurrent with accelerating unrest at the volcano, and the emergence of two new high-flux fumaroles in recent years. Our findings highlight the importance of continued and enhanced surveillance and research strategies at La Soufri\`ere de Guadeloupe, the results of which can be used to better understand hydrothermal volcanism the world over. ",2020,volcanoes in remote places,9.028364,8.461191,2020-,6,#5E0897
53,"Comparative analysis of the impact of geological activity on  astronomical sites of the Canary Islands, Hawaii and Chile","Comparative analysis of the impact of geological activity on   astronomical sites of the Canary Islands, Hawaii and Chile  An analysis of the impact of seismic and volcanic activity was carried out at selected astronomical sites, namely the observatories of El Teide (Tenerife, Canary Islands), Roque de los Muchachos (La Palma, Canary Islands), Mauna Kea (Hawaii) and Paranal (Chile) and the candidate site of Cerro Ventarrones (Chile). Hazard associated to volcanic activity is low or negligible at all sites, whereas seismic hazard is very high in Chile and Hawaii. The lowest geological hazard in both seismic and volcanic activity was found at Roque de los Muchachos observatory, in the island of La Palma. ",2009,volcanoes in remote places,5.422346,6.462748,2006-2010,3,#FEF65C
54,"Methods for analyzing surface texture effects of volcanoes with Plinian  and subplinian eruptions types: Cases of study Lascar (23 S) and Chaiten (42  S), Chile","Methods for analyzing surface texture effects of volcanoes with Plinian   and subplinian eruptions types: Cases of study Lascar (23 S) and Chaiten (42   S), Chile  This paper presents a new methodology that provides the analysis of surface texture changes in areas adjacent to the volcano and its impact product of volcanic activity. To do this, algorithms from digital image processing such as the co-occurrence matrix and the wavelet transform are used. These methods are working on images taken by the Landsat satellite platform sensor 5 TM and Landsat 7 ETM + sensor, and implemented with the purpose of evaluating superficial changes that can warn of surface movements of the volcano. The results were evaluated by similarity metrics for grayscale images, and validated in two different scenarios that have the same type of eruption, but differ, essentially, in climate and vegetation. Finally, the proposed algorithm is presented, setting the parameters and constraints for implementation and use. ",2016,volcanoes in remote places,4.693839,7.5651984,2016-2020,5,#7402B1
55,Constraints on the production of phosphine by Venusian volcanoes,"Constraints on the production of phosphine by Venusian volcanoes  The initial reports of the presence of phosphine in the cloud decks of Venus has led to the suggestion that volcanism was the source of phosphine, through volcanic phosphides ejected into the clouds. Here we examine the idea that mantle plume volcanism, bringing material from the deep mantle to the surface, could generate observed amounts of phosphine through interaction of explosively erupted phosphide with sulfuric acid clouds. Direct eruption of deep mantle phosphide is unphysical, but shallower material could contain traces of phosphide, and could be erupted to the surface. Explosive eruption that efficiently transported material to the clouds would require ocean:magma interactions or subduction of hydrated oceanic crust, neither of which occur on modern Venus. The transport of erupted material to altitudes coinciding with the observations of phosphine is consequently very inefficient. Using the model proposed by Truong and Lunine as a base case, we estimate that an eruption volume of at least 21,600 km3/year would be required to explain the presence of 1 ppb phosphine in the clouds. This is greater than any historical terrestrial eruption rate, and would have several detectable consequences for remote and in situ observations to confirm. More realistic lithospheric mineralogy, volcano mechanics or atmospheric photochemistry require even more volcanism. ",2021,volcanoes in remote places,9.275536,9.273534,2020-,6,#5E0897
56,Cessation of Volcanism on Earth-Possibilities in near geological future,"Cessation of Volcanism on Earth-Possibilities in near geological future  The number of active volcanoes and its latitudinal extent is likely to be related to the magnitude of internal heat in rocky planets. A critical value of internal heat may require in these planets to sustain volcanic activity and the decline of volcanic activity since their formation of these planets is inferred to be governed by radioactive decay laws. We find that major volcanic activity in Mars, Moon, Mercury and Venus has ceased when their respective surface heat flux values are within ten percentage of the current surface heat flux value of Earth. The reduction in spatial extent of recent volcanic activity in Venus compared to the geological past is inferred to be part of significant reduction in volcanic activity in this twin planet of Earth. We suggest that the volcanic activity in Earth is also declining significantly since the period of mass extinction of dinosaurs 65 million years ago. It may cease completely within a time span between 19 to 65 million years from now with possible implications in Earth's interior, climate and biosphere. ",2018,volcanoes in remote places,8.950314,9.103818,2016-2020,5,#7402B1
57,Predicting Safe Regions within Lava Flows over Topography,"Predicting Safe Regions within Lava Flows over Topography  We present a shallow, isothermal, Newtonian model for the transient interaction of lava flows with topography. Numerical integrations and simple mathematical approximations are deployed to quantify how topography controls lava thicknesses and flow speeds. Considering idealised topographic features, we show that modest depressions thicken and accelerate the flow - even far downstream - whilst mounds have the opposite effect. However, deep ponds of lava form in depressions of sufficient amplitude, which introduces a long timescale for lava to fill the depression and hence the accelerated downstream flow may never be attained. Relatively large mounds completely divert the lava, providing protected lava-free regions for homes and infrastructure. There can, however, be hazardous, deep, fast flow around the edges of the mound, owing to diversion. Additionally, we show that our model accurately predicts the lava-free region that has been observed in the eruption 35 kyr ago at Marcath Volcano, Nevada. ",2022,volcanoes in remote places,5.7872143,5.45038,2020-,6,#5E0897
58,Resolving Io's Volcanoes from a Mutual Event Observation at the Large  Binocular Telescope,"Resolving Io's Volcanoes from a Mutual Event Observation at the Large   Binocular Telescope  Unraveling the geological processes ongoing at Io's numerous sites of active volcanism requires high spatial resolution to, for example, measure the areal coverage of lava flows or identify the presence of multiple emitting regions within a single volcanic center. In de Kleer et al. (2017) we described observations with the Large Binocular Telescope (LBT) during an occultation of Io by Europa at ~6:17 UT on 2015 March 08, and presented a map of the temperature distribution within Loki Patera derived from these data. Here we present emission maps of three other volcanic centers derived from the same observation: Pillan Patera, Kurdalagon Patera, and the vicinity of Ulgen Patera/PV59/N Lerna Regio. The emission is localized by the light curves and resolved into multiple distinct emitting regions in two of the cases. Both Pillan and Kurdalagon Paterae had undergone eruptions in the months prior to our observations, and the location and intensity of the emission is interpreted in the context of the temporal evolution of these eruptions observed from other facilities. The emission from Kurdalagon Patera is resolved into two distinct emitting regions separated by only a few degrees in latitude that were unresolved by Keck observations from the same month. ",2021,volcanoes in remote places,9.912297,8.6942425,2020-,6,#5E0897
59,Arabia: from craters to stone circles,"Arabia: from craters to stone circles  The Arabia Shield has a volcanic nature inside. A region of the Western Saudi Arabia is in fact covered with vast fields of lava known as harraat. These lands are spotted by many stone circles and other quite interesting archaeological remains of the Neolithic period, such as the ""desert kites"", the hunters used to guide the game across the harrah in some corrals. With Google Maps, we can observe both sceneries, the volcanic nature of the land and a portrait of Arabia during the Neolithic times. ",2010,volcanoes in remote places,9.306799,8.974654,2006-2010,3,#FEF65C
60,Comparison of two early warning systems for regional flash flood hazard  forecasting,"Comparison of two early warning systems for regional flash flood hazard   forecasting  The anticipation of flash flood events is crucial to issue warnings to mitigate their impact. This work presents a comparison of two early warning systems for real-time flash flood hazard forecasting at regional scale. The two systems are based in a gridded drainage network and they use weather radar precipitation inputs to assess the hazard level in different points of the study area, considering the return period (in years) as the indicator of the flash flood hazard. The essential difference between the systems is that one is a rainfall-based system (ERICHA), using the upstream basin-aggregated rainfall as the variable to determine the hazard level, while the other (Flood-PROOFS) is a system based on a distributed rainfall-runoff model to compute the streamflows at pixel scale. The comparison has been done for three rainfall events in the autumn of 2014 that resulted in severe flooding in the Liguria region (Northwest of Italy). The results obtained by the two systems show many similarities, particularly for larger catchments and for large return periods (extreme floods). ",2019,flash flood,3.7292309,7.0682325,2016-2020,5,#7402B1
61,"FlowDB a large scale precipitation, river, and flash flood dataset","FlowDB a large scale precipitation, river, and flash flood dataset  Flooding results in 8 billion dollars of damage annually in the US and causes the most deaths of any weather related event. Due to climate change scientists expect more heavy precipitation events in the future. However, no current datasets exist that contain both hourly precipitation and river flow data. We introduce a novel hourly river flow and precipitation dataset and a second subset of flash flood events with damage estimates and injury counts. Using these datasets we create two challenges (1) general stream flow forecasting and (2) flash flood damage estimation. We have created several publicly available benchmarks and an easy to use package. Additionally, in the future we aim to augment our dataset with snow pack data and soil index moisture data to improve predictions. ",2020,flash flood,4.158283,7.092652,2020-,6,#5E0897
62,Flood Prediction Using Machine Learning Models,"Flood Prediction Using Machine Learning Models  Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy. ",2022,flash flood,4.1629286,6.9495845,2020-,6,#5E0897
63,"Flash-X, a multiphysics simulation software instrument","Flash-X, a multiphysics simulation software instrument  Flash-X is a highly composable multiphysics software system that can be used to simulate physical phenomena in several scientific domains. It derives some of its solvers from FLASH, which was first released in 2000. Flash-X has a new framework that relies on abstractions and asynchronous communications for performance portability across a range of increasingly heterogeneous hardware platforms. Flash-X is meant primarily for solving Eulerian formulations of applications with compressible and/or incompressible reactive flows. It also has a built-in, versatile Lagrangian framework that can be used in many different ways, including implementing tracers, particle-in-cell simulations, and immersed boundary methods. ",2022,flash flood,2.8270547,6.5653396,2020-,6,#5E0897
64,Validating the FLASH Code: Vortex-Dominated Flows,"Validating the FLASH Code: Vortex-Dominated Flows  As a component of the Flash Center's validation program, we compare FLASH simulation results with experimental results from Los Alamos National Laboratory. The flow of interest involves the lateral interaction between a planar Ma=1.2 shock wave with a cylinder of gaseous sulfur hexafluoride (SF_6) in air, and in particular the development of primary and secondary instabilities after the passage of the shock. While the overall evolution of the flow is comparable in the simulations and experiments, small-scale features are difficult to match. We focus on the sensitivity of numerical results to simulation parameters. ",2004,flash flood,6.082143,5.58418,2001-2005,2,#FFE135
65,Flashes of light below the dripping faucet: an optical signal from  capillary oscillations of water drops,"Flashes of light below the dripping faucet: an optical signal from   capillary oscillations of water drops  Falling water drops from a dripping faucet, illuminated from above, exhibit a row of bright strips of light, a few centimeters apart at a fixed distance below the faucet. Flash photographs of the drops show that they are oblate in shape when the flashes occur and the bright flashes of light originate from the edge of the drop that is on the opposite of the overhead light source. Here we show that that the spots result from the same internal reflection that gives rise to the rainbow in a cloud of spherical drops . The periodic flashes reflect the capillary oscillations of the liquid drop between alternating prolate and oblate shapes and the dramatic enhancement in the oblate phase result from a combination of several optical effects. Ray tracing analysis shows that the flashes occur when the rainbow angle, which is 42 $\deg$ in spherical drops, but sweeps over a wide range between 35 $\deg$ and 65 $\deg$ for typical ellipsoidal drops and the intensity of the caustic is strongly enhanced in the oblate phase. This phenomenon can be seen in all brightly lit water sprays with millimeter size drops and is responsible for their white color. ",2009,flash flood,9.492994,7.916832,2006-2010,3,#FEF65C
66,"ML-based Flood Forecasting: Advances in Scale, Accuracy and Reach","ML-based Flood Forecasting: Advances in Scale, Accuracy and Reach  Floods are among the most common and deadly natural disasters in the world, and flood warning systems have been shown to be effective in reducing harm. Yet the majority of the world's vulnerable population does not have access to reliable and actionable warning systems, due to core challenges in scalability, computational costs, and data availability. In this paper we present two components of flood forecasting systems which were developed over the past year, providing access to these critical systems to 75 million people who didn't have this access before. ",2020,flash flood,4.0773344,7.201465,2020-,6,#5E0897
67,Kolmogorovian turbulence in transitional pipe flows,"Kolmogorovian turbulence in transitional pipe flows  As everyone knows who has opened a kitchen faucet, pipe flow is laminar at low flow velocities and turbulent at high flow velocities. At intermediate velocities there is a transition wherein plugs of laminar flow alternate along the pipe with ""flashes"" of a type of fluctuating, non-laminar flow which remains poorly known. We show experimentally that the fluid friction of flash flow is diagnostic of turbulence. We also show that the statistics of flash flow are in keeping with Kolmogorov's phenomenological theory of turbulence (so that, e.g., the energy spectra of both flash flow and turbulent flow satisfy small-scale universality). We conclude that transitional pipe flows are two- phase flows in which one phase is laminar and the other, carried by flashes, is turbulent in the sense of Kolmogorov. ",2017,flash flood,5.946411,5.1582456,2016-2020,5,#7402B1
68,Convectively-coupled High-frequency Atmospheric waves triggered Kerala  floods in 2018 and 2019,"Convectively-coupled High-frequency Atmospheric waves triggered Kerala   floods in 2018 and 2019  Floods have repeatedly battered the South Indian state, Kerala, as a result of the unprecedented heavy rainfall during Boreal Summers, in recent years. The state witnessed large departures from normal rainfall in 2018 and 2019. Previous studies have seldom adopted a systematic approach to understand the phenomenon responsible for the recurrent extreme events. Hence, this study, based on spectral methods, identifies a characteristic propagation of high-frequency equatorial waves in the atmosphere, which travelled from near tropical west Pacific to the east coast of Africa. These waves stimulated intense convection and ensured sufficient availability of moisture over the state, and are hence responsible for Kerala Floods. ",2021,flash flood,8.557575,7.86746,2020-,6,#5E0897
69,High Frequency Trading and Mini Flash Crashes,"High Frequency Trading and Mini Flash Crashes  We analyse all Mini Flash Crashes (or Flash Equity Failures) in the US equity markets in the four most volatile months during 2006-2011. In contrast to previous studies, we find that Mini Flash Crashes are the result of regulation framework and market fragmentation, in particular due to the aggressive use of Intermarket Sweep Orders and Regulation NMS protecting only Top of the Book. We find strong evidence that Mini Flash Crashes have an adverse impact on market liquidity and are associated with Fleeting Liquidity. ",2012,flash flood,3.645902,5.722481,2011-2015,4,#9100CF
70,A Radar-Based Hail Climatology of Australia,"A Radar-Based Hail Climatology of Australia  In Australia, hailstorms present considerable public safety and economic risks, where they are considered the most damaging natural hazard in terms of annual insured losses. Despite these impacts, the current climatological distribution of hailfall across the continent is still comparatively poorly understood. This study aims to supplement previous national hail climatologies, such as those based on environmental proxies or satellite radiometer data, with more direct radar-based hail observations. The heterogeneous and incomplete nature of the Australian radar network complicates this task and prompts the introduction of some novel methodological elements. We introduce an empirical correction technique to account for hail reflectivity biases at C-band, derived by comparing overlapping C- and S-band observations. Furthermore, we demonstrate how object-based hail swath analysis may be used to produce resolution-invariant hail frequencies, and describe an interpolation method used to create a spatially continuous hail climatology. The Maximum Estimated Size of Hail (MESH) parameter is then applied to a mixture of over fifty operational radars in the Australian radar archive, resulting in the first nationwide, radar-based hail climatology. The spatiotemporal distribution of hailstorms is examined, including their physical characteristics, seasonal and diurnal frequency, and regional variations of such properties across the continent. ",2023,hail storm,9.643678,7.9078116,2020-,6,#5E0897
71,Thunderstorm nowcasting with deep learning: a multi-hazard data fusion  model,"Thunderstorm nowcasting with deep learning: a multi-hazard data fusion   model  Predictions of thunderstorm-related hazards are needed in several sectors, including first responders, infrastructure management and aviation. To address this need, we present a deep learning model that can be adapted to different hazard types. The model can utilize multiple data sources; we use data from weather radar, lightning detection, satellite visible/infrared imagery, numerical weather prediction and digital elevation models. We demonstrate the ability of the model to predict lightning, hail and heavy precipitation probabilistically on a 1 km resolution grid, with a temporal resolution of 5 min and lead times up to 60 min. Shapley values quantify the importance of the different data sources, showing that the weather radar products are the most important predictors for all three hazard types. ",2022,hail storm,4.098918,7.177585,2020-,6,#5E0897
72,Long-term hail risk assessment with deep neural networks,"Long-term hail risk assessment with deep neural networks  Hail risk assessment is necessary to estimate and reduce damage to crops, orchards, and infrastructure. Also, it helps to estimate and reduce consequent losses for businesses and, particularly, insurance companies. But hail forecasting is challenging. Data used for designing models for this purpose are tree-dimensional geospatial time series. Hail is a very local event with respect to the resolution of available datasets. Also, hail events are rare - only 1% of targets in observations are marked as ""hail"". Models for nowcasting and short-term hail forecasts are improving. Introducing machine learning models to the meteorology field is not new. There are also various climate models reflecting possible scenarios of climate change in the future. But there are no machine learning models for data-driven forecasting of changes in hail frequency for a given area.   The first possible approach for the latter task is to ignore spatial and temporal structure and develop a model capable of classifying a given vertical profile of meteorological variables as favorable to hail formation or not. Although such an approach certainly neglects important information, it is very light weighted and easily scalable because it treats observations as independent from each other. The more advanced approach is to design a neural network capable to process geospatial data. Our idea here is to combine convolutional layers responsible for the processing of spatial data with recurrent neural network blocks capable to work with temporal structure.   This study compares two approaches and introduces a model suitable for the task of forecasting changes in hail frequency for ongoing decades. ",2022,hail storm,4.005536,7.1420975,2020-,6,#5E0897
73,A Hail Size Distribution Impact Transducer,A Hail Size Distribution Impact Transducer  An active impact transducer has been designed and tested for the purpose of monitoring hail fall in the vicinity of the Space Shuttle launch pads. An important outcome of this design is the opportunity to utilize frequency analysis to discriminate between the audio signal generated from raindrop impacts and that of hailstone impacts. The sound of hail impacting a metal plate is sub-tly but distinctly different than the sound of rain impacts. This useful characteristic permits application of signal processing algorithms that are inherently more robust than techniques relying on amplitude processing alone in the implementation of a hail disdrometer. ,2014,hail storm,5.8334985,7.7716475,2011-2015,4,#9100CF
74,Can hail and rain nucleate cloud droplets?,"Can hail and rain nucleate cloud droplets?  We present results from moist convection in a mixture of pressurized sulfur hexa-flouride (liquid and vapor) and helium (gas) to model the wet and dry components of the earth's atmosphere. To allow for homogeneous nucleation, we operate the experiment close to critical conditions. We report on the nucleation of microdroplets in the wake of large cold liquid drops falling through the supersaturated atmosphere and show that the homogeneous nucleation is caused by isobaric cooling of the saturated sulfur hexaflouride vapor. Our results carry over to atmospheric clouds: falling hail and cold rain drops may enhance the heterogeneous nucleation of microdroplets in their wake under supersaturated atmospheric conditions. We also observed that under appropriate conditions settling microdroplets form a rather stable horizontal cloud layer, which separates regions of super and sub critical saturation. ",2017,hail storm,6.9925632,6.1040044,2016-2020,5,#7402B1
75,Stable Systems with Power Law Conditions for Poisson Hail,"Stable Systems with Power Law Conditions for Poisson Hail  We consider Poisson hail models and characterize up to boundaries the collection of critical moments which guarantee stability. In particular, we treat the case of infinite speed of propagation. ",2021,hail storm,5.111621,3.878239,2020-,6,#5E0897
76,Large deviations and rain showers,"Large deviations and rain showers  Rainfall from ice-free cumulus clouds requires collisions of large numbers of microscopic droplets to create every raindrop. The onset of rain showers can be surprisingly rapid, much faster than the mean time required for a single collision. Large-deviation theory is used to explain this observation. ",2015,hail storm,6.447108,5.720026,2016-2020,5,#7402B1
77,A new approach for rain gush formation associated with ionic wind,"A new approach for rain gush formation associated with ionic wind  Based upon experimental observation in the laboratory, we propose that ionic wind from corona discharge inside a thundercloud would play an important role in producing a rain gush. A cyclic chain of events inside a super-saturated environment in a thundercloud is proposed, each event enhancing the successive ones until lightning occurs. These successive events are collision between snowflakes and rimers, charge separation, corona discharge, avalanche ionization, ionic wind originating from the positively and negatively charged masses of cloud, vortex motion and turbulence when mixed with the updraft, more collision, more charge separation, stronger corona discharge, and so on. Meanwhile, avalanche ionization would produce more CCN (cloud condensation nuclei) resulting in more precipitation and hence rimers formation in the super-saturated environment. More collision in the buoyant turbulence would lead to more fusion of droplets and the formation of larger rimers. The cyclic processes would repeat themselves until the electric field between the two oppositely charged masses of cloud was strong enough to induce a breakdown. The latter would create a sudden short circuit between the two charged masses of cloud neutralizing the charges. There would be no more ionic wind, hence, much less buoyant turbulence. The updraft alone would not be sufficiently strong to support larger rimers which would fall down 'suddenly' to the earth surface as a rain gush. ",2018,hail storm,9.385207,8.313945,2016-2020,5,#7402B1
78,Atmospheric electricity and thunderstorm ground enhancements,Atmospheric electricity and thunderstorm ground enhancements  The comparative analysis of three thunderstorms on Aragats in May 2021 demonstrates that relativistic runaway electron avalanches are developing in large areas of the thunderous atmosphere. ,2021,hail storm,9.575214,7.309573,2020-,6,#5E0897
79,The red rain phenomenon of Kerala and its possible extraterrestrial  origin,"The red rain phenomenon of Kerala and its possible extraterrestrial   origin  A red rain phenomenon occurred in Kerala, India starting from 25th July 2001, in which the rainwater appeared coloured in various localized places that are spread over a few hundred kilometers in Kerala. Maximum cases were reported during the first 10 days and isolated cases were found to occur for about 2 months. The striking red colouration of the rainwater was found to be due to the suspension of microscopic red particles having the appearance of biological cells. These particles have no similarity with usual desert dust. An estimated minimum quantity of 50,000 kg of red particles has fallen from the sky through red rain. An analysis of this strange phenomenon further shows that the conventional atmospheric transport processes like dust storms etc. cannot explain this phenomenon. The electron microscopic study of the red particles shows fine cell structure indicating their biological cell like nature. EDAX analysis shows that the major elements present in these cell like particles are carbon and oxygen. Strangely, a test for DNA using Ethidium Bromide dye fluorescence technique indicates absence of DNA in these cells. In the context of a suspected link between a meteor airburst event and the red rain, the possibility for the extraterrestrial origin of these particles from cometary fragments is discussed. ",2006,hail storm,9.695823,9.231904,2006-2010,3,#FEF65C
80,Exposure to War and Its Labor Market Consequences over the Life Cycle,"Exposure to War and Its Labor Market Consequences over the Life Cycle  With 70 million dead, World War II remains the most devastating conflict in history. Of the survivors, millions were displaced, returned maimed from the battlefield, or spent years in captivity. We examine the impact of such wartime experiences on labor market careers and show that they often become apparent only at certain life stages. While war injuries reduced employment in old age, former prisoners of war postponed their retirement. Many displaced workers, particularly women, never returned to employment. These responses are in line with standard life-cycle theory and thus likely extend to other conflicts. ",2023,millions loss of life,4.453524,7.0791154,2020-,6,#5E0897
81,Public Health and access to medicine. Pharmaceutical industry's role,"Public Health and access to medicine. Pharmaceutical industry's role  Every year, 10 million people die from lack of access to treatment for curable diseases, specially in developing countries. Meanwhile, legal but unsafe drugs cause 130 thousand deaths per year. How can this be happening in 21st Century? What role does the pharmaceutical industry play in this tragedy? In this research, WHO reports are analyzed and primary information gathered so as to answer this questions. ",2019,millions loss of life,4.604886,7.0292497,2016-2020,5,#7402B1
82,"Modeling the Health Expenditure in Japan, 2011. A Healthy Life Years  Lost Methodology","Modeling the Health Expenditure in Japan, 2011. A Healthy Life Years   Lost Methodology  The Healthy Life Years Lost Methodology (HLYL) is introduced to model and estimate the Health Expenditure in Japan in 2011. The HLYL theory and estimation methods are presented in our books in the Springer Series on Demographic Methods and Population Analysis vol. 45 and 46 titled: Exploring the Health State of a Population by Dynamic Modeling Methods and Demography and Health Issues: Population Aging, Mortality and Data Analysis. Special applications appear in Chapters of these books as in The Health-Mortality Approach in Estimating the Healthy Life Years Lost Compared to the Global Burden of Disease Studies and Applications in World, USA and Japan and in Estimation of the Healthy Life Expectancy in Italy Through a Simple Model Based on Mortality Rate by Skiadas and Arezzo. Here further to present the main part of the methodology with more details and illustrations, we develop and extend a life table important to estimate the healthy life years lost along with the fitting to the health expenditure in the related case. The application results are quite promising and important to support decision makers and health agencies with a powerful tool to improve the health expenditure allocation and the future predictions. ",2019,millions loss of life,4.284059,6.756938,2016-2020,5,#7402B1
83,Efficient Monte Carlo Simulation of Biological Aging,"Efficient Monte Carlo Simulation of Biological Aging  A bit-string model of biological life-histories is parallelized, with hundreds of millions of individuals. It gives the desired drastic decay of survival probabilities with increasing age for 32 age intervals. ",1995,millions loss of life,4.890121,6.2866063,1990-1995,0,#B2AB10
84,The Absurdity of Death Estimates Based on the Vaccine Adverse Event  Reporting System,The Absurdity of Death Estimates Based on the Vaccine Adverse Event   Reporting System  We demonstrate from first principles a core fallacy employed by a coterie of authors who claim that data from the Vaccine Adverse Reporting System (VAERS) show that hundreds of thousands of U.S. deaths are attributable to COVID vaccination. ,2022,millions loss of life,4.6247144,7.4614463,2020-,6,#5E0897
85,Preemptive periodic epidemic control reduces life and healthcare system  costs without aggravation of social and economic losses,"Preemptive periodic epidemic control reduces life and healthcare system   costs without aggravation of social and economic losses  Many countries are managing COVID-19 epidemic by switching between lighter and heavier restrictions. While an open-close and a close-open cycle have comparable socio-economic costs, the former leads to a much heavier burden in terms of deaths and pressure on the healthcare system. An empirical demonstration of the toll ensuing from procrastination was recently observed in Israel, where both cycles were enforced from late August to mid-December 2020, yielding some 1,600 deaths with open-close compared to 440 with close-open. ",2021,millions loss of life,4.668852,7.294156,2020-,6,#5E0897
86,A note on large deviations in life insurance,"A note on large deviations in life insurance  We study large and moderate deviations for a life insurance portfolio, without assuming identically distributed losses. The crucial assumption is that losses are bounded, and that variances are bounded below. From a standard large deviations upper bound, we get an exponential bound for the probability of the average loss exceeding a threshold. A counterexample shows that a full large deviation principle does not follow from our assumptions. ",2020,millions loss of life,3.5756783,5.8290367,2020-,6,#5E0897
87,Reanimating a Dead Economy: Financial and Economic Analysis of a Zombie  Outbreak,"Reanimating a Dead Economy: Financial and Economic Analysis of a Zombie   Outbreak  In this paper, we study the financial and economic implications of a zombie epidemic on a major industrialized nation. We begin with a consideration of the epidemiological modeling of the zombie contagion. The emphasis of this work is on the computation of direct and indirect financial consequences of this contagion of the walking dead. A moderate zombie outbreak leaving 1 million people dead in a major industrialized nation could result in GDP losses of 23.44% over the subsequent year and a drop in financial market of 29.30%. We conclude by recommending policy actions necessary to prevent this potential economic collapse. ",2020,millions loss of life,3.5687335,6.0825906,2020-,6,#5E0897
88,Mass loss summary - a personal perspective,"Mass loss summary - a personal perspective  For the occasion of the official retirement of Henny Lamers, a meeting was held to celebrate Henny's contribution to mass loss from stars and stellar clusters. Stellar mass loss is crucial for understanding the life and death of massive stars, as well as their environments. Henny has made important contributions to many aspects of our understanding of hot-star winds. Here, the most dominant aspects of the stellar part of the meeting: (i) O star wind clumping, (ii) mass loss near the Eddington limit, and (iii) and the driving of Wolf-Rayet winds, are highlighted. ",2007,millions loss of life,9.715848,9.051931,2006-2010,3,#FEF65C
89,The mass-loss dominated lives of the most massive stars,"The mass-loss dominated lives of the most massive stars  Utrecht has a long tradition in both spectroscopy and mass-loss studies. Here we present a novel methodology to calibrate mass-loss rates on purely spectroscopic grounds. We utilize this to predict the final fates of massive stars, involving pair-instability and long gamma-ray bursts (GRBs) at low metallicity Z. ",2012,millions loss of life,10.154798,9.60933,2011-2015,4,#9100CF
90,Hephaestus: A large scale multitask dataset towards InSAR understanding,"Hephaestus: A large scale multitask dataset towards InSAR understanding  Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products in particular, are one of the largest sources of Earth Observation data. InSAR provides unique information on diverse geophysical processes and geology, and on the geotechnical properties of man-made structures. However, there are only a limited number of applications that exploit the abundance of InSAR data and deep learning methods to extract such knowledge. The main barrier has been the lack of a large curated and annotated InSAR dataset, which would be costly to create and would require an interdisciplinary team of experts experienced on InSAR data interpretation. In this work, we put the effort to create and make available the first of its kind, manually annotated dataset that consists of 19,919 individual Sentinel-1 interferograms acquired over 44 different volcanoes globally, which are split into 216,106 InSAR patches. The annotated dataset is designed to address different computer vision problems, including volcano state classification, semantic segmentation of ground deformation, detection and classification of atmospheric signals in InSAR imagery, interferogram captioning, text to InSAR generation, and InSAR image quality assessment. ",2022,insar,8.869156,8.792029,2020-,6,#5E0897
91,Thirty years of pulsar studies at ESO. The Italian Contribution,"Thirty years of pulsar studies at ESO. The Italian Contribution  In May 1982, when Italy joined ESO, only two isolated neutron stars (INSs) had been identified in the optical: the Crab and Vela pulsars. Thanks to the ESO telescopes and the perseverance of a few Italian astronomers, now about 30 INSs have been identified in the optical/IR, and a new important channel in their multi-wavelength studies has been opened. In this contribution, I review the major steps in 30 years of INS studies at ESO, highlight the role of Italian astronomers, and introduce future perspectives with the E-ELT. ",2012,insar,9.37007,8.6312,2011-2015,4,#9100CF
92,Constructive Equivariant Observer Design for Inertial Navigation,"Constructive Equivariant Observer Design for Inertial Navigation  Inertial Navigation Systems (INS) are algorithms that fuse inertial measurements of angular velocity and specific acceleration with supplementary sensors including GNSS and magnetometers to estimate the position, velocity and attitude, or extended pose, of a vehicle. The industry-standard extended Kalman filter (EKF) does not come with strong stability or robustness guarantees and can be subject to catastrophic failure. This paper exploits a Lie group symmetry of the INS dynamics to propose the first nonlinear observer for INS with error dynamics that are almost-globally asymptotically and locally exponentially stable, independently of the chosen gains. The observer is aided only by a GNSS measurement of position. As expected, the convergence guarantee depends on persistence of excitation of the vehicle's specific acceleration in the inertial frame. Simulation results demonstrate the observer's performance and its ability to converge from extreme errors in the initial state estimates. ",2023,insar,3.33693,5.4970055,2020-,6,#5E0897
93,SAMplus: adaptive optics at optical wavelengths for SOAR,"SAMplus: adaptive optics at optical wavelengths for SOAR  Adaptive Optics (AO) is an innovative technique that substantially improves the optical performance of ground-based telescopes. The SOAR Adaptive Module (SAM) is a laser-assisted AO instrument, designed to compensate ground-layer atmospheric turbulence in near-IR and visible wavelengths over a large Field of View. Here we detail our proposal to upgrade SAM, dubbed SAMplus, that is focused on enhancing its performance in visible wavelengths and increasing the instrument reliability. As an illustration, for a seeing of 0.62 arcsec at 500 nm and a typical turbulence profile, current SAM improves the PSF FWHM to 0.40 arcsec, and with the upgrade we expect to deliver images with a FWHM of $\approx0.34$ arcsec -- up to 0.23 arcsec FWHM PSF under good seeing conditions. Such capabilities will be fully integrated with the latest SAM instruments, putting SOAR in an unique position as observatory facility. ",2018,insar,8.806617,8.120568,2016-2020,5,#7402B1
94,How Do You Want That Insulator?,How Do You Want That Insulator?  A normal insulator is turned into an exotic topological insulator by tuning its elemental composition. ,2011,insar,8.428699,3.6668904,2011-2015,4,#9100CF
95,INDRA: Intrusion Detection using Recurrent Autoencoders in Automotive  Embedded Systems,"INDRA: Intrusion Detection using Recurrent Autoencoders in Automotive   Embedded Systems  Today's vehicles are complex distributed embedded systems that are increasingly being connected to various external systems. Unfortunately, this increased connectivity makes the vehicles vulnerable to security attacks that can be catastrophic. In this work, we present a novel Intrusion Detection System (IDS) called INDRA that utilizes a Gated Recurrent Unit (GRU) based recurrent autoencoder to detect anomalies in Controller Area Network (CAN) bus-based automotive embedded systems. We evaluate our proposed framework under different attack scenarios and also compare it with the best known prior works in this area. ",2020,insar,1.3081295,7.334146,2020-,6,#5E0897
96,The European Pulsar Timing Array: current efforts and a LEAP toward the  future,"The European Pulsar Timing Array: current efforts and a LEAP toward the   future  The European Pulsar Timing Array (EPTA) is a multi-institutional, multi-telescope collaboration, with the goal of using high-precision pulsar timing to directly detect gravitational waves. In this article we discuss the EPTA member telescopes, current achieved timing precision, and near-future goals. We report a preliminary upper limit to the amplitude of a gravitational wave background. We also discuss the Large European Array for Pulsars, in which the five major European telescopes involved in pulsar timing will be combined to provide a coherent array that will give similar sensitivity to the Arecibo radio telescope, and larger sky coverage. ",2010,insar,9.629888,7.7569613,2006-2010,3,#FEF65C
97,Pulsars with the Australian Square Kilometre Array Pathfinder,"Pulsars with the Australian Square Kilometre Array Pathfinder  The Australian Square Kilometre Array Pathfinder (ASKAP) is a 36-element array with a 30-square-degree field of view being built at the proposed SKA site in Western Australia. We are conducting a Design Study for pulsar observations with ASKAP, planning both timing and search observations. We provide an overview of the ASKAP telescope and an update on pulsar-related progress. ",2010,insar,9.141471,8.365402,2006-2010,3,#FEF65C
98,The Parkes Multibeam Pulsar Survey,"The Parkes Multibeam Pulsar Survey  The Parkes multibeam pulsar survey is a high-frequency, fast-sampled survey of the Galactic Plane, expected to discover at least 500 new pulsars. To date, over 200 pulsars have been found, including several young pulsars and at least one with a very high magnetic field. Seven of the new stars are in binary systems; this number includes one probable double-neutron-star system, and one pulsar with an extremely massive companion. ",1999,insar,9.695731,7.961937,1996-2000,1,#B2AB10
99,Pulsars With Jets May Harbor Dynamically Important Accretion Disks,"Pulsars With Jets May Harbor Dynamically Important Accretion Disks  For many astrophysical sources with jets, there is evidence for the contemporaneous presence of disks. In contrast, pulsars such as the Crab and Vela show jets but have not yet revealed direct evidence for accretion disks. Here we show that for such pulsars, an accretion disk radiating below detectable thresholds may simultaneously account for (1) observed deviations in the braking indices from that of the simple dipole, (2) observed pulsar timing ages, and (3) possibly even the jet morphology via a disk outflow that interacts with the pulsar wind within, collimating and/or redirecting it. ",2003,insar,9.394647,8.288761,2001-2005,2,#FFE135
100,On the contribution of the horizontal sea-bed displacements into the  tsunami generation process,"On the contribution of the horizontal sea-bed displacements into the   tsunami generation process  The main reason for the generation of tsunamis is the deformation of the bottom of the ocean caused by an underwater earthquake. Usually, only the vertical bottom motion is taken into account while the horizontal co-seismic displacements are neglected in the absence of landslides. In the present study we propose a methodology based on the well-known Okada solution to reconstruct in more details all components of the bottom coseismic displacements. Then, the sea-bed motion is coupled with a three-dimensional weakly nonlinear water wave solver which allows us to simulate a tsunami wave generation. We pay special attention to the evolution of kinetic and potential energies of the resulting wave while the contribution of the horizontal displacements into wave energy balance is also quantified. Such contribution of horizontal displacements to the tsunami generation has not been discussed before, and it is different from the existing approaches. The methods proposed in this study are illustrated on the July 17, 2006 Java tsunami and some more recent events. ",2010,tsunamis,6.6476636,6.925526,2006-2010,3,#FEF65C
101,On the modelling of tsunami generation and tsunami inundation,"On the modelling of tsunami generation and tsunami inundation  While the propagation of tsunamis is well understood and well simulated by numerical models, there are still a number of unanswered questions related to the generation of tsunamis or the subsequent inundation. We review some of the basic generation mechanisms as well as their simulation. In particular, we present a simple and computationally inexpensive model that describes the seabed displacement during an underwater earthquake. This model is based on the finite fault solution for the slip distribution under some assumptions on the kinematics of the rupturing process. We also consider an unusual source for tsunami generation: the sinking of a cruise ship. Then we review some aspects of tsunami run-up. In particular, we explain why the first wave of a tsunami is sometimes less devastating than the subsequent waves. A resonance effect can boost the waves that come later. We also look at a particular feature of the 11 March 2011 tsunami in Japan - the formation of macro-scale vortices - and show that these macro-scale vortices can be captured by the nonlinear shallow water equations. ",2012,tsunamis,5.537769,6.154252,2011-2015,4,#9100CF
102,Air-burst Generated Tsunamis,"Air-burst Generated Tsunamis  This paper examines the questions of whether smaller asteroids that burst in the air over water can generate tsunamis that could pose a threat to distant locations. Such air burst-generated tsunamis are qualitatively different than the more frequently studied earthquake-generated tsunamis, and differ as well from impact asteroids. Numerical simulations are presented using the shallow water equations in several settings, demonstrating very little tsunami threat from this scenario. A model problem with an explicit solution that demonstrates and explains the same phenomena found in the computations is analyzed. We discuss the question of whether compressibility and dispersion are important effects that should be included, and show results from a more sophisticated model problem using the linearized Euler equations that begins to addresses this. ",2017,tsunamis,5.713593,4.819585,2016-2020,5,#7402B1
103,Tsunamis in Galaxy Clusters: Heating of Cool Cores by Acoustic Waves,"Tsunamis in Galaxy Clusters: Heating of Cool Cores by Acoustic Waves  Using an analytical model and numerical simulations, we show that acoustic waves generated by turbulent motion in intracluster medium effectively heat the central region of a so-called ``cooling flow'' cluster. We assume that the turbulence is generated by substructure motion in a cluster or cluster mergers. Our analytical model can reproduce observed density and temperature profiles of a few clusters. We also show that waves can transfer more energy from the outer region of a cluster than thermal conduction alone. Numerical simulations generally support the results of the analytical study. ",2003,tsunamis,10.274878,8.860733,2001-2005,2,#FFE135
104,Dynamics of tsunami waves,"Dynamics of tsunami waves  The life of a tsunami is usually divided into three phases: the generation (tsunami source), the propagation and the inundation. Each phase is complex and often described separately. A brief description of each phase is given. Model problems are identified. Their formulation is given. While some of these problems can be solved analytically, most require numerical techniques. The inundation phase is less documented than the other phases. It is shown that methods based on Smoothed Particle Hydrodynamics (SPH) are particularly well-suited for the inundation phase. Directions for future research are outlined. ",2006,tsunamis,5.95392,4.511301,2006-2010,3,#FEF65C
105,Optical tsunamis: shoaling of shallow water rogue waves in nonlinear  fibers with normal dispersion,"Optical tsunamis: shoaling of shallow water rogue waves in nonlinear   fibers with normal dispersion  In analogy with ocean waves running up towards the beach, shoaling of prechirped optical pulses may occur in the normal group-velocity dispersion regime of optical fibers. We present exact Riemann wave solutions of the optical shallow water equations and show that they agree remarkably well with the numerical solutions of the nonlinear Schr\""odinger equation, at least up to the point where a vertical pulse front develops. We also reveal that extreme wave events or optical tsunamis may be generated in dispersion tapered fibers in the presence of higher-order dispersion. ",2013,tsunamis,7.1024776,5.119338,2011-2015,4,#9100CF
106,Estimating earthquake-induced tsunami height probabilities without  sampling,"Estimating earthquake-induced tsunami height probabilities without   sampling  Given a distribution of earthquake-induced seafloor elevations, we present a method to compute the probability of the resulting tsunamis reaching a certain size on shore. Instead of sampling, the proposed method relies on optimization to compute the most likely fault slips that result in a seafloor deformation inducing a large tsunami wave. We model tsunamis induced by bathymetry change using the shallow water equations on an idealized slice through the sea. The earthquake slip model is based on a sum of multivariate log-normal distributions, and follows the Gutenberg-Richter law for moment magnitudes 7--9. For a model problem inspired by the Tohoku-Oki 2011 earthquake and tsunami, we quantify annual probabilities of differently sized tsunami waves. Our method also identifies the most effective tsunami mechanisms. These mechanisms have smoothly varying fault slip patches that lead to an expansive but moderately large bathymetry change. The resulting tsunami waves are compressed as they approach shore and reach close-to-vertical leading wave edge close to shore. ",2021,tsunamis,5.237064,6.379023,2020-,6,#5E0897
107,Comparison between three-dimensional linear and nonlinear tsunami  generation models,"Comparison between three-dimensional linear and nonlinear tsunami   generation models  The modeling of tsunami generation is an essential phase in understanding tsunamis. For tsunamis generated by underwater earthquakes, it involves the modeling of the sea bottom motion as well as the resulting motion of the water above it. A comparison between various models for three-dimensional water motion, ranging from linear theory to fully nonlinear theory, is performed. It is found that for most events the linear theory is sufficient. However, in some cases, more sophisticated theories are needed. Moreover, it is shown that the passive approach in which the seafloor deformation is simply translated to the ocean surface is not always equivalent to the active approach in which the bottom motion is taken into account, even if the deformation is supposed to be instantaneous. ",2006,tsunamis,5.5661488,4.6796737,2006-2010,3,#FEF65C
108,Scenario-based Tsunami hazard assessment for Northeastern Adriatic  coasts,"Scenario-based Tsunami hazard assessment for Northeastern Adriatic   coasts  Significant tsunamis in Northern Adriatic are rare and only a few historical events were reported in the literature, with sources mostly located along with central and southern parts of the Adriatic coasts. Recently, a tsunami alert system has been established for the whole Mediterranean area; however, a detailed description of the potential impact of tsunami waves on coastal areas is still missing for several sites. This study aims at modelling the hazard associated with possible tsunamis, generated by offshore earthquakes, with the purpose of contributing to tsunami risk assessment for selected urban areas located along the Northeastern Adriatic coasts. Tsunami modelling is performed by the NAMI DANCE software, which allows accounting for seismic source properties, variable bathymetry, and non-linear effects in waves propagation. Preliminary hazard scenarios at the shoreline are developed for the coastal areas of Northeastern Italy and at selected cities (namely Trieste, Monfalcone, Lignano and Grado). A wide set of potential tsunamigenic sources of tectonic origin, located in three distance ranges (namely at Adriatic-wide, regional and local scales), are considered for the modelling; sources are defined according to available literature, which includes catalogues of historical tsunami and existing active faults databases. Accordingly, a preliminary set of tsunami-related parameters and maps are obtained (e.g. maximum run-up, arrival times, synthetic mareograms), relevant towards planning mitigation actions at the selected sites. ",2023,tsunamis,5.0123076,6.4617,2020-,6,#5E0897
109,Tsunami hydrodynamic force on a building using a SPH real scale  numerical simulation,"Tsunami hydrodynamic force on a building using a SPH real scale   numerical simulation  One of the most important aspects in tsunami studies is the wave behavior when it approaches the coast. Information on physical parameters that characterize waves is often limited because of the diffilculties in achieving accurate measurements at the time of the event. The impact of a tsunami on the coast is governed by nonlinear physics such as turbulence with spatial and temporal variability. The use of the Smoothed Particle Hydrodynamic method (SPH) presents advantages over models based on two-dimensional Shallow Waters Equations (SWE), because the assumed vertical velocity simplifies hydrodynamics in two dimensions. The study presented here reports numerical SPH simulations of the tsunami event occurred in Coquimbo (Chile) on September 16 of 2015. On the basis of the reconstruction of the physical parameters that characterized this event (flow velocities, direction and water elevations), calibrated by a reference rodel, force values on buildings located on the study coast were numerically calculated, and compared with an estimate of the Chilean Structural Design Standard. Finally, discussion and conclusions of the comparison of both methodologies are presented, including an influence analysis of the topographical detail of the model in the estimation of hydrodynamic forces. ",2021,tsunamis,5.630187,6.3817816,2020-,6,#5E0897
110,Towards responsible research in digital technology for health care,"Towards responsible research in digital technology for health care  Digital technology is everywhere for the benefit of our daily and professional life. It strongly impacts our life and was crucial to maintain professional and social activities during the COVID19 crisis. Similarly, digital technologies are key within biomedical engineering research topics. Innovations have been generated and introduced over the last 40 years, demonstrating how computing and digital technologies have impacted health care. Although the benefits of digital technology are obvious now, we are at the convergence of several issues which makes us aware about social, societal and environmental challenges associated with this technology. In the social domain, digital technologies raise concern about exclusion (financial, geographical, educational, demographical, racial, gender, language, and disabled related exclusion) and physical and mental health. In the societal dimension, digital technologies raise concern about politics and democracy (sovereignty and governance, cognitive filters and citizen's engagement), privacy and security (data acquisition and usage transparency, level of personal approval, and level of anonymization), and economics. In the environmental dimension, digital technologies raise concern about energy consumption and hardware production. This paper introduces and defines these challenges for digital technology in general, as well as when applied to health care. The objective of this paper is to make the research community more aware about the challenges of digital technology and to promote more transparency for innovative and responsible research. ",2021,digital technologies,1.6177664,7.2720213,2020-,6,#5E0897
111,Digital Transformation: Environmental Friend or Foe? Panel Discussion at  the Australasian Conference on Information Systems 2019,"Digital Transformation: Environmental Friend or Foe? Panel Discussion at   the Australasian Conference on Information Systems 2019  The advent of digital technologies such as social media, mobile, analytics, cloud computing and internet-of-things has provided unique opportunities for organizations to engage in innovations that are affordable, easy-to-use, easy-to-learn and easy-to-implement. Transformations through such technologies often have positive impacts on business processes, products and services. As such, organizations have managed to increase productivity and efficiency, reduce cycle time and make substantial gains through digital transformation. Such transformations have also been positively associated with reducing harmful environmental impacts by providing organizations alternative ways of undertaking their business activities. However, in recent times, especially with an abundance of technologies being available at near-zero costs, questions regarding the potential negative impacts of digital transformation on the environment have arisen. The morass of the ubiquitous technologies around us necessitates the continuing creation of large data centers, that are increasing their capacity yielding a negative impact on the environment. Considering this dialectical contradiction, a panel was conducted at the Australasian Conference on Information Systems (ACIS) in Perth, Australia, in 2019. Its aim was to invigorate the dialogue regarding the impact of digital transformation on environmental sustainability and suggested some directions for future research in this area. ",2020,digital technologies,1.6356246,7.1625385,2020-,6,#5E0897
112,Socio-Technological Challenges and Opportunities: Paths Forward,"Socio-Technological Challenges and Opportunities: Paths Forward  Advancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible -- stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world's physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world. ",2021,digital technologies,1.5042518,6.9990296,2020-,6,#5E0897
113,"Appropriation, coloniality, and digital technologies. Observations from  within an African place","Appropriation, coloniality, and digital technologies. Observations from   within an African place  This paper provides an assessment of experiences and understanding of digital technologies from within an African place. It provides philosophical reflections upon the introduction and existence - appropriation - of digital technologies. Digital technologies are inherently linked to a colonialising power and, in general, unaligned with local, African ways of knowing. Imported technologies are set in modern, universalised doing and unsensitive to the importance of aligned being in African contexts. Sensitivities, it is argued, can be fostered by a decolonial turn, where focus shifts from the individual to the community. ",2021,digital technologies,1.9204673,7.607681,2020-,6,#5E0897
114,Future of work: ethics,"Future of work: ethics  Work must be reshaped in the upcoming new era characterized by new challenges and the presence of new technologies and computational tools. Over-automation seems to be the driver of the digitalization process. Substitution is the paradigm leading Artificial Intelligence and robotics development against human cognition. Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities. Digital technology is characterized also by scalability because of its easy and inexpensive deployment. Thus, automation can lead to the absence of jobs and scalable negative impact in human development and the performance of business. A look at digitalization from the lens of Sustainable Development Goals can tell us how digitalization impact in different sectors and areas considering society as a complex interconnected system. Here, reflections on how AI and Data impact future of work and sustainable development are provided grounded on an ethical core that comprises human-level principles and also systemic principles. ",2021,digital technologies,1.7332104,7.5441556,2020-,6,#5E0897
115,Towards Digital Engineering -- The Advent of Digital Systems Engineering,"Towards Digital Engineering -- The Advent of Digital Systems Engineering  Digital Engineering, the digital transformation of engineering to leverage digital technologies, is coming globally. This paper explores digital systems engineering, which aims at developing theory, methods, models, and tools to support the emerging digital engineering. A critical task is to digitalize engineering artifacts, thus enabling information sharing across platform, across life cycle, and across domains. We identify significant challenges and enabling digital technologies; analyze the transition from traditional engineering to digital engineering; define core concepts, including ""digitalization"", ""unique identification"", ""digitalized artifacts"", ""digital augmentation"", and others; present a big picture of digital systems engineering in four levels: vision, strategy, action, and foundation; briefly discuss each of main areas of research issues. Digitalization enables fast infusing and leveraging novel digital technologies; unique identification enables information traceability and accountability in engineering lifecycle; provenance enables tracing dependency relations among engineering artifacts; supporting model reproducibility and replicability; helping with trustworthiness evaluation of digital engineering artifacts. ",2020,digital technologies,1.7663034,7.1423435,2020-,6,#5E0897
116,"Digital Twin: Enabling Technologies, Challenges and Open Research","Digital Twin: Enabling Technologies, Challenges and Open Research  Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins. ",2019,digital technologies,1.9249406,7.231192,2016-2020,5,#7402B1
117,Digitalising the Water Sector: Implications for Water Service Management  and Governance,"Digitalising the Water Sector: Implications for Water Service Management   and Governance  Digital technologies are becoming central to water governance and management, yet their impact and developmental implications are under-researched, particularly in the global South. This paper addresses this knowledge gap by examining the process of water service digitalisation and the resulting effects on service providers. Drawing on qualitative methods, we apply ideas on digitalisation, value, and power to investigate the implementation and impact of digital technologies in Ghana's state water utility company. We find digital water innovations to be recent, and delivering relatively limited impacts as yet, with value mainly accruing at the utility's operational rather than strategic level. The digital technologies present avenues for power shifts and struggles internally and externally as well as some changes in water management structures and responsibilities. We end with a brief discussion on the implications for water service governance and research. ",2021,digital technologies,1.8059969,7.3374195,2020-,6,#5E0897
118,Digital Library Technology for Locating and Accessing Scientific Data,"Digital Library Technology for Locating and Accessing Scientific Data  In this paper we describe our efforts to bring scientific data into the digital library. This has required extension of the standard WWW, and also the extension of metadata standards far beyond the Dublin Core. Our system demonstrates this technology for real scientific data from astronomy. ",1999,digital technologies,1.8125657,7.714712,1996-2000,1,#B2AB10
119,"Digital Twin for Non-Terrestrial Networks: Vision, Challenges, and  Enabling Technologies","Digital Twin for Non-Terrestrial Networks: Vision, Challenges, and   Enabling Technologies  The ongoing digital transformation has sparked the emergence of various new network applications that demand cutting-edge technologies to enhance their efficiency and functionality. One of the promising technologies in this direction is the digital twin, which is a new approach to design and manage complicated cyber-physical systems with a high degree of automation, intelligence, and resilience. This article discusses the use of digital twin technology as a new approach for modeling non-terrestrial networks (NTNs). Digital twin technology can create accurate data-driven NTN models that operate in real-time, allowing for rapid testing and deployment of new NTN technologies and services, besides facilitating innovation and cost reduction. Specifically, we provide a vision on integrating the digital twin into NTNs and explore the primary deployment challenges, as well as the key potential enabling technologies within NTN realm. In closing, we present a case study that employs a data-driven digital twin model for dynamic and service-oriented network slicing within an open radio access network (O-RAN) NTN architecture. ",2023,digital technologies,1.6720041,7.001458,2020-,6,#5E0897
120,Artificial intelligence-driven digital twin of a modern house  demonstrated in virtual reality,"Artificial intelligence-driven digital twin of a modern house   demonstrated in virtual reality  A digital twin is a powerful tool that can help monitor and optimize physical assets in real-time. Simply put, it is a virtual representation of a physical asset, enabled through data and simulators, that can be used for a variety of purposes such as prediction, monitoring, and decision-making. However, the concept of digital twin can be vague and difficult to understand, which is why a new concept called ""capability level"" has been introduced. This concept categorizes digital twins based on their capability and defines a scale from zero to five, with each level indicating an increasing level of functionality. These levels are standalone, descriptive, diagnostic, predictive, prescriptive, and autonomous. By understanding the capability level of a digital twin, we can better understand its potential and limitations. To demonstrate the concepts, we use a modern house as an example. The house is equipped with a range of sensors that collect data about its internal state, which can then be used to create digital twins of different capability levels. These digital twins can be visualized in virtual reality, allowing users to interact with and manipulate the virtual environment. The current work not only presents a blueprint for developing digital twins but also suggests future research directions to enhance this technology. Digital twins have the potential to transform the way we monitor and optimize physical assets, and by understanding their capabilities, we can unlock their full potential. ",2022,digital twins,1.8034744,7.2845488,2020-,6,#5E0897
121,"Digital Twin: Values, Challenges and Enablers","Digital Twin: Values, Challenges and Enablers  A digital twin can be defined as an adaptive model of a complex physical system. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders. ",2019,digital twins,1.8675663,7.173541,2016-2020,5,#7402B1
122,"Digital Twin: Enabling Technologies, Challenges and Open Research","Digital Twin: Enabling Technologies, Challenges and Open Research  Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins. ",2019,digital twins,1.9249406,7.231192,2016-2020,5,#7402B1
123,Digital Twins,"Digital Twins  Digital Twins are one of the hottest digital trends. In this contribution we will shortly review the concept of Digital Twins and the chances for novel industrial applications. Mathematics are a key enabler and the impact will be highlighted along four specific examples addressing Digital Product Twins democratizing Design, Digital Production Twins enabling robots to mill, Digital Production Twins driving industrialization of additive manufacturing, and Digital Performance Twins boosting operations. We conclude the article with an outlook on the next wave of Digital Twins, Executable Digital Twins, and will review the associated challenges and opportunities for mathematics. ",2020,digital twins,1.7402998,7.133769,2020-,6,#5E0897
124,Digital Twin as a Service (DTaaS): A Platform for Digital Twin  Developers and Users,"Digital Twin as a Service (DTaaS): A Platform for Digital Twin   Developers and Users  Establishing digital twins is a non-trivial endeavour especially when users face significant challenges in creating them from scratch. Ready availability of reusable models, data and tool assets, can help with creation and use of digital twins. A number of digital twin frameworks exist to facilitate creation and use of digital twins. In this paper we propose a digital twin framework to author digital twin assets, create digital twins from reusable assets and make the digital twins available as a service to other users. The proposed framework automates the management of reusable assets, storage, provision of compute infrastructure, communication and monitoring tasks. The users operate at the level of digital twins and delegate rest of the work to the digital twin as a service framework. ",2023,digital twins,1.7711574,7.1641116,2020-,6,#5E0897
125,TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production  Systems,"TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production   Systems  Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system. ",2020,digital twins,1.8395668,7.252539,2020-,6,#5E0897
126,"Digital Twins: Potentials, Ethical Issues, and Limitations","Digital Twins: Potentials, Ethical Issues, and Limitations  After Big Data and Artificial Intelligence (AI), the subject of Digital Twins has emerged as another promising technology, advocated, built, and sold by various IT companies. The approach aims to produce highly realistic models of real systems. In the case of dynamically changing systems, such digital twins would have a life, i.e. they would change their behaviour over time and, in perspective, take decisions like their real counterparts \textemdash so the vision. In contrast to animated avatars, however, which only imitate the behaviour of real systems, like deep fakes, digital twins aim to be accurate ""digital copies"", i.e. ""duplicates"" of reality, which may interact with reality and with their physical counterparts. This chapter explores, what are possible applications and implications, limitations, and threats. ",2022,digital twins,1.7941656,7.342728,2020-,6,#5E0897
127,Twin-S: A Digital Twin for Skull-base Surgery,"Twin-S: A Digital Twin for Skull-base Surgery  Purpose: Digital twins are virtual interactive models of the real world, exhibiting identical behavior and properties. In surgical applications, computational analysis from digital twins can be used, for example, to enhance situational awareness. Methods: We present a digital twin framework for skull-base surgeries, named Twin-S, which can be integrated within various image-guided interventions seamlessly. Twin-S combines high-precision optical tracking and real-time simulation. We rely on rigorous calibration routines to ensure that the digital twin representation precisely mimics all real-world processes. Twin-S models and tracks the critical components of skull-base surgery, including the surgical tool, patient anatomy, and surgical camera. Significantly, Twin-S updates and reflects real-world drilling of the anatomical model in frame rate. Results: We extensively evaluate the accuracy of Twin-S, which achieves an average 1.39 mm error during the drilling process. We further illustrate how segmentation masks derived from the continuously updated digital twin can augment the surgical microscope view in a mixed reality setting, where bone requiring ablation is highlighted to provide surgeons additional situational awareness. Conclusion: We present Twin-S, a digital twin environment for skull-base surgery. Twin-S tracks and updates the virtual model in real-time given measurements from modern tracking technologies. Future research on complementing optical tracking with higher-precision vision-based approaches may further increase the accuracy of Twin-S. ",2022,digital twins,3.88257,8.81099,2020-,6,#5E0897
128,Digital Twins for Patient Care via Knowledge Graphs and Closed-Form  Continuous-Time Liquid Neural Networks,"Digital Twins for Patient Care via Knowledge Graphs and Closed-Form   Continuous-Time Liquid Neural Networks  Digital twin technology has is anticipated to transform healthcare, enabling personalized medicines and support, earlier diagnoses, simulated treatment outcomes, and optimized surgical plans. Digital twins are readily gaining traction in industries like manufacturing, supply chain logistics, and civil infrastructure. Not in patient care, however. The challenge of modeling complex diseases with multimodal patient data and the computational complexities of analyzing it have stifled digital twin adoption in the biomedical vertical. Yet, these major obstacles can potentially be handled by approaching these models in a different way. This paper proposes a novel framework for addressing the barriers to clinical twin modeling created by computational costs and modeling complexities. We propose structuring patient health data as a knowledge graph and using closed-form continuous-time liquid neural networks, for real-time analytics. By synthesizing multimodal patient data and leveraging the flexibility and efficiency of closed form continuous time networks and knowledge graph ontologies, our approach enables real time insights, personalized medicine, early diagnosis and intervention, and optimal surgical planning. This novel approach provides a comprehensive and adaptable view of patient health along with real-time analytics, paving the way for digital twin simulations and other anticipated benefits in healthcare. ",2023,digital twins,3.7000265,7.8395844,2020-,6,#5E0897
129,"Revisiting Digital Twins: Origins, Fundamentals and Practices","Revisiting Digital Twins: Origins, Fundamentals and Practices  The Digital Twins (DT) has quickly become a hot topic since it was proposed. It not only appears in all kinds of commercial propaganda, but also is widely quoted by academic circles. However, there are misstatements and misuse of the term DT in business and academy. This paper revisits Digital Twins and defines it to be a more advanced system/product/service modelling and simulation environment that combines the most modern Information Communication Technology (ICTs) and engineering mechanisms digitization, and characterized by system/product/service life cycle management, physically geometric visualization, real-time sensing and measurement of system operating conditions, predictability of system performance/safety/lifespan, complete engineering mechanisms-based simulations. The idea of Digital Twins originates from modelling and simulation practices of engineering informatization, including Virtual Manufacturing (VM), Model Predictive Control (MPC), and Building Information Model (BIM). Based on the two-element VM model, we propose a three-element model to represent Digital Twins. Digital Twins does not have its own unique technical characteristics; the existing practices of Digital Twins are extensions of the engineering informatization embracing modern ICTs. These insights clarify the origin of Digital Twins and its technical essentials. ",2022,digital twins,1.8085738,7.1234097,2020-,6,#5E0897
130,DisplaceNet: Recognising Displaced People from Images by Exploiting  Dominance Level,"DisplaceNet: Recognising Displaced People from Images by Exploiting   Dominance Level  Every year millions of men, women and children are forced to leave their homes and seek refuge from wars, human rights violations, persecution, and natural disasters. The number of forcibly displaced people came at a record rate of 44,400 every day throughout 2017, raising the cumulative total to 68.5 million at the years end, overtaken the total population of the United Kingdom. Up to 85% of the forcibly displaced find refuge in low- and middle-income countries, calling for increased humanitarian assistance worldwide. To reduce the amount of manual labour required for human-rights-related image analysis, we introduce DisplaceNet, a novel model which infers potential displaced people from images by integrating the control level of the situation and conventional convolutional neural network (CNN) classifier into one framework for image classification. Experimental results show that DisplaceNet achieves up to 4% coverage-the proportion of a data set for which a classifier is able to produce a prediction-gain over the sole use of a CNN classifier. Our dataset, codes and trained models will be available online at https://github.com/GKalliatakis/DisplaceNet. ",2019,displaced,2.0031657,7.954762,2016-2020,5,#7402B1
131,Experimental creation and analysis of displaced number states,"Experimental creation and analysis of displaced number states  We create displaced number states, which are nonclassical generalizations of coherent states, of a vibrational mode of a single trapped ion. ",2012,displaced,6.6577663,3.5950303,2011-2015,4,#9100CF
132,The displaced disks problem via symplectic topology,"The displaced disks problem via symplectic topology  We prove that a $C^0$--small area preserving homeomorphism of a closed surface with vanishing mass flow can not displace a topological disk of large area. This resolves the displaced disks problem posed by F. B\'eguin, S. Crovisier, and F. Le Roux. ",2013,displaced,5.0014734,2.1121716,2011-2015,4,#9100CF
133,Refugee Resettlement Housing Scout,"Refugee Resettlement Housing Scout  According to the United States High Commission for Refugees (UNHCr), there are 65.3 million forcibly displaced people in the world today, 21.5 million of them being refugees. This has led to an unprecedented refugee crisis which has led countries to accept refugee families and to resettle them. Diverse agencies are helping refugees coming to US to resettle and start their new life in the country. One of the first and most challenging steps of this process is to find affordable housing that also meets a suite of additional constraints and priorities. These include being within a mile of public transportation and near schools, faith centers and international grocery stores. We detail an interactive data-driven web-based tool, which incorporates in one consolidated platform most of the needed information. The tool searches, filters and demonstrates a list of possible housing locations, and allows for the dynamic prioritization based on user-specified importance weights on the diverse criteria. The platform was created in a partnership with New American Pathways, a nonprofit that supports refugee resettlement in the metro Atlanta, but exemplifies a methodology that can help many other organizations with similar goals. ",2016,displaced,2.0131733,7.5768538,2016-2020,5,#7402B1
134,Heterogeneous Effects of Job Displacement on Earnings,"Heterogeneous Effects of Job Displacement on Earnings  This paper considers how the effect of job displacement varies across different individuals. In particular, our interest centers on features of the distribution of the individual-level effect of job displacement. Identifying features of this distribution is particularly challenging -- e.g., even if we could randomly assign workers to be displaced or not, many of the parameters that we consider would not be point identified. We exploit our access to panel data, and our approach relies on comparing outcomes of displaced workers to outcomes the same workers would have experienced if they had not been displaced and if they maintained the same rank in the distribution of earnings as they had before they were displaced. Using data from the Displaced Workers Survey, we find that displaced workers earn about $157 per week less, on average, than they would have earned if they had not been displaced. We also find that there is substantial heterogeneity. We estimate that 42% of workers have higher earnings than they would have had if they had not been displaced and that a large fraction of workers have experienced substantially more negative effects than the average effect of displacement. Finally, we also document major differences in the distribution of the effect of job displacement across education levels, sex, age, and counterfactual earnings levels. Throughout the paper, we rely heavily on quantile regression. First, we use quantile regression as a flexible (yet feasible) first step estimator of conditional distributions and quantile functions that our main results build on. We also use quantile regression to study how covariates affect the distribution of the individual-level effect of job displacement. ",2020,displaced,4.020349,6.4436126,2020-,6,#5E0897
135,Estimating Displaced Populations from Overhead,"Estimating Displaced Populations from Overhead  We introduce a deep learning approach to perform fine-grained population estimation for displacement camps using high-resolution overhead imagery. We train and evaluate our approach on drone imagery cross-referenced with population data for refugee camps in Cox's Bazar, Bangladesh in 2018 and 2019. Our proposed approach achieves 7.02% mean absolute percent error on sequestered camp imagery. We believe our experiments with real-world displacement camp data constitute an important step towards the development of tools that enable the humanitarian community to effectively and rapidly respond to the global displacement crisis. ",2020,displaced,3.5988674,7.5075293,2020-,6,#5E0897
136,What is the impact of labor displacement on management consulting  services?,"What is the impact of labor displacement on management consulting   services?  Labor displacement off-or nearshore is a performance improvement instrument that currently sparks a lot of interest in the service sector. This article proposes a model to understand the consequences of such a decision on management consulting firms. Its calibration on the market of consulting services for the German transportation industry highlights that, under realistic assumptions, labor displacement translates in price decrease by-0.5% on average per year and that for MC practices to remain competitive/profitable they have to at least increase the amount of work they off/nears shore by +0.7% a year. ",2020,displaced,2.531546,6.819615,2020-,6,#5E0897
137,"The Effects of Inequality, Density, and Heterogeneous Residential  Preferences on Urban Displacement and Metropolitan Structure: An Agent-Based  Model","The Effects of Inequality, Density, and Heterogeneous Residential   Preferences on Urban Displacement and Metropolitan Structure: An Agent-Based   Model  Urban displacement - when a household is forced to relocate due to conditions affecting its home or surroundings - often results from rising housing costs, particularly in wealthy, prosperous cities. However, its dynamics are complex and often difficult to understand. This paper presents an agent-based model of urban settlement, agglomeration, displacement, and sprawl. New settlements form around a spatial amenity that draws initial, poor settlers to subsist on the resource. As the settlement grows, subsequent settlers of varying income, skills, and interests are heterogeneously drawn to either the original amenity or to the emerging human agglomeration. As this agglomeration grows and densifies, land values increase, and the initial poor settlers are displaced from the spatial amenity on which they relied. Through path dependence, high-income residents remain clustered around this original amenity for which they have no direct use or interest. This toy model explores these dynamics, demonstrating a simplified mechanism of how urban displacement and gentrification can be sensitive to income inequality, density, and varied preferences for different types of amenities. ",2017,displaced,3.8867986,6.727484,2016-2020,5,#7402B1
138,"The Heterogeneous Earnings Impact of Job Loss Across Workers,  Establishments, and Markets","The Heterogeneous Earnings Impact of Job Loss Across Workers,   Establishments, and Markets  Using generalized random forests and rich Swedish administrative data, we show that the earnings effects of job displacement due to establishment closures are extremely heterogeneous across workers, establishments, and markets. The decile of workers with the largest predicted effects lose 50 percent of annual earnings the year after displacement and accumulated losses amount to 250 percent during a decade. In contrast, workers in the least affected decile experience only marginal losses of less than 6 percent in the year after displacement. Workers in the most affected decile tend to be lower paid workers on negative earnings trajectories. This implies that the economic value of (lost) jobs is greatest for workers with low earnings. The reason is that many of these workers fail to find new employment after displacement. Overall, the effects are heterogeneous both within and across establishments and combinations of important individual characteristics such as age and schooling. Adverse market conditions matter the most for already vulnerable workers. The most effective way to target workers with large effects, without using a complex model, is by focusing on older workers in routine-task intensive jobs ",2023,displaced,3.4080913,6.041541,2020-,6,#5E0897
139,Displacement Data Assimilation,"Displacement Data Assimilation  We show that modifying a Bayesian data assimilation scheme by incorporating kinematically-consistent displacement corrections produces a scheme that is demonstrably better at estimating partially observed state vectors in a setting where feature information important. While the displacement transformation is not tied to any particular assimilation scheme, here we implement it within an ensemble Kalman Filter and demonstrate its effectiveness in tracking stochastically perturbed vortices. ",2016,displaced,4.040537,7.257074,2016-2020,5,#7402B1
140,A Civil Protection Early Warning System to Improve the Resilience of  Adriatic-Ionian Territories to Natural and Man-made Risk,"A Civil Protection Early Warning System to Improve the Resilience of   Adriatic-Ionian Territories to Natural and Man-made Risk  We are currently witnessing an increased occurrence of extreme weather events, causing a great deal of disruption and distress across the globe. In this setting, the importance and utility of Early Warning Systems is becoming increasingly obvious. In this work, we present the design of an early warning system called TransCPEarlyWarning, aimed at seven countries in the Adriatic-Ionian area in Europe. The overall objective is to increase the level of cooperation among national civil protection institutions in these countries, addressing natural and man-made risks from the early warning stage and improving the intervention capabilities of civil protection mechanisms. The system utilizes an innovative approach with a lever effect, while also aiming to support the whole system of Civil Protection. ",2022,typhoon early warning system,3.143402,7.0104017,2020-,6,#5E0897
141,"""Shaking in 5 seconds!"" A Voluntary Smartphone-based Earthquake Early  Warning System","""Shaking in 5 seconds!"" A Voluntary Smartphone-based Earthquake Early   Warning System  Public earthquake early warning systems have the potential to reduce individual risk by warning people of an incoming tremor but their development has been hampered by costly infrastructure. Furthermore, users' understanding of such a service and their reactions to warnings remains poorly studied. The smartphone app of the Earthquake Network initiative turns users' smartphones into motion detectors and provides the first example of purely smartphone-based earthquake early warnings, without the need for dedicated seismic station infrastructure and operating in multiple countries. We demonstrate here that early warnings have been emitted in multiple countries even for damaging shaking levels and so this offers an alternative in the many regions unlikely to be covered by conventional early warning systems in the foreseeable future. We also show that although warnings are understood and appreciated by users, notably to get psychologically prepared, only a fraction take protective actions such as ""drop, cover and hold"". ",2021,typhoon early warning system,5.255776,6.4506927,2020-,6,#5E0897
142,"Real-time Earthquake Early Warning with Deep Learning: Application to  the 2016 Central Apennines, Italy Earthquake Sequence","Real-time Earthquake Early Warning with Deep Learning: Application to   the 2016 Central Apennines, Italy Earthquake Sequence  Earthquake early warning systems are required to report earthquake locations and magnitudes as quickly as possible before the damaging S wave arrival to mitigate seismic hazards. Deep learning techniques provide potential for extracting earthquake source information from full seismic waveforms instead of seismic phase picks. We developed a novel deep learning earthquake early warning system that utilizes fully convolutional networks to simultaneously detect earthquakes and estimate their source parameters from continuous seismic waveform streams. The system determines earthquake location and magnitude as soon as one station receives earthquake signals and evolutionarily improves the solutions by receiving continuous data. We apply the system to the 2016 Mw 6.0 earthquake in Central Apennines, Italy and its subsequent sequence. Earthquake locations and magnitudes can be reliably determined as early as four seconds after the earliest P phase, with mean error ranges of 6.8-3.7 km and 0.31-0.23, respectively. ",2020,typhoon early warning system,5.2871943,6.511113,2020-,6,#5E0897
143,Towards the Development of a Rule-based Drought Early Warning Expert  Systems using Indigenous Knowledge,"Towards the Development of a Rule-based Drought Early Warning Expert   Systems using Indigenous Knowledge  Drought forecasting and prediction is a complicated process due to the complexity and scalability of the environmental parameters involved. Hence, it required a high level of expertise to predict. In this paper, we describe the research and development of a rule-based drought early warning expert systems (RB-DEWES) for forecasting drought using local indigenous knowledge obtained from domain experts. The system generates inference by using rule set and provides drought advisory information with attributed certainty factor (CF) based on the user's input. The system is believed to be the first expert system for drought forecasting to use local indigenous knowledge on drought. The architecture and components such as knowledge base, JESS inference engine and model base of the system and their functions are presented. ",2018,typhoon early warning system,3.5973666,7.206671,2016-2020,5,#7402B1
144,The transformer earthquake alerting model: A new versatile approach to  earthquake early warning,"The transformer earthquake alerting model: A new versatile approach to   earthquake early warning  Earthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance notice of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts, and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer from apparent aleatoric uncertainties due to simplified modelling or short warning times. Here we propose a novel early warning method, the deep-learning based transformer earthquake alerting model (TEAM), to mitigate these limitations. TEAM analyzes raw, strong motion waveforms of an arbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing seismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard, Japan and Italy, that are complementary in their seismicity. On both datasets TEAM outperforms existing early warning methods considerably, offering accurate and timely warnings. Using domain adaptation, TEAM even provides reliable alerts for events larger than any in the training data, a property of highest importance as records from very large events are rare in many regions. ",2020,typhoon early warning system,5.2530956,6.5263486,2020-,6,#5E0897
145,A simulation framework for statistical inference on the alerting  capabilities of smartphone-based earthquake early warning systems. With a  case study on the Earthquake Network system in Haiti,"A simulation framework for statistical inference on the alerting   capabilities of smartphone-based earthquake early warning systems. With a   case study on the Earthquake Network system in Haiti  Smartphone-based earthquake early warning systems implemented by citizen science initiatives are characterized by a significant variability in their smartphone network geometry. This has an direct impact on the earthquake detection capability and performance of the system. Here, a simulation framework based on the Monte Carlo method is implemented for making inference on relevant quantities of the earthquake detection such as the detection distance from the epicentre, the detection delay and the warning time for people exposed to high ground shaking levels. The framework is applied to Haiti, which has experienced deadly earthquakes in the past decades, and to the network of the Earthquake Network citizen science initiative, which is popular in the country. It is discovered that relatively low penetrations of the initiative among the population allow to offer a robust early warning service, with warning times up to 12 second for people exposed to intensities between 7.5 and 8.5 of the modified Mercalli scale. ",2022,typhoon early warning system,5.2808642,6.460625,2020-,6,#5E0897
146,Survival modelling of smartphone trigger data for earthquake parameter  estimation in early warning. With applications to 2023 Turkish-Syrian and  2019 Ridgecrest events,"Survival modelling of smartphone trigger data for earthquake parameter   estimation in early warning. With applications to 2023 Turkish-Syrian and   2019 Ridgecrest events  Crowdsourced smartphone-based earthquake early warning systems recently emerged as reliable alternatives to the more expensive solutions based on scientific-grade instruments. For instance, during the 2023 Turkish-Syrian deadly event, the system implemented by the Earthquake Network citizen science initiative provided a forewarning up to 25 seconds. We develop a statistical methodology based on a survival mixture cure model which provides full Bayesian inference on epicentre, depth and origin time, and we design an efficient tempering MCMC algorithm to address multi-modality of the posterior distribution. The methodology is applied to data collected by the Earthquake Network, including the 2023 Turkish-Syrian and 2019 Ridgecrest events. ",2023,typhoon early warning system,4.5832663,6.66151,2020-,6,#5E0897
147,Early warning signal for interior crises in excitable systems,"Early warning signal for interior crises in excitable systems  The ability to reliably predict critical transitions in dynamical systems is a long-standing goal of diverse scientific communities. Previous work focused on early warning signals related to local bifurcations (critical slowing down) and non-bifurcation type transitions. We extend this toolbox and report on a characteristic scaling behavior (critical attractor growth) which is indicative of an impending global bifurcation, an interior crisis in excitable systems. We demonstrate our early warning signal in a conceptual climate model as well as in a model of coupled neurons known to exhibit extreme events. We observed critical attractor growth prior to interior crises of chaotic as well as strange-nonchaotic attractors. These observations promise to extend the classes of transitions that can be predicted via early warning signals. ",2017,typhoon early warning system,5.277716,5.9195886,2016-2020,5,#7402B1
148,An Early Warning System for Bankruptcy Prediction: lessons from the  Venezuelan Bank Crisis,An Early Warning System for Bankruptcy Prediction: lessons from the   Venezuelan Bank Crisis  During 1993-94 Venezuela experienced a severe banking crisis which ended up with 18 commercial banks intervened by the government. Here we develop an early warning system for detecting credit related bankruptcy through discriminant functions developed on financial and macroeconomic data predating the crisis. A robustness test performed on these functions shows high precision in error estimation. The model calibrated on pre-crisis data could detect abnormal financial tension in the late Banco Capital many months before it was intervened and liquidated. ,2007,typhoon early warning system,3.3510811,6.111467,2006-2010,3,#FEF65C
149,SNEWS: The SuperNova Early Warning System,"SNEWS: The SuperNova Early Warning System  World-wide, several detectors currently running or nearing completion are sensitive to a prompt core collapse supernova neutrino signal in the Galaxy. The SNEWS system will be able to provide a robust early warning of a supernova's occurrence to the astronomical community using a coincidence of neutrino signals around the world. This talk describes the nature of the neutrino signal, detection techniques and the motivation for a coincidence alert. It describes the implementation of SNEWS, its current status, and its future, which can include gravitational wave detectors. ",1999,typhoon early warning system,11.118874,6.504406,1996-2000,1,#B2AB10
150,Flood Prediction Using Machine Learning Models,"Flood Prediction Using Machine Learning Models  Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy. ",2022,flooding,4.1629286,6.9495845,2020-,6,#5E0897
151,On Complexity of Flooding Games on Graphs with Interval Representations,"On Complexity of Flooding Games on Graphs with Interval Representations  The flooding games, which are called Flood-It, Mad Virus, or HoneyBee, are a kind of coloring games and they have been becoming popular online. In these games, each player colors one specified cell in his/her turn, and all connected neighbor cells of the same color are also colored by the color. This flooding or coloring spreads on the same color cells. It is natural to consider these new coloring games on more general boards, or general graphs. Recently, computational complexities of the variants of the flooding games on several graph classes have been studied. In this paper, we investigate the flooding games on some graph classes characterized by interval representations. Our results state that the number of colors is a key parameter to determine the computational complexity of the flooding games. When the number of colors is a fixed constant, these games can be solved in polynomial time on an interval graph. On the other hand, if the number of colors is not bounded, the flooding game is NP-complete on a proper interval graph. We also state similar results for split graphs. ",2012,flooding,2.9533195,3.7105772,2011-2015,4,#9100CF
152,Exploiting ConvNet Diversity for Flooding Identification,"Exploiting ConvNet Diversity for Flooding Identification  Flooding is the world's most costly type of natural disaster in terms of both economic losses and human causalities. A first and essential procedure towards flood monitoring is based on identifying the area most vulnerable to flooding, which gives authorities relevant regions to focus. In this work, we propose several methods to perform flooding identification in high-resolution remote sensing images using deep learning. Specifically, some proposed techniques are based upon unique networks, such as dilated and deconvolutional ones, while other was conceived to exploit diversity of distinct networks in order to extract the maximum performance of each classifier. Evaluation of the proposed algorithms were conducted in a high-resolution remote sensing dataset. Results show that the proposed algorithms outperformed several state-of-the-art baselines, providing improvements ranging from 1 to 4% in terms of the Jaccard Index. ",2017,flooding,3.484325,7.8556933,2016-2020,5,#7402B1
153,Numerical Simulations of Polymer Flooding Process in Porous Media on  Distributed-memory Parallel Computers,"Numerical Simulations of Polymer Flooding Process in Porous Media on   Distributed-memory Parallel Computers  Polymer flooding is a mature enhanced oil recovery technique that has been successfully applied in many field projects. By injecting polymer into a reservoir, the viscosity of water is increased, and the efficiency of water flooding is improved. As a result, more oil can be recovered. This paper presents numerical simulations of a polymer flooding process using parallel computers, where the numerical modeling of polymer retention, inaccessible pore volumes, a permeability reduction and polymer absorption are considered. Darcy's law is employed to model the behavoir of a fluid in porous media, and the upstream finite difference (volume) method is applied to discretize the mass conservation equations. Numerical methods, including discretization schemes, linear solver methods, nonlinearization methods and parallel techniques are introduced. Numerical experiments show that, on one hand, computed results match those from the commercial simulator, Schlumberger-Eclipse, which is widely applied by the petroleum industry, and, on the other hand, our simulator has excellent scalability, which is demonstrated by field applications with up to 27 million grid blocks using up to 2048 CPU cores. ",2018,flooding,5.632527,5.135289,2016-2020,5,#7402B1
154,Flood Routing Technique for Data Networks,"Flood Routing Technique for Data Networks  In this paper, a new routing algorithm based on a flooding method is introduced. Flooding techniques have been used previously, e.g. for broadcasting the routing table in the ARPAnet [1] and other special purpose networks [3][4][5]. However, sending data using flooding can often saturate the network [2] and it is usually regarded as an inefficient broadcast mechanism. Our approach is to flood a very short packet to explore an optimal route without relying on a pre-established routing table, and an efficient flood control algorithm to reduce the signalling traffic overhead. This is an inherently robust mechanism in the face of a network configuration change, achieves automatic load sharing across alternative routes and has potential to solve many contemporary routing problems. An earlier version of this mechanism was originally developed for virtual circuit establishment in the experimental Caroline ATM LAN [6][7] at Monash University. ",1998,flooding,2.1650753,5.8896956,1996-2000,1,#B2AB10
155,Local impacts on road networks and access to critical locations during  extreme floods,"Local impacts on road networks and access to critical locations during   extreme floods  Floods affected more than 2 billion people worldwide from 1998 to 2017 and their occurrence is expected to increase due to climate warming, population growth and rapid urbanization. Recent approaches for understanding the resilience of transportation networks when facing floods mostly use the framework of percolation but we show here on a realistic high-resolution flood simulation that it is inadequate. Indeed, the giant connected component is not relevant and instead, we propose to partition the road network in terms of accessibility of local towns and define new measures that characterize the impact of the flooding event. Our analysis allows to identify cities that will be pivotal during the flooding by providing to a large number of individuals critical services such as hospitalization services, food supply, etc. This approach is particularly relevant for practical risk management and will help decision makers for allocating resources in space and time. ",2022,flooding,4.1036963,6.9595823,2020-,6,#5E0897
156,Global Flood Prediction: a Multimodal Machine Learning Approach,"Global Flood Prediction: a Multimodal Machine Learning Approach  Flooding is one of the most destructive and costly natural disasters, and climate changes would further increase risks globally. This work presents a novel multimodal machine learning approach for multi-year global flood risk prediction, combining geographical information and historical natural disaster dataset. Our multimodal framework employs state-of-the-art processing techniques to extract embeddings from each data modality, including text-based geographical data and tabular-based time-series data. Experiments demonstrate that a multimodal approach, that is combining text and statistical data, outperforms a single-modality approach. Our most advanced architecture, employing embeddings extracted using transfer learning upon DistilBert model, achieves 75\%-77\% ROCAUC score in predicting the next 1-5 year flooding event in historically flooded locations. This work demonstrates the potentials of using machine learning for long-term planning in natural disaster management. ",2023,flooding,4.0601277,6.979293,2020-,6,#5E0897
157,Physics Informed Data Driven model for Flood Prediction: Application of  Deep Learning in prediction of urban flood development,"Physics Informed Data Driven model for Flood Prediction: Application of   Deep Learning in prediction of urban flood development  Flash floods in urban areas occur with increasing frequency. Detecting these floods would greatlyhelp alleviate human and economic losses. However, current flood prediction methods are eithertoo slow or too simplified to capture the flood development in details. Using Deep Neural Networks,this work aims at boosting the computational speed of a physics-based 2-D urban flood predictionmethod, governed by the Shallow Water Equation (SWE). Convolutional Neural Networks(CNN)and conditional Generative Adversarial Neural Networks(cGANs) are applied to extract the dy-namics of flood from the data simulated by a Partial Differential Equation(PDE) solver. Theperformance of the data-driven model is evaluated in terms of Mean Squared Error(MSE) andPeak Signal to Noise Ratio(PSNR). The deep learning-based, data-driven flood prediction modelis shown to be able to provide precise real-time predictions of flood development ",2019,flooding,4.337165,6.902153,2016-2020,5,#7402B1
158,"FlowDB a large scale precipitation, river, and flash flood dataset","FlowDB a large scale precipitation, river, and flash flood dataset  Flooding results in 8 billion dollars of damage annually in the US and causes the most deaths of any weather related event. Due to climate change scientists expect more heavy precipitation events in the future. However, no current datasets exist that contain both hourly precipitation and river flow data. We introduce a novel hourly river flow and precipitation dataset and a second subset of flash flood events with damage estimates and injury counts. Using these datasets we create two challenges (1) general stream flow forecasting and (2) flash flood damage estimation. We have created several publicly available benchmarks and an easy to use package. Additionally, in the future we aim to augment our dataset with snow pack data and soil index moisture data to improve predictions. ",2020,flooding,4.158283,7.092652,2020-,6,#5E0897
159,The ecological impact of flooding: a study of tree damage,"The ecological impact of flooding: a study of tree damage  The objective of this research was to identify factors affecting tree damage in the historical Minot flood of 2011. We hypothesized that tree height, identity, origin, and maximum water height affect in the severity of damage sustained by a tree in a flood event. All these factors were significant but highly interactive. The results from this research can influence planting practices in valleys and other flood prone areas to mitigate future damage. ",2015,flooding,3.8425713,6.627114,2016-2020,5,#7402B1
160,Wildfire Prediction to Inform Fire Management: Statistical Science  Challenges,"Wildfire Prediction to Inform Fire Management: Statistical Science   Challenges  Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales. ",2013,wildfire,4.313369,7.0253096,2011-2015,4,#9100CF
161,Image-based Early Detection System for Wildfires,"Image-based Early Detection System for Wildfires  Wildfires are a disastrous phenomenon which cause damage to land, loss of property, air pollution, and even loss of human life. Due to the warmer and drier conditions created by climate change, more severe and uncontrollable wildfires are expected to occur in the coming years. This could lead to a global wildfire crisis and have dire consequences on our planet. Hence, it has become imperative to use technology to help prevent the spread of wildfires. One way to prevent the spread of wildfires before they become too large is to perform early detection i.e, detecting the smoke before the actual fire starts. In this paper, we present our Wildfire Detection and Alert System which use machine learning to detect wildfire smoke with a high degree of accuracy and can send immediate alerts to users. Our technology is currently being used in the USA to monitor data coming in from hundreds of cameras daily. We show that our system has a high true detection rate and a low false detection rate. Our performance evaluation study also shows that on an average our system detects wildfire smoke faster than an actual person. ",2022,wildfire,3.4636755,7.844294,2020-,6,#5E0897
162,Mitigating Greenhouse Gas Emissions Through Generative Adversarial  Networks Based Wildfire Prediction,"Mitigating Greenhouse Gas Emissions Through Generative Adversarial   Networks Based Wildfire Prediction  Over the past decade, the number of wildfire has increased significantly around the world, especially in the State of California. The high-level concentration of greenhouse gas (GHG) emitted by wildfires aggravates global warming that further increases the risk of more fires. Therefore, an accurate prediction of wildfire occurrence greatly helps in preventing large-scale and long-lasting wildfires and reducing the consequent GHG emissions. Various methods have been explored for wildfire risk prediction. However, the complex correlations among a lot of natural and human factors and wildfire ignition make the prediction task very challenging. In this paper, we develop a deep learning based data augmentation approach for wildfire risk prediction. We build a dataset consisting of diverse features responsible for fire ignition and utilize a conditional tabular generative adversarial network to explore the underlying patterns between the target value of risk levels and all involved features. For fair and comprehensive comparisons, we compare our proposed scheme with five other baseline methods where the former outperformed most of them. To corroborate the robustness, we have also tested the performance of our method with another dataset that also resulted in better efficiency. By adopting the proposed method, we can take preventive strategies of wildfire mitigation to reduce global GHG emissions. ",2021,wildfire,4.123949,7.2447534,2020-,6,#5E0897
163,A wildland fire model with data assimilation,"A wildland fire model with data assimilation  A wildfire model is formulated based on balance equations for energy and fuel, where the fuel loss due to combustion corresponds to the fuel reaction rate. The resulting coupled partial differential equations have coefficients that can be approximated from prior measurements of wildfires. An ensemble Kalman filter technique with regularization is then used to assimilate temperatures measured at selected points into running wildfire simulations. The assimilation technique is able to modify the simulations to track the measurements correctly even if the simulations were started with an erroneous ignition location that is quite far away from the correct one. ",2007,wildfire,4.782197,6.6583037,2006-2010,3,#FEF65C
164,A Multimodal Supervised Machine Learning Approach for Satellite-based  Wildfire Identification in Europe,"A Multimodal Supervised Machine Learning Approach for Satellite-based   Wildfire Identification in Europe  The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results demonstrate the effectiveness of our approach in the task of wildfire identification. ",2023,wildfire,3.931474,7.617138,2020-,6,#5E0897
165,Analytical and numerical insights into wildfire dynamics: Exploring the  advection-diffusion-reaction model,"Analytical and numerical insights into wildfire dynamics: Exploring the   advection-diffusion-reaction model  Understanding the dynamics of wildfire is crucial for developing management and intervention strategies. Mathematical and computational models can be used to improve our understanding of wildfire processes and dynamics. This paper presents a systematic study of a widely used advection-diffusion-reaction wildfire model with non-linear coupling. The importance of single mechanisms is discovered by analysing hierarchical sub-models. Numerical simulations provide further insight into the dynamics. As a result, the influence of wind and model parameters such as the bulk density or the heating value on the wildfire propagation speed and the remaining biomass after the burn are assessed. Linearisation techniques for a reduced model provide surprisingly good estimates for the propagation speed in the full model. ",2023,wildfire,5.9689336,6.0140047,2020-,6,#5E0897
166,A Multi-Modal Wildfire Prediction and Personalized Early-Warning System  Based on a Novel Machine Learning Framework,"A Multi-Modal Wildfire Prediction and Personalized Early-Warning System   Based on a Novel Machine Learning Framework  Wildfires are increasingly impacting the environment, human health and safety. Among the top 20 California wildfires, those in 2020-2021 burned more acres than the last century combined. California's 2018 wildfire season caused damages of $148.5 billion. Among millions of impacted people, those living with disabilities (around 15% of the world population) are disproportionately impacted due to inadequate means of alerts. In this project, a multi-modal wildfire prediction and personalized early warning system has been developed based on an advanced machine learning architecture. Sensor data from the Environmental Protection Agency and historical wildfire data from 2012 to 2018 have been compiled to establish a comprehensive wildfire database, the largest of its kind. Next, a novel U-Convolutional-LSTM (Long Short-Term Memory) neural network was designed with a special architecture for extracting key spatial and temporal features from contiguous environmental parameters indicative of impending wildfires. Environmental and meteorological factors were incorporated into the database and classified as leading indicators and trailing indicators, correlated to risks of wildfire conception and propagation respectively. Additionally, geological data was used to provide better wildfire risk assessment. This novel spatio-temporal neural network achieved >97% accuracy vs. around 76% using traditional convolutional neural networks, successfully predicting 2018's five most devastating wildfires 5-14 days in advance. Finally, a personalized early warning system, tailored to individuals with sensory disabilities or respiratory exacerbation conditions, was proposed. This technique would enable fire departments to anticipate and prevent wildfires before they strike and provide early warnings for at-risk individuals for better preparation, thereby saving lives and reducing economic damages. ",2022,wildfire,4.044775,7.5373993,2020-,6,#5E0897
167,Aerial Imagery Pile burn detection using Deep Learning: the FLAME  dataset,"Aerial Imagery Pile burn detection using Deep Learning: the FLAME   dataset  Wildfires are one of the costliest and deadliest natural disasters in the US, causing damage to millions of hectares of forest resources and threatening the lives of people and animals. Of particular importance are risks to firefighters and operational forces, which highlights the need for leveraging technology to minimize danger to people and property. FLAME (Fire Luminosity Airborne-based Machine learning Evaluation) offers a dataset of aerial images of fires along with methods for fire detection and segmentation which can help firefighters and researchers to develop optimal fire management strategies. This paper provides a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms. The paper also highlights solutions to two machine learning problems: (1) Binary classification of video frames based on the presence [and absence] of fire flames. An Artificial Neural Network (ANN) method is developed that achieved a 76% classification accuracy. (2) Fire detection using segmentation methods to precisely determine fire borders. A deep learning method is designed based on the U-Net up-sampling and down-sampling approach to extract a fire mask from the video frames. Our FLAME method approached a precision of 92% and a recall of 84%. Future research will expand the technique for free burning broadcast fire using thermal images. ",2020,wildfire,3.8199792,7.8017254,2020-,6,#5E0897
168,Uncertainty Aware Wildfire Management,"Uncertainty Aware Wildfire Management  Recent wildfires in the United States have resulted in loss of life and billions of dollars, destroying countless structures and forests. Fighting wildfires is extremely complex. It is difficult to observe the true state of fires due to smoke and risk associated with ground surveillance. There are limited resources to be deployed over a massive area and the spread of the fire is challenging to predict. This paper proposes a decision-theoretic approach to combat wildfires. We model the resource allocation problem as a partially-observable Markov decision process. We also present a data-driven model that lets us simulate how fires spread as a function of relevant covariates. A major problem in using data-driven models to combat wildfires is the lack of comprehensive data sources that relate fires with relevant covariates. We present an algorithmic approach based on large-scale raster and vector analysis that can be used to create such a dataset. Our data with over 2 million data points is the first open-source dataset that combines existing fire databases with covariates extracted from satellite imagery. Through experiments using real-world wildfire data, we demonstrate that our forecasting model can accurately model the spread of wildfires. Finally, we use simulations to demonstrate that our response strategy can significantly reduce response times compared to baseline methods. ",2020,wildfire,4.3807707,7.2853494,2020-,6,#5E0897
169,Multimodal Wildland Fire Smoke Detection,"Multimodal Wildland Fire Smoke Detection  Research has shown that climate change creates warmer temperatures and drier conditions, leading to longer wildfire seasons and increased wildfire risks in the United States. These factors have in turn led to increases in the frequency, extent, and severity of wildfires in recent years. Given the danger posed by wildland fires to people, property, wildlife, and the environment, there is an urgency to provide tools for effective wildfire management. Early detection of wildfires is essential to minimizing potentially catastrophic destruction. In this paper, we present our work on integrating multiple data sources in SmokeyNet, a deep learning model using spatio-temporal information to detect smoke from wildland fires. Camera image data is integrated with weather sensor measurements and processed by SmokeyNet to create a multimodal wildland fire smoke detection system. We present our results comparing performance in terms of both accuracy and time-to-detection for multimodal data vs. a single data source. With a time-to-detection of only a few minutes, SmokeyNet can serve as an automated early notification system, providing a useful tool in the fight against destructive wildfires. ",2022,wildfire,4.047656,7.689841,2020-,6,#5E0897
170,Artificial intelligence-driven digital twin of a modern house  demonstrated in virtual reality,"Artificial intelligence-driven digital twin of a modern house   demonstrated in virtual reality  A digital twin is a powerful tool that can help monitor and optimize physical assets in real-time. Simply put, it is a virtual representation of a physical asset, enabled through data and simulators, that can be used for a variety of purposes such as prediction, monitoring, and decision-making. However, the concept of digital twin can be vague and difficult to understand, which is why a new concept called ""capability level"" has been introduced. This concept categorizes digital twins based on their capability and defines a scale from zero to five, with each level indicating an increasing level of functionality. These levels are standalone, descriptive, diagnostic, predictive, prescriptive, and autonomous. By understanding the capability level of a digital twin, we can better understand its potential and limitations. To demonstrate the concepts, we use a modern house as an example. The house is equipped with a range of sensors that collect data about its internal state, which can then be used to create digital twins of different capability levels. These digital twins can be visualized in virtual reality, allowing users to interact with and manipulate the virtual environment. The current work not only presents a blueprint for developing digital twins but also suggests future research directions to enhance this technology. Digital twins have the potential to transform the way we monitor and optimize physical assets, and by understanding their capabilities, we can unlock their full potential. ",2022,digital twin,1.8034744,7.2845488,2020-,6,#5E0897
171,"Digital Twin: Values, Challenges and Enablers","Digital Twin: Values, Challenges and Enablers  A digital twin can be defined as an adaptive model of a complex physical system. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders. ",2019,digital twin,1.8675663,7.173541,2016-2020,5,#7402B1
172,"Digital Twin: Enabling Technologies, Challenges and Open Research","Digital Twin: Enabling Technologies, Challenges and Open Research  Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins. ",2019,digital twin,1.9249406,7.231192,2016-2020,5,#7402B1
173,TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production  Systems,"TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production   Systems  Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system. ",2020,digital twin,1.8395668,7.252539,2020-,6,#5E0897
174,"Digital twin, physics-based model, and machine learning applied to  damage detection in structures","Digital twin, physics-based model, and machine learning applied to   damage detection in structures  This work is interested in digital twins, and the development of a simplified framework for them, in the context of dynamical systems. Digital twin is an ingenious concept that helps on organizing different areas of expertise aiming at supporting engineering decisions related to a specific asset; it articulates computational models, sensors, learning, real time analysis, diagnosis, prognosis, and so on. In this framework, and to leverage its capacity, we explore the integration of physics-based models with machine learning. A digital twin is constructed for a damaged structure, where a discrete physics-based computational model is employed to investigate several damage scenarios. A machine learning classifier, that serves as the digital twin, is trained with data taken from a stochastic computational model. This strategy allows the use of an interpretable model (physics-based) to build a fast digital twin (machine learning) that will be connected to the physical twin to support real time engineering decisions. Different classifiers (quadratic discriminant, support vector machines, etc) are tested, and different model parameters (number of sensors, level of noise, damage intensity, uncertainty, operational parameters, etc) are considered to construct datasets for the training. The accuracy of the digital twin depends on the scenario analyzed. Through the chosen application, we are able to emphasize each step of a digital twin construction, including the possibility of integrating physics-based models with machine learning. The different scenarios explored yield conclusions that might be helpful for a large range of applications. ",2020,digital twin,1.8922253,7.116981,2020-,6,#5E0897
175,Towards Next Generation of Pedestrian and Connected Vehicle In-the-loop  Research: A Digital Twin Co-Simulation Framework,"Towards Next Generation of Pedestrian and Connected Vehicle In-the-loop   Research: A Digital Twin Co-Simulation Framework  Digital Twin is an emerging technology that replicates real-world entities into a digital space. It has attracted increasing attention in the transportation field and many researchers are exploring its future applications in the development of Intelligent Transportation System (ITS) technologies. Connected vehicles (CVs) and pedestrians are among the major traffic participants in ITS. However, the usage of Digital Twin in research involving both CV and pedestrian remains largely unexplored. In this study, a Digital Twin framework for CV and pedestrian in-the-loop simulation is proposed. The proposed framework consists of the physical world, the digital world, and data transmission in between. The features for the entities (CV and pedestrian) that need digital twining are divided into external state and internal state, and the attributes in each state are described. We also demonstrate a sample architecture under the proposed Digital Twin framework, which is based on Carla-Sumo Co-simulation and Cave automatic virtual environment (CAVE). A case study that investigates Vehicle-Pedestrian (V2P) warning system is conducted to validate the effectiveness of the presented architecture. The proposed framework is expected to provide guidance to the future Digital Twin research, and the architecture we build can serve as the testbed for further research and development of ITS applications on CV and pedestrians. ",2022,digital twin,1.9258076,7.172704,2020-,6,#5E0897
176,"Role of Digital Twin in Optical Communication: Fault Management,  Hardware Configuration, and Transmission Simulation","Role of Digital Twin in Optical Communication: Fault Management,   Hardware Configuration, and Transmission Simulation  Optical communication is developing rapidly in the directions of hardware resource diversification, transmission system flexibility, and network function virtualization. Its proliferation poses a significant challenge to traditional optical communication management and control systems. Digital twin (DT), a technology that utilizes data, models, and algorithms and integrates multiple disciplines, acts as a bridge between the real and virtual worlds for comprehensive connectivity. In the digital space, virtual models are stablished dynamically to simulate and describe the states, behaviors, and rules of physical objects in the physical space. DT has been significantly developed and widely applied in industrial and military fields. This study introduces the DT technology to optical communication through interdisciplinary crossing and proposes a DT framework suitable for optical communication. The intelligent fault management model, flexible hardware configuration model, and dynamic transmission simulation model are established in the digital space with the help of deep learning algorithms to ensure the highreliability operation and high-efficiency management of optical communication systems and networks. ",2020,digital twin,1.3144189,6.155891,2020-,6,#5E0897
177,"Digital Twin for Non-Terrestrial Networks: Vision, Challenges, and  Enabling Technologies","Digital Twin for Non-Terrestrial Networks: Vision, Challenges, and   Enabling Technologies  The ongoing digital transformation has sparked the emergence of various new network applications that demand cutting-edge technologies to enhance their efficiency and functionality. One of the promising technologies in this direction is the digital twin, which is a new approach to design and manage complicated cyber-physical systems with a high degree of automation, intelligence, and resilience. This article discusses the use of digital twin technology as a new approach for modeling non-terrestrial networks (NTNs). Digital twin technology can create accurate data-driven NTN models that operate in real-time, allowing for rapid testing and deployment of new NTN technologies and services, besides facilitating innovation and cost reduction. Specifically, we provide a vision on integrating the digital twin into NTNs and explore the primary deployment challenges, as well as the key potential enabling technologies within NTN realm. In closing, we present a case study that employs a data-driven digital twin model for dynamic and service-oriented network slicing within an open radio access network (O-RAN) NTN architecture. ",2023,digital twin,1.6720041,7.001458,2020-,6,#5E0897
178,Drone-based AI and 3D Reconstruction for Digital Twin Augmentation,"Drone-based AI and 3D Reconstruction for Digital Twin Augmentation  Digital Twin is an emerging technology at the forefront of Industry 4.0, with the ultimate goal of combining the physical space and the virtual space. To date, the Digital Twin concept has been applied in many engineering fields, providing useful insights in the areas of engineering design, manufacturing, automation, and construction industry. While the nexus of various technologies opens up new opportunities with Digital Twin, the technology requires a framework to integrate the different technologies, such as the Building Information Model used in the Building and Construction industry. In this work, an Information Fusion framework is proposed to seamlessly fuse heterogeneous components in a Digital Twin framework from the variety of technologies involved. This study aims to augment Digital Twin in buildings with the use of AI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a drone-based Digital Twin augmentation framework with reusable and customisable components. A proof of concept is also developed, and extensive evaluation is conducted for 3D reconstruction and applications of AI for defect detection. ",2021,digital twin,1.7904818,7.128372,2020-,6,#5E0897
179,Digital Twin as a Service (DTaaS): A Platform for Digital Twin  Developers and Users,"Digital Twin as a Service (DTaaS): A Platform for Digital Twin   Developers and Users  Establishing digital twins is a non-trivial endeavour especially when users face significant challenges in creating them from scratch. Ready availability of reusable models, data and tool assets, can help with creation and use of digital twins. A number of digital twin frameworks exist to facilitate creation and use of digital twins. In this paper we propose a digital twin framework to author digital twin assets, create digital twins from reusable assets and make the digital twins available as a service to other users. The proposed framework automates the management of reusable assets, storage, provision of compute infrastructure, communication and monitoring tasks. The users operate at the level of digital twins and delegate rest of the work to the digital twin as a service framework. ",2023,digital twin,1.7711574,7.1641116,2020-,6,#5E0897
180,Business (mis)Use Cases of Generative AI,"Business (mis)Use Cases of Generative AI  Generative AI is a class of machine learning technology that learns to generate new data from training data. While deep fakes and media-and art-related generative AI breakthroughs have recently caught people's attention and imagination, the overall area is in its infancy for business use. Further, little is known about generative AI's potential for malicious misuse at large scale. Using co-creation design fictions with AI engineers, we explore the plausibility and severity of business misuse cases. ",2020,generative ai,2.0022895,7.5002713,2020-,6,#5E0897
181,Art and the science of generative AI: A deeper dive,"Art and the science of generative AI: A deeper dive  A new class of tools, colloquially called generative AI, can produce high-quality artistic media for visual arts, concept art, music, fiction, literature, video, and animation. The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI - and making policy decisions around it - requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity. We argue that generative AI is not the harbinger of art's demise, but rather is a new medium with its own distinct affordances. In this vein, we consider the impacts of this new medium on creators across four themes: aesthetics and culture, legal questions of ownership and credit, the future of creative work, and impacts on the contemporary media ecosystem. Across these themes, we highlight key research questions and directions to inform policy and beneficial uses of the technology. ",2023,generative ai,1.7595254,7.655662,2020-,6,#5E0897
182,A survey of Generative AI Applications,"A survey of Generative AI Applications  Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field. ",2023,generative ai,1.6285986,7.3716645,2020-,6,#5E0897
183,AI for the Generation and Testing of Ideas Towards an AI Supported  Knowledge Development Environment,"AI for the Generation and Testing of Ideas Towards an AI Supported   Knowledge Development Environment  New systems employ Machine Learning to sift through large knowledge sources, creating flexible Large Language Models. These models discern context and predict sequential information in various communication forms. Generative AI, leveraging Transformers, generates textual or visual outputs mimicking human responses. It proposes one or multiple contextually feasible solutions for a user to contemplate. However, generative AI does not currently support traceability of ideas, a useful feature provided by search engines indicating origin of information. The narrative style of generative AI has gained positive reception. People learn from stories. Yet, early ChatGPT efforts had difficulty with truth, reference, calculations, and aspects like accurate maps. Current capabilities of referencing locations and linking to apps seem to be better catered by the link-centric search methods we've used for two decades. Deploying truly believable solutions extends beyond simulating contextual relevance as done by generative AI. Combining the creativity of generative AI with the provenance of internet sources in hybrid scenarios could enhance internet usage. Generative AI, viewed as drafts, stimulates thinking, offering alternative ideas for final versions or actions. Scenarios for information requests are considered. We discuss how generative AI can boost idea generation by eliminating human bias. We also describe how search can verify facts, logic, and context. The user evaluates these generated ideas for selection and usage. This paper introduces a system for knowledge workers, Generate And Search Test, enabling individuals to efficiently create solutions previously requiring top collaborations of experts. ",2023,generative ai,2.5195932,7.883466,2020-,6,#5E0897
184,Empowering Business Transformation: The Positive Impact and Ethical  Considerations of Generative AI in Software Product Management -- A  Systematic Literature Review,"Empowering Business Transformation: The Positive Impact and Ethical   Considerations of Generative AI in Software Product Management -- A   Systematic Literature Review  Generative Artificial Intelligence (GAI) has made outstanding strides in recent years, with a good-sized impact on software product management. Drawing on pertinent articles from 2016 to 2023, this systematic literature evaluation reveals generative AI's potential applications, benefits, and constraints in this area. The study shows that technology can assist in idea generation, market research, customer insights, product requirements engineering, and product development. It can help reduce development time and costs through automatic code generation, customer feedback analysis, and more. However, the technology's accuracy, reliability, and ethical consideration persist. Ultimately, generative AI's practical application can significantly improve software product management activities, leading to more efficient use of resources, better product outcomes, and improved end-user experiences. ",2023,generative ai,1.8660235,7.471178,2020-,6,#5E0897
185,VerifAI: Verified Generative AI,"VerifAI: Verified Generative AI  Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach can ensure the correctness of generative AI, promote transparency, and enable decision-making with greater confidence. Our vision is to promote the development of verifiable generative AI and contribute to a more trustworthy and responsible use of AI. ",2023,generative ai,1.9122334,7.5156865,2020-,6,#5E0897
186,Public Perception of Generative AI on Twitter: An Empirical Study Based  on Occupation and Usage,"Public Perception of Generative AI on Twitter: An Empirical Study Based   on Occupation and Usage  The emergence of generative AI has sparked substantial discussions, with the potential to have profound impacts on society in all aspects. As emerging technologies continue to advance, it is imperative to facilitate their proper integration into society, managing expectations and fear. This paper investigates users' perceptions of generative AI using 3M posts on Twitter from January 2019 to March 2023, especially focusing on their occupation and usage. We find that people across various occupations, not just IT-related ones, show a strong interest in generative AI. The sentiment toward generative AI is generally positive, and remarkably, their sentiments are positively correlated with their exposure to AI. Among occupations, illustrators show exceptionally negative sentiment mainly due to concerns about the unethical usage of artworks in constructing AI. People use ChatGPT in diverse ways, and notably the casual usage in which they ""play with"" ChatGPT tends to associate with positive sentiments. After the release of ChatGPT, people's interest in AI in general has increased dramatically; however, the topic with the most significant increase and positive sentiment is related to crypto, indicating the hype-worthy characteristics of generative AI. These findings would offer valuable lessons for policymaking on the emergence of new technology and also empirical insights for the considerations of future human-AI symbiosis. ",2023,generative ai,1.7351912,7.6560264,2020-,6,#5E0897
187,Generative AI at Work,"Generative AI at Work  We study the staggered introduction of a generative AI-based conversational assistant using data from 5,000 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and low-skilled workers, and minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve. In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention. ",2023,generative ai,2.0582144,7.9488454,2020-,6,#5E0897
188,Rolling the Dice: Imagining Generative AI as a Dungeons & Dragons  Storytelling Companion,"Rolling the Dice: Imagining Generative AI as a Dungeons & Dragons   Storytelling Companion  AI Advancements have augmented casual writing and story generation, but their usage poses challenges in collaborative storytelling. In role-playing games such as Dungeons & Dragons (D&D), composing prompts using generative AI requires a technical understanding to generate ideal results, which is difficult for novices. Thus, emergent narratives organically developed based on player actions and decisions have yet to be fully utilized. This paper envisions the use of generative AI in transforming storytelling into an interactive drama using dynamic and immersive narratives. First, we describe scenarios where narratives are created and character conversations are designed within an overarching fantasy disposition. Then, we recommend design guidelines to help create tools using generative AI in interactive storytelling. Lastly, we raise questions on its potential impact on player immersion and cognitive load. Our contributions may be expanded within the broader interactive storytelling domain, such as speech-conversational AI and persona-driven chatbots. ",2023,generative ai,2.0278537,7.9821744,2020-,6,#5E0897
189,"Towards social generative AI for education: theory, practices and ethics","Towards social generative AI for education: theory, practices and ethics  This paper explores educational interactions involving humans and artificial intelligences not as sequences of prompts and responses, but as a social process of conversation and exploration. In this conception, learners continually converse with AI language models within a dynamic computational medium of internet tools and resources. Learning happens when this distributed system sets goals, builds meaning from data, consolidates understanding, reconciles differences, and transfers knowledge to new domains. Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors. This raises fundamental problems of ethics. Such systems should be aware of their limitations, their responsibility to learners and the integrity of the internet, and their respect for human teachers and experts. We need to consider how to design and constrain social generative AI for education. ",2023,generative ai,1.694697,7.4927125,2020-,6,#5E0897
190,Image-based Early Detection System for Wildfires,"Image-based Early Detection System for Wildfires  Wildfires are a disastrous phenomenon which cause damage to land, loss of property, air pollution, and even loss of human life. Due to the warmer and drier conditions created by climate change, more severe and uncontrollable wildfires are expected to occur in the coming years. This could lead to a global wildfire crisis and have dire consequences on our planet. Hence, it has become imperative to use technology to help prevent the spread of wildfires. One way to prevent the spread of wildfires before they become too large is to perform early detection i.e, detecting the smoke before the actual fire starts. In this paper, we present our Wildfire Detection and Alert System which use machine learning to detect wildfire smoke with a high degree of accuracy and can send immediate alerts to users. Our technology is currently being used in the USA to monitor data coming in from hundreds of cameras daily. We show that our system has a high true detection rate and a low false detection rate. Our performance evaluation study also shows that on an average our system detects wildfire smoke faster than an actual person. ",2022,wild fires,3.4636755,7.844294,2020-,6,#5E0897
191,Wildfire Prediction to Inform Fire Management: Statistical Science  Challenges,"Wildfire Prediction to Inform Fire Management: Statistical Science   Challenges  Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales. ",2013,wild fires,4.313369,7.0253096,2011-2015,4,#9100CF
192,Towards a Real-Time Data Driven Wildland Fire Model,"Towards a Real-Time Data Driven Wildland Fire Model  A wildland fire model based on semi-empirical relations for the spread rate of a surface fire and post-frontal heat release is coupled with the Weather Research and Forecasting atmospheric model (WRF). The propagation of the fire front is implemented by a level set method. Data is assimilated by a morphing ensemble Kalman filter, which provides amplitude as well as position corrections. Thermal images of a fire will provide the observations and will be compared to a synthetic image from the model state. ",2008,wild fires,4.455134,6.914899,2006-2010,3,#FEF65C
193,A wildland fire model with data assimilation,"A wildland fire model with data assimilation  A wildfire model is formulated based on balance equations for energy and fuel, where the fuel loss due to combustion corresponds to the fuel reaction rate. The resulting coupled partial differential equations have coefficients that can be approximated from prior measurements of wildfires. An ensemble Kalman filter technique with regularization is then used to assimilate temperatures measured at selected points into running wildfire simulations. The assimilation technique is able to modify the simulations to track the measurements correctly even if the simulations were started with an erroneous ignition location that is quite far away from the correct one. ",2007,wild fires,4.782197,6.6583037,2006-2010,3,#FEF65C
194,Multimodal Wildland Fire Smoke Detection,"Multimodal Wildland Fire Smoke Detection  Research has shown that climate change creates warmer temperatures and drier conditions, leading to longer wildfire seasons and increased wildfire risks in the United States. These factors have in turn led to increases in the frequency, extent, and severity of wildfires in recent years. Given the danger posed by wildland fires to people, property, wildlife, and the environment, there is an urgency to provide tools for effective wildfire management. Early detection of wildfires is essential to minimizing potentially catastrophic destruction. In this paper, we present our work on integrating multiple data sources in SmokeyNet, a deep learning model using spatio-temporal information to detect smoke from wildland fires. Camera image data is integrated with weather sensor measurements and processed by SmokeyNet to create a multimodal wildland fire smoke detection system. We present our results comparing performance in terms of both accuracy and time-to-detection for multimodal data vs. a single data source. With a time-to-detection of only a few minutes, SmokeyNet can serve as an automated early notification system, providing a useful tool in the fight against destructive wildfires. ",2022,wild fires,4.047656,7.689841,2020-,6,#5E0897
195,Mitigating Greenhouse Gas Emissions Through Generative Adversarial  Networks Based Wildfire Prediction,"Mitigating Greenhouse Gas Emissions Through Generative Adversarial   Networks Based Wildfire Prediction  Over the past decade, the number of wildfire has increased significantly around the world, especially in the State of California. The high-level concentration of greenhouse gas (GHG) emitted by wildfires aggravates global warming that further increases the risk of more fires. Therefore, an accurate prediction of wildfire occurrence greatly helps in preventing large-scale and long-lasting wildfires and reducing the consequent GHG emissions. Various methods have been explored for wildfire risk prediction. However, the complex correlations among a lot of natural and human factors and wildfire ignition make the prediction task very challenging. In this paper, we develop a deep learning based data augmentation approach for wildfire risk prediction. We build a dataset consisting of diverse features responsible for fire ignition and utilize a conditional tabular generative adversarial network to explore the underlying patterns between the target value of risk levels and all involved features. For fair and comprehensive comparisons, we compare our proposed scheme with five other baseline methods where the former outperformed most of them. To corroborate the robustness, we have also tested the performance of our method with another dataset that also resulted in better efficiency. By adopting the proposed method, we can take preventive strategies of wildfire mitigation to reduce global GHG emissions. ",2021,wild fires,4.123949,7.2447534,2020-,6,#5E0897
196,Uncertainty Aware Wildfire Management,"Uncertainty Aware Wildfire Management  Recent wildfires in the United States have resulted in loss of life and billions of dollars, destroying countless structures and forests. Fighting wildfires is extremely complex. It is difficult to observe the true state of fires due to smoke and risk associated with ground surveillance. There are limited resources to be deployed over a massive area and the spread of the fire is challenging to predict. This paper proposes a decision-theoretic approach to combat wildfires. We model the resource allocation problem as a partially-observable Markov decision process. We also present a data-driven model that lets us simulate how fires spread as a function of relevant covariates. A major problem in using data-driven models to combat wildfires is the lack of comprehensive data sources that relate fires with relevant covariates. We present an algorithmic approach based on large-scale raster and vector analysis that can be used to create such a dataset. Our data with over 2 million data points is the first open-source dataset that combines existing fire databases with covariates extracted from satellite imagery. Through experiments using real-world wildfire data, we demonstrate that our forecasting model can accurately model the spread of wildfires. Finally, we use simulations to demonstrate that our response strategy can significantly reduce response times compared to baseline methods. ",2020,wild fires,4.3807707,7.2853494,2020-,6,#5E0897
197,Analytical and numerical insights into wildfire dynamics: Exploring the  advection-diffusion-reaction model,"Analytical and numerical insights into wildfire dynamics: Exploring the   advection-diffusion-reaction model  Understanding the dynamics of wildfire is crucial for developing management and intervention strategies. Mathematical and computational models can be used to improve our understanding of wildfire processes and dynamics. This paper presents a systematic study of a widely used advection-diffusion-reaction wildfire model with non-linear coupling. The importance of single mechanisms is discovered by analysing hierarchical sub-models. Numerical simulations provide further insight into the dynamics. As a result, the influence of wind and model parameters such as the bulk density or the heating value on the wildfire propagation speed and the remaining biomass after the burn are assessed. Linearisation techniques for a reduced model provide surprisingly good estimates for the propagation speed in the full model. ",2023,wild fires,5.9689336,6.0140047,2020-,6,#5E0897
198,"Computational modeling of extreme wildland fire events: a synthesis of  scientific understanding with applications to forecasting, land management,  and firefighter safety","Computational modeling of extreme wildland fire events: a synthesis of   scientific understanding with applications to forecasting, land management,   and firefighter safety  The understanding and prediction of large wildland fire events around the world is a growing interdisciplinary research area advanced rapidly by development and use of computational models. Recent models bidirectionally couple computational fluid dynamics models including weather prediction models with modules containing algorithms representing fire spread and heat release, simulating fire-atmosphere interactions across scales spanning three orders of magnitude. Integrated with weather data and airborne and satellite remote sensing data on wildland fuels and active fire detection, modern coupled weather-fire modeling systems are being used to solve current science problems. Compared to legacy tools, these dynamic computational modeling systems increase cost and complexity but have produced breakthrough insights notably into the mechanisms underlying extreme wildfire events such as fine-scale extreme winds associated with interruptions of the electricity grid and have been configured to forecast a fire's growth, expanding our ability to anticipate how they will unfold. We synthesize case studies of recent extreme events, expanding applications, and the challenges and limitations in our remote sensing systems, fire prediction tools, and meteorological models that add to wildfires' mystery and apparent unpredictability. ",2020,wild fires,5.2372694,6.7067246,2020-,6,#5E0897
199,Real-Time Data Driven Wildland Fire Modeling,"Real-Time Data Driven Wildland Fire Modeling  We are developing a wildland fire model based on semi-empirical relations that estimate the rate of spread of a surface fire and post-frontal heat release, coupled with WRF, the Weather Research and Forecasting atmospheric model. A level set method identifies the fire front. Data are assimilated using both amplitude and position corrections using a morphing ensemble Kalman filter. We will use thermal images of a fire for observations that will be compared to synthetic image based on the model state. ",2008,wild fires,4.452081,6.891712,2006-2010,3,#FEF65C
200,Earthquakes in cities revisited,"Earthquakes in cities revisited  During the last twenty years, a number of publications of theoretical-numerical nature have appeared which come to the apparently-reassuring conclusion that seismic motion on the ground in cities is smaller than what this motion would be in the absence of the buildings (but for the same underground and seismic load). Other than the fact that this finding tells nothing about the motion within the buildings, it must be confronted with the overwhelming empirical evidence (e.g, earthquakes in Sendai (2011), Kathmandu (2015), Tainan City (2016), etc.) that shaking within buildings of a city is often large enough to damage or even destroy these structures. I show, on several examples, that theory can be reconciled with empirical evidence, and suggest that the crucial subject of seismic response in cities is in need of more thorough research. ",2016,earthquakes in major cities,5.279921,6.365577,2016-2020,5,#7402B1
201,Space-Time Clustering and Correlations of Major Earthquakes,"Space-Time Clustering and Correlations of Major Earthquakes  Earthquake occurrence in nature is thought to result from correlated elastic stresses, leading to clustering in space and time. We show that occurrence of major earthquakes in California correlates with time intervals when fluctuations in small earthquakes are suppressed relative to the long term average. We estimate a probability of less than 1% that this coincidence is due to random clustering. ",2006,earthquakes in major cities,5.3030124,6.1382008,2006-2010,3,#FEF65C
202,Triggering of large earthquakes is driven by their twins,"Triggering of large earthquakes is driven by their twins  Fundamentally related to the UV divergence problem in Physics, conventional wisdom in seismology is that the smallest earthquakes, which are numerous and often go undetected, dominate the triggering of major earthquakes, making the prediction of the latter difficult if not inherently impossible. By developing a rigorous validation procedure, we show that, in fact, large earthquakes (above magnitude 6.3 in California) are preferentially triggered by large events. Because of the magnitude correlations intrinsic in the validated model, we further rationalize the existence of earthquake doublets. These findings have far-reaching implications for short-term and medium-term seismic risk assessment, as well as for the development of a deeper theory without UV cut-off that is locally self-similar. ",2021,earthquakes in major cities,5.377548,6.293648,2020-,6,#5E0897
203,Global Earthquake Prediction Systems,"Global Earthquake Prediction Systems  Terra Seismic can predict most major earthquakes (M6.2 or greater) at least 2 - 5 months before they will strike. Global earthquake prediction is based on determinations of the stressed areas that will start to behave abnormally before major earthquakes. The size of the observed stressed areas roughly corresponds to estimates calculated from Dobrovolskys formula. To identify abnormalities and make predictions, Terra Seismic applies various methodologies, including satellite remote sensing methods and data from ground-based instruments. We currently process terabytes of information daily, and use more than 80 different multiparameter prediction systems. Alerts are issued if the abnormalities are confirmed by at least five different systems. We observed that geophysical patterns of earthquake development and stress accumulation are generally the same for all key seismic regions. Thus, the same earthquake prediction methodologies and systems can be applied successfully worldwide. Our technology has been used to retrospectively test data gathered since 1970 and it successfully detected about 90 percent of all significant quakes over the last 50 years. ",2020,earthquakes in major cities,5.3322253,6.468651,2020-,6,#5E0897
204,Anomalous diffusion of epicentres,Anomalous diffusion of epicentres  The classification of earthquakes in main shocks and aftershocks by a method recently proposed by M. Baiesi and M. Paczuski allows to the generation of a complex network composed of clusters that group the most correlated events. The spatial distribution of epicentres inside these structures corresponding to the catalogue of earthquakes in the eastern region of Cuba shows anomalous anti-diffusive behaviour evidencing the attractive nature of the main shock and the possible description in terms of fractional kinetics. ,2007,earthquakes in major cities,5.3281693,6.273119,2006-2010,3,#FEF65C
205,Locating earthquakes with a network of seismic stations via a deep  learning method,"Locating earthquakes with a network of seismic stations via a deep   learning method  The accurate and automated determination of earthquake locations is still a challenging endeavor. However, such information is critical for monitoring seismic activity and assessing potential hazards in real time. Recently, a convolutional neural network was applied to detect earthquakes from single-station waveforms and approximately map events across several large surface areas. In this study, we locate 194 earthquakes induced during oil and gas operations in Oklahoma, USA, within an error range of approximately 4.9 km on average to the epicenter and 1.0 km to the depth in catalogs with data from 30 network stations by applying the fully convolutional network. The network is trained by 1,013 historic events, and the output is a 3D volume of the event location probability in the Earth. The trained system requires approximately one hundredth of a second to locate an event without the need for any velocity model or human interference. ",2018,earthquakes in major cities,5.3018227,6.4881587,2016-2020,5,#7402B1
206,Detecting local earthquakes via fiber-optic cables in telecommunication  conduits under Stanford University campus using deep learning,"Detecting local earthquakes via fiber-optic cables in telecommunication   conduits under Stanford University campus using deep learning  With fiber-optic seismic acquisition development, continuous dense seismic monitoring is becoming increasingly more accessible. Repurposing fiber cables in telecommunication conduits makes it possible to run seismic studies at low cost, even in locations where traditional seismometers are not easily installed, such as in urban areas. However, due to the large volume of continuous streaming data, data collected in such a manner will go to waste unless we significantly automate the processing workflow. We train a convolutional neural network (CNN) for earthquake detection using data acquired over three years by fiber cables in telecommunication conduits under Stanford University campus. We demonstrate that fiber-optic systems can effectively complement sparse seismometer networks to detect local earthquakes. The CNN allows for reliable earthquake detection despite a low signal-to-noise ratio and even detects small-amplitude previously-uncataloged events. ",2022,earthquakes in major cities,3.863862,7.453926,2020-,6,#5E0897
207,Time-frequency analysis of Transitory/Permanent frequency decrease in  civil engineering structures during earthquakes,"Time-frequency analysis of Transitory/Permanent frequency decrease in   civil engineering structures during earthquakes  The analysis of strong motion recordings in structures is crucial to understand the damaging process during earthquakes. A very precise time-frequency representation, the reassigned smoothed pseudo-Wigner-Ville method, allowed us to follow the variation of the Millikan Library (California) and the Grenoble City Hall building (France) resonance frequencies during earthquakes. Under strong motions, a quick frequency drop, attributed to damage of the soil-structure system, followed by a slower increase is found. However, in the case of weak earthquakes, we show that frequency variations come from the ground motion spectrum and cannot be interpreted in terms of change of the soil-structure system. ",2008,earthquakes in major cities,5.4096456,6.4316797,2006-2010,3,#FEF65C
208,Scaling and precursor motifs in earthquake networks,"Scaling and precursor motifs in earthquake networks  A measure of the correlation between two earthquakes is used to link events to their aftershocks, generating a growing network structure. In this framework one can quantify whether an aftershock is close or far, from main shocks of all magnitudes. We find that simple network motifs involving links to far aftershocks appear frequently before the three biggest earthquakes of the last 16 years in Southern California. Hence, networks could be useful to detect symptoms typically preceding major events. ",2004,earthquakes in major cities,5.3069715,6.317299,2001-2005,2,#FFE135
209,Scale-free and small-world properties of earthquake network in Chile,"Scale-free and small-world properties of earthquake network in Chile  The properties of earthquake networks have been studied so far mainly for the seismic data sets taken from California, Japan and Iran, and features common in these regions have been reported in the literature. Here, an earthquake network is constructed and analyzed for the Chilean data to examine if the scale-free and small-world properties of the earthquake networks constructed in the other geographical regions can also be found in seismicity in Chile. It is shown that the result is affirmative: in all the regions both the exponent ""gamma"" of the power-law connectivity distribution and the clustering coefficient C take the universal invariant values ""gamma ~1"" and ""C~0.85"", respectively, as the cell size becomes larger than a certain value, which is the scale of coarse graining needed for constructing earthquake network. An interpretation for this remarkable result is presented based on physical considerations. ",2010,earthquakes in major cities,5.2450542,6.241241,2006-2010,3,#FEF65C
210,Should Engineers be Concerned about Vulnerability of Highway Bridges to  Potentially-Induced Seismic Hazards?,"Should Engineers be Concerned about Vulnerability of Highway Bridges to   Potentially-Induced Seismic Hazards?  This paper evaluates the vulnerability of highway bridges in areas subjected to human induced seismic hazards that are commonly associated with petroleum activities and wastewater disposal. Recently, there has been a significant growth in the rate of such earthquakes, especially in areas of Texas, Oklahoma, and Kansas. The magnitudes of these earthquakes are usually lower than tectonic earthquakes that can occur in high seismic regions; however, such induced earthquakes can occur in areas that historically have had negligible seismicity. Thus, the infrastructure in these locations was likely designed for no to low seismic demands, making them vulnerable to seismic damage. Ongoing research is aimed at evaluating the vulnerability bridge infrastructure to these human induced seismic hazards. In this paper, fragility curves are developed specifically for steel girder bridges by considering major sources of uncertainty, including uncertainty in ground motions and local soil conditions expected in the Texas, Oklahoma, and Kansas region, as well as uncertainty in design and detailing practices in the area. The results of this fragility analysis are presented herein as a basis for discussion of potential seismic risks in areas affected by induced earthquakes. ",2018,earthquakes in rural regions,5.3864956,6.326588,2016-2020,5,#7402B1
211,Regional Seismic Information Entropy for Detecting Precursors of  Earthquake Activation,"Regional Seismic Information Entropy for Detecting Precursors of   Earthquake Activation  Here a method is presented for detecting precursors of earthquakes from time series data on earthquakes in a target region. Regional Entropy of Seismic Information, a quantity representing the average influence of an earthquake in the target region to the diversity of clusters to which earthquakes distribute, is introduced. Based on a rough qualitative model of the dynamics of land crust, it is hypothesized that the saturation after the increase in the Regional Entropy of Seismic Information precedes the activation of earthquakes. On the open earthquake catalog, this hypothesis is validated. This temporal change turned out to correlate more with the activation of earthquakes in Japanese regions, by one to two years precedence, than the compared baseline methods. ",2018,earthquakes in rural regions,5.3571396,6.3566675,2016-2020,5,#7402B1
212,Scale-free and small-world properties of earthquake network in Chile,"Scale-free and small-world properties of earthquake network in Chile  The properties of earthquake networks have been studied so far mainly for the seismic data sets taken from California, Japan and Iran, and features common in these regions have been reported in the literature. Here, an earthquake network is constructed and analyzed for the Chilean data to examine if the scale-free and small-world properties of the earthquake networks constructed in the other geographical regions can also be found in seismicity in Chile. It is shown that the result is affirmative: in all the regions both the exponent ""gamma"" of the power-law connectivity distribution and the clustering coefficient C take the universal invariant values ""gamma ~1"" and ""C~0.85"", respectively, as the cell size becomes larger than a certain value, which is the scale of coarse graining needed for constructing earthquake network. An interpretation for this remarkable result is presented based on physical considerations. ",2010,earthquakes in rural regions,5.2450542,6.241241,2006-2010,3,#FEF65C
213,"The electromagnetic fields under, on and up Earth surface as earthquakes  precursor in the Balkans and Black Sea regions","The electromagnetic fields under, on and up Earth surface as earthquakes   precursor in the Balkans and Black Sea regions  It is demonstrate that the analysis of accuracy measurement of geomagnetic field and the behavior of local tide gravitational potential can serve as an earthquake precursor ",2002,earthquakes in rural regions,9.709135,7.028907,2001-2005,2,#FFE135
214,Network similarity and statistical analysis of earthquake seismic data,"Network similarity and statistical analysis of earthquake seismic data  We study the structural similarity of earthquake networks constructed from seismic catalogs of different geographical regions. A hierarchical clustering of underlying undirected earthquake networks is shown using Jensen-Shannon divergence in graph spectra. The directed nature of links indicates that each earthquake network is strongly connected, which motivates us to study the directed version statistically. Our statistical analysis of each earthquake region identifies the hub regions. We calculate the conditional probability of the forthcoming occurrences of earthquakes in each region. The conditional probability of each event has been compared with their stationary distribution. ",2016,earthquakes in rural regions,5.093335,6.1912003,2016-2020,5,#7402B1
215,Earthquakes in cities revisited,"Earthquakes in cities revisited  During the last twenty years, a number of publications of theoretical-numerical nature have appeared which come to the apparently-reassuring conclusion that seismic motion on the ground in cities is smaller than what this motion would be in the absence of the buildings (but for the same underground and seismic load). Other than the fact that this finding tells nothing about the motion within the buildings, it must be confronted with the overwhelming empirical evidence (e.g, earthquakes in Sendai (2011), Kathmandu (2015), Tainan City (2016), etc.) that shaking within buildings of a city is often large enough to damage or even destroy these structures. I show, on several examples, that theory can be reconciled with empirical evidence, and suggest that the crucial subject of seismic response in cities is in need of more thorough research. ",2016,earthquakes in rural regions,5.279921,6.365577,2016-2020,5,#7402B1
216,Anomalous diffusion of epicentres,Anomalous diffusion of epicentres  The classification of earthquakes in main shocks and aftershocks by a method recently proposed by M. Baiesi and M. Paczuski allows to the generation of a complex network composed of clusters that group the most correlated events. The spatial distribution of epicentres inside these structures corresponding to the catalogue of earthquakes in the eastern region of Cuba shows anomalous anti-diffusive behaviour evidencing the attractive nature of the main shock and the possible description in terms of fractional kinetics. ,2007,earthquakes in rural regions,5.3281693,6.273119,2006-2010,3,#FEF65C
217,Earthquake Nowcasting with Deep Learning,Earthquake Nowcasting with Deep Learning  We review previous approaches to nowcasting earthquakes and introduce new approaches based on deep learning using three distinct models based on recurrent neural networks and transformers. We discuss different choices for observables and measures presenting promising initial results for a region of Southern California from 1950-2020. Earthquake activity is predicted as a function of 0.1-degree spatial bins for time periods varying from two weeks to four years. The overall quality is measured by the Nash Sutcliffe Efficiency comparing the deviation of nowcast and observation with the variance over time in each spatial region. The software is available as open-source together with the preprocessed data from the USGS. ,2021,earthquakes in rural regions,5.2981706,6.4302797,2020-,6,#5E0897
218,On the short time prediction of earthquakes in the Balkan- Black Sea  region based on geomagnetic field measurements and tide gravitational  potential behavior,"On the short time prediction of earthquakes in the Balkan- Black Sea   region based on geomagnetic field measurements and tide gravitational   potential behavior  The paper is a first attempt for statistical estimation of method for a short time prediction of incoming earthquake in the Balkan and Black Sea region from January to June, 2002. The essence of the discovery is that the geomagnetic local ""quake"" is a precursor of future earthquake, which time is determined by the tide gravitational potential behavior. A Balkan Black Sea region Earthquake's ""When, where"" Prediction Network is proposed. ",2002,earthquakes in rural regions,5.1922,6.515808,2001-2005,2,#FFE135
219,Earthquake Correlations and Networks- A Comparative Study,"Earthquake Correlations and Networks- A Comparative Study  We quantify the correlation between earthquakes and use the same to distinguish between relevant causally connected earthquakes. Our correlation metric is a variation on the one introduced by Baiesi and Paczuski (2004). A network of earthquakes is constructed, which is time ordered and with links between the more correlated ones. Recurrences to earthquakes are identified employing correlation thresholds to demarcate the most meaningful ones in each cluster. Data pertaining to three different seismic regions, viz. California, Japan and Himalayas, are comparatively analyzed using such a network model. The distribution of recurrence lengths and recurrence times are two of the key features analyzed to draw conclusions about the universal aspects of such a network model. We find that the unimodal feature of recurrence length distribution, which helps to associate typical rupture lengths with different magnitude earthquakes, is robust across the different seismic regions. The out-degree of the networks shows a hub structure rooted on the large magnitude earthquakes. In-degree distribution is seen to be dependent on the density of events in the neighborhood. Power laws, with two regimes having different exponents, are obtained with recurrence time distribution. This is in agreement with the Omori law for aftershocks and extends it to spatial recurrences. The crossover to the second power law regime can be taken to be signalling the end of aftershock regime in an objective fashion. ",2010,earthquakes in rural regions,5.1541,6.4215603,2006-2010,3,#FEF65C
220,A Multi-Scale Deep Learning Framework for Projecting Weather Extremes,"A Multi-Scale Deep Learning Framework for Projecting Weather Extremes  Weather extremes are a major societal and economic hazard, claiming thousands of lives and causing billions of dollars in damage every year. Under climate change, their impact and intensity are expected to worsen significantly. Unfortunately, general circulation models (GCMs), which are currently the primary tool for climate projections, cannot characterize weather extremes accurately. To address this, we present a multi-resolution deep-learning framework that, firstly, corrects a GCM's biases by matching low-order and tail statistics of its output with observations at coarse scales; and secondly, increases the level of detail of the debiased GCM output by reconstructing the finer scales as a function of the coarse scales. We use the proposed framework to generate statistically realistic realizations of the climate over Western Europe from a simple GCM corrected using observational atmospheric reanalysis. We also discuss implications for probabilistic risk assessment of natural disasters in a changing climate. ",2022,extreme weather,4.2316723,6.8717732,2020-,6,#5E0897
221,A modular framework for extreme weather generation,"A modular framework for extreme weather generation  Extreme weather events have an enormous impact on society and are expected to become more frequent and severe with climate change. In this context, resilience planning becomes crucial for risk mitigation and coping with these extreme events. Machine learning techniques can play a critical role in resilience planning through the generation of realistic extreme weather event scenarios that can be used to evaluate possible mitigation actions. This paper proposes a modular framework that relies on interchangeable components to produce extreme weather event scenarios. We discuss possible alternatives for each of the components and show initial results comparing two approaches on the task of generating precipitation scenarios. ",2021,extreme weather,3.895094,7.0422344,2020-,6,#5E0897
222,Deep Learning Techniques in Extreme Weather Events: A Review,"Deep Learning Techniques in Extreme Weather Events: A Review  Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events. ",2023,extreme weather,3.9395947,7.2964315,2020-,6,#5E0897
223,The Extreme Space Weather Event in 1903 October/November: An Outburst  from the Quiet Sun,"The Extreme Space Weather Event in 1903 October/November: An Outburst   from the Quiet Sun  While the Sun is generally more eruptive during its maximum and declining phases, observational evidence shows certain cases of powerful solar eruptions during the quiet phase of the solar activity. Occurring in the weak Solar Cycle 14 just after its minimum, the extreme space weather event in 1903 October -- November was one of these cases. Here, we reconstruct the time series of geomagnetic activity based on contemporary observational records. With the mid-latitude magnetograms, the 1903 magnetic storm is thought to be caused by a fast coronal mass ejection (~1500 km/s) and is regarded as an intense event with an estimated minimum Dst' of ~-513 nT The reconstructed time series has been compared with the equatorward extension of auroral oval (~44.1{\deg} in invariant latitude) and the time series of telegraphic disturbances. This case study shows that potential threats posed by extreme space weather events exist even during weak solar cycles or near their minima. ",2020,extreme weather,9.009808,7.9838862,2020-,6,#5E0897
224,Extreme precipitation forecasting using attention augmented convolutions,"Extreme precipitation forecasting using attention augmented convolutions  Extreme precipitation wreaks havoc throughout the world, causing billions of dollars in damage and uprooting communities, ecosystems, and economies. Accurate extreme precipitation prediction allows more time for preparation and disaster risk management for such extreme events. In this paper, we focus on short-term extreme precipitation forecasting (up to a 12-hour ahead-of-time prediction) from a sequence of sea level pressure and zonal wind anomalies. Although existing machine learning approaches have shown promising results, the associated model and climate uncertainties may reduce their reliability. To address this issue, we propose a self-attention augmented convolution mechanism for extreme precipitation forecasting, systematically combining attention scores with traditional convolutions to enrich feature data and reduce the expected errors of the results. The proposed network architecture is further fused with a highway neural network layer to gain the benefits of unimpeded information flow across several layers. Our experimental results show that the framework outperforms classical convolutional models by 12%. The proposed method increases machine learning as a tool for gaining insights into the physical causes of changing extremes, lowering uncertainty in future forecasts. ",2022,extreme weather,4.2828894,6.869573,2020-,6,#5E0897
225,Downscaling Extreme Rainfall Using Physical-Statistical Generative  Adversarial Learning,"Downscaling Extreme Rainfall Using Physical-Statistical Generative   Adversarial Learning  Modeling the risk of extreme weather events in a changing climate is essential for developing effective adaptation and mitigation strategies. Although the available low-resolution climate models capture different scenarios, accurate risk assessment for mitigation and adaption often demands detail that they typically cannot resolve. Here, we develop a dynamic data-driven downscaling (super-resolution) method that incorporates physics and statistics in a generative framework to learn the fine-scale spatial details of rainfall. Our method transforms coarse-resolution ($0.25^{\circ} \times 0.25^{\circ}$) climate model outputs into high-resolution ($0.01^{\circ} \times 0.01^{\circ}$) rainfall fields while efficaciously quantifying uncertainty. Results indicate that the downscaled rainfall fields closely match observed spatial fields and their risk distributions. ",2022,extreme weather,4.301077,6.8241863,2020-,6,#5E0897
226,Extreme Space Weather Events Recorded in History,"Extreme Space Weather Events Recorded in History  This section shows an overview of a recent development of the studies on great space weather events in history. Its discussion starts from the Carrington event and compare its intensity with the extreme storms within the coverage of the regular magnetic measurements. Extending its analyses back beyond their onset, this section shows several case studies of extreme storms with sunspot records in the telescopic observations and candidate auroral records in historical records. Before the onset of telescopic observations, this section shows the chronological coverages of the records of unaided-eye sunspot and candidate aurorae and several case studies on their basis. ",2020,extreme weather,9.2229,8.439993,2020-,6,#5E0897
227,The Extreme Space Weather Event in February/March 1941,"The Extreme Space Weather Event in February/March 1941  Given the infrequency of extreme geomagnetic storms, it is significant to note the concentration of three extreme geomagnetic storms in 1941, whose intensities ranked fourth, twelfth, and fifth within the aa index between 1868-2010. Among them, the geomagnetic storm on 1 March 1941 was so intense that three of the four Dst station magnetograms went off scale. Herein, we reconstruct its time series and measure the storm intensity with an alternative Dst estimate (Dst*). The source solar eruption at 09:29 - 09:38 GMT on 28 February was located at RGO AR 13814 and its significant intensity is confirmed by large magnetic crochets of 35 nT measured at Abinger. This solar eruption most likely released a fast interplanetary coronal mass ejection with estimated speed 2260 km/s. After its impact at 03:57 - 03:59 GMT on 1 March, an extreme magnetic storm was recorded worldwide. Comparative analyses on the contemporary magnetograms show the storm peak intensity of minimum Dst* < -464 nT at 16 GMT, comparable to the most and the second most extreme magnetic storms within the standard Dst index since 1957. This storm triggered significant low-latitude aurorae in the East Asian sector and their equatorward boundary has been reconstructed as 38.5{\deg} in invariant latitude. This result agrees with British magnetograms which indicate auroral oval moving above Abinger at 53.0{\deg} in magnetic latitude. The storm amplitude was even more enhanced in equatorial stations and consequently casts caveats on their usage for measurements of the storm intensity in Dst estimates. ",2020,extreme weather,9.343206,8.147046,2020-,6,#5E0897
228,Trends in the extremes of environments associated with severe US  thunderstorms,"Trends in the extremes of environments associated with severe US   thunderstorms  Severe thunderstorms can have devastating impacts. Concurrently high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are known to be conducive to severe weather, so high values of PROD=$\sqrt{\mathrm{CAPE}} \times$SRH have been used to indicate high risk of severe thunderstorms. We consider the extreme values of these three variables for a large area of the contiguous US over the period 1979-2015, and use extreme-value theory and a multiple testing procedure to show that there is a significant time trend in the extremes for PROD maxima in April, May and August, for CAPE maxima in April, May and June, and for maxima of SRH in April and May. These observed increases in CAPE are also relevant for rainfall extremes and are expected in a warmer climate, but have not previously been reported. Moreover, we show that the El Ni\~no-Southern Oscillation explains variation in the extremes of PROD and SRH in February. Our results suggest that the risk from severe thunderstorms in April and May is increasing in parts of the US where it was already high, and that the risk from storms in February tends to be higher over the main part of the region during La Ni\~na years. Our results differ from those obtained in earlier studies using extreme-value techniques to analyze a quantity similar to PROD. ",2019,extreme weather,9.381575,8.375207,2016-2020,5,#7402B1
229,Changing U.S. Extreme Temperature Statistics,"Changing U.S. Extreme Temperature Statistics  The rise in global mean temperature is an incomplete description of warming. For many purposes, including agriculture and human life, temperature extremes may be more important than temperature means and changes in local extremes may be more important than mean global changes. We define a nonparametric statistic to describe extreme temperature behavior by quantifying the frequency of local daily all-time highs and lows, normalized by their frequency in the null hypothesis of no climate change. We average this metric over 1218 weather stations in the 48 contiguous United States, and find significantly fewer all-time lows than for the null hypothesis of unchanging climate. Record highs, by contrast, exhibit no significant trend. The metric is evaluated by Monte Carlo simulation for stationary and warming temperature distributions, permitting comparison of the statistics of historic temperature records with those of modeled behavior. ",2017,extreme weather,4.6720176,6.640664,2016-2020,5,#7402B1
230,Pest presence prediction using interpretable machine learning,"Pest presence prediction using interpretable machine learning  Helicoverpa Armigera, or cotton bollworm, is a serious insect pest of cotton crops that threatens the yield and the quality of lint. The timely knowledge of the presence of the insects in the field is crucial for effective farm interventions. Meteo-climatic and vegetation conditions have been identified as key drivers of crop pest abundance. In this work, we applied an interpretable classifier, i.e., Explainable Boosting Machine, which uses earth observation vegetation indices, numerical weather predictions and insect trap catches to predict the onset of bollworm harmfulness in cotton fields in Greece. The glass-box nature of our approach provides significant insight on the main drivers of the model and the interactions among them. Model interpretability adds to the trustworthiness of our approach and therefore its potential for rapid uptake and context-based implementation in operational farm management scenarios. Our results are satisfactory and the importance of drivers, through our analysis on global and local explainability, is in accordance with the literature. ",2022,insect pest forecasting,3.8238707,7.557277,2020-,6,#5E0897
231,An Efficient Insect Pest Classification Using Multiple Convolutional  Neural Network Based Models,"An Efficient Insect Pest Classification Using Multiple Convolutional   Neural Network Based Models  Accurate insect pest recognition is significant to protect the crop or take the early treatment on the infected yield, and it helps reduce the loss for the agriculture economy. Design an automatic pest recognition system is necessary because manual recognition is slow, time-consuming, and expensive. The Image-based pest classifier using the traditional computer vision method is not efficient due to the complexity. Insect pest classification is a difficult task because of various kinds, scales, shapes, complex backgrounds in the field, and high appearance similarity among insect species. With the rapid development of deep learning technology, the CNN-based method is the best way to develop a fast and accurate insect pest classifier. We present different convolutional neural network-based models in this work, including attention, feature pyramid, and fine-grained models. We evaluate our methods on two public datasets: the large-scale insect pest dataset, the IP102 benchmark dataset, and a smaller dataset, namely D0 in terms of the macro-average precision (MPre), the macro-average recall (MRec), the macro-average F1- score (MF1), the accuracy (Acc), and the geometric mean (GM). The experimental results show that combining these convolutional neural network-based models can better perform than the state-of-the-art methods on these two datasets. For instance, the highest accuracy we obtained on IP102 and D0 is $74.13\%$ and $99.78\%$, respectively, bypassing the corresponding state-of-the-art accuracy: $67.1\%$ (IP102) and $98.8\%$ (D0). We also publish our codes for contributing to the current research related to the insect pest classification problem. ",2021,insect pest forecasting,3.5234625,7.9245076,2020-,6,#5E0897
232,Deep learning powered real-time identification of insects using citizen  science data,"Deep learning powered real-time identification of insects using citizen   science data  Insect-pests significantly impact global agricultural productivity and quality. Effective management involves identifying the full insect community, including beneficial insects and harmful pests, to develop and implement integrated pest management strategies. Automated identification of insects under real-world conditions presents several challenges, including differentiating similar-looking species, intra-species dissimilarity and inter-species similarity, several life cycle stages, camouflage, diverse imaging conditions, and variability in insect orientation. A deep-learning model, InsectNet, is proposed to address these challenges. InsectNet is endowed with five key features: (a) utilization of a large dataset of insect images collected through citizen science; (b) label-free self-supervised learning for large models; (c) improving prediction accuracy for species with a small sample size; (d) enhancing model trustworthiness; and (e) democratizing access through streamlined MLOps. This approach allows accurate identification (>96% accuracy) of over 2500 insect species, including pollinator (e.g., butterflies, bees), parasitoid (e.g., some wasps and flies), predator species (e.g., lady beetles, mantises, dragonflies) and harmful pest species (e.g., armyworms, cutworms, grasshoppers, stink bugs). InsectNet can identify invasive species, provide fine-grained insect species identification, and work effectively in challenging backgrounds. It also can abstain from making predictions when uncertain, facilitating seamless human intervention and making it a practical and trustworthy tool. InsectNet can guide citizen science data collection, especially for invasive species where early detection is crucial. Similar approaches may transform other agricultural challenges like disease detection and underscore the importance of data collection, particularly through citizen science efforts.. ",2023,insect pest forecasting,3.7428749,7.689724,2020-,6,#5E0897
233,Automatic Moth Detection from Trap Images for Pest Management,"Automatic Moth Detection from Trap Images for Pest Management  Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required. ",2016,insect pest forecasting,4.11253,7.587387,2016-2020,5,#7402B1
234,Development of a Forecasting and Warning System on the Ecological  Life-Cycle of Sunn Pest,Development of a Forecasting and Warning System on the Ecological   Life-Cycle of Sunn Pest  We provide a machine learning solution that replaces the traditional methods for deciding the pesticide application time of Sunn Pest. We correlate climate data with phases of Sunn Pest in its life-cycle and decide whether the fields should be sprayed. Our solution includes two groups of prediction models. The first group contains decision trees that predict migration time of Sunn Pest from winter quarters to wheat fields. The second group contains random forest models that predict the nymphal stage percentages of Sunn Pest which is a criterion for pesticide application. We trained our models on four years of climate data which was collected from Kir\c{s}ehir and Aksaray. The experiments show that our promised solution make correct predictions with high accuracies. ,2019,insect pest forecasting,4.2221174,6.9417987,2016-2020,5,#7402B1
235,High performing ensemble of convolutional neural networks for insect  pest image detection,"High performing ensemble of convolutional neural networks for insect   pest image detection  Pest infestation is a major cause of crop damage and lost revenues worldwide. Automatic identification of invasive insects would greatly speedup the identification of pests and expedite their removal. In this paper, we generate ensembles of CNNs based on different topologies (ResNet50, GoogleNet, ShuffleNet, MobileNetv2, and DenseNet201) altered by random selection from a simple set of data augmentation methods or optimized with different Adam variants for pest identification. Two new Adam algorithms for deep network optimization based on DGrad are proposed that introduce a scaling factor in the learning rate. Sets of the five CNNs that vary in either data augmentation or the type of Adam optimization were trained on both the Deng (SMALL) and the large IP102 pest data sets. Ensembles were compared and evaluated using three performance indicators. The best performing ensemble, which combined the CNNs using the different augmentation methods and the two new Adam variants proposed here, achieved state of the art on both insect data sets: 95.52% on Deng and 73.46% on IP102, a score on Deng that competed with human expert classifications. Additional tests were performed on data sets for medical imagery classification that further validated the robustness and power of the proposed Adam optimization variants. All MATLAB source code is available at https://github.com/LorisNanni/. ",2021,insect pest forecasting,3.200629,8.206337,2020-,6,#5E0897
236,Predicting Future Mosquito Larval Habitats Using Time Series Climate  Forecasting and Deep Learning,"Predicting Future Mosquito Larval Habitats Using Time Series Climate   Forecasting and Deep Learning  Mosquito habitat ranges are projected to expand due to climate change. This investigation aims to identify future mosquito habitats by analyzing preferred ecological conditions of mosquito larvae. After assembling a data set with atmospheric records and larvae observations, a neural network is trained to predict larvae counts from ecological inputs. Time series forecasting is conducted on these variables and climate projections are passed into the initial deep learning model to generate location-specific larvae abundance predictions. The results support the notion of regional ecosystem-driven changes in mosquito spread, with high-elevation regions in particular experiencing an increase in susceptibility to mosquito infestation. ",2022,insect pest forecasting,3.8776438,7.0499964,2020-,6,#5E0897
237,Global stabilization of sterile insect technique model by feedback laws,"Global stabilization of sterile insect technique model by feedback laws  The Sterile Insect Technique or SIT is presently one of the most ecological methods for controlling insect pests responsible for disease transmission or crop destruction worldwide. This technique consists of releasing sterile males into the insect pest population. This approach aims at reducing fertility in the population and, consequently, reduce significantly the native insect population after a few generations. In this work, we study the global stabilization of a pest population at extinction equilibrium by the SIT method. We construct explicit feedback laws that stabilize the model and do numerical simulations to show the efficiency of our feedback laws. The different feedback laws are also compared taking into account their possible implementation in field interventions. ",2023,insect pest forecasting,4.377031,6.6871195,2020-,6,#5E0897
238,Mathematical model for pest-insect control using mating disruption and  trapping,"Mathematical model for pest-insect control using mating disruption and   trapping  Controlling pest insects is a challenge of main importance to preserve crop production. In the context of Integrated Pest Management (IPM) programs, we develop a generic model to study the impact of mating disruption control using an artificial female pheromone to confuse males and adversely affect their mating opportunities. Consequently the reproduction rate is diminished leading to a decline in the population size. For more efficient control, trapping is used to capture the males attracted to the artificial pheromone. The model, derived from biological and ecological assumptions, is governed by a system of ODEs. A theoretical analysis of the model without control is first carried out to establish the properties of the endemic equilibrium. Then, control is added and the theoretical analysis of the model enables to identify threshold values of pheromone which are practically interesting for field applications. In particular, we show that there is a threshold above which the global asymptotic stability of the trivial equilibrium is ensured, i.e. the population goes to extinction. Finally we illustrate the theoretical results via numerical experiments. ",2016,insect pest forecasting,4.8178477,6.535524,2016-2020,5,#7402B1
239,What a million Indian farmers say?: A crowdsourcing-based method for  pest surveillance,"What a million Indian farmers say?: A crowdsourcing-based method for   pest surveillance  Many different technologies are used to detect pests in the crops, such as manual sampling, sensors, and radar. However, these methods have scalability issues as they fail to cover large areas, are uneconomical and complex. This paper proposes a crowdsourced based method utilising the real-time farmer queries gathered over telephones for pest surveillance. We developed data-driven strategies by aggregating and analyzing historical data to find patterns and get future insights into pest occurrence. We showed that it can be an accurate and economical method for pest surveillance capable of enveloping a large area with high spatio-temporal granularity. Forecasting the pest population will help farmers in making informed decisions at the right time. This will also help the government and policymakers to make the necessary preparations as and when required and may also ensure food security. ",2021,insect pest forecasting,4.0907173,7.409014,2020-,6,#5E0897
240,Some comments on the possible causes of climate change,"Some comments on the possible causes of climate change  Climate change is an important current issue and there is much debate about the causes and effects. This article examines the changes in our climate, comparing the recent changes with those in the past. There have been changes in temperature, resulting in an average global rise over the last 300 years, as well as widespread melting of snow and ice, and rising global average sea level. There are many theories for the causes of the recent change in the climate, including some natural and some human influenced. The most widely believed cause of the climate change is increasing levels of Greenhouse gases in the atmosphere and as the atmosphere plays an important role in making our planet inhabitable, it is important to understand it in order to protect it. However, there are other theories for the cause of climate change, the Sun and cosmic rays, for example, are felt by some to have a significant role to play. There is also well-established evidence that the three Milankovitch cycles change the amount and alter the distribution of sunlight over the Earth, heating and cooling it. There are many influences on our planet and they all have differing levels of impact. The purpose of this article is to review the present overall position and urge open, reasoned discussion of the problem. ",2008,climate change,4.960994,6.6753173,2006-2010,3,#FEF65C
241,"Avoiding the ""Great Filter"": An Assessment of Climate Change Solutions  and Combinations for Effective Implementation","Avoiding the ""Great Filter"": An Assessment of Climate Change Solutions   and Combinations for Effective Implementation  Climate change is the long-term shift in global weather patterns, largely caused by anthropogenic activity of greenhouse gas emissions. Global climate temperatures have unmistakably risen and naturally occurring climate variability alone cannot account for this trend. Human activities are estimated to have caused about 1.0 degree C of global warming above the pre-industrial baseline and if left unchecked, will continue to drastically damage the Earth and its inhabitants. Globally, natural disasters and subsequent economic losses have become increasingly impactful because of climate change. Both wildlife ecosystems and human habitats have been negatively impacted, from rising sea levels to alarming frequency of severe weather events around the world. Attempts towards alleviating the effects of global warming have often been at odds and remain divided among a multitude of strategies, reducing the overall effectiveness of these efforts. It is evident that collaborative action is required for avoiding the most severe consequences of climate change. This paper evaluates the main strategies (industrial/energy, political, economic, agricultural, atmospheric, geological, coastal, and social) towards both mitigating and adapting to climate change. As well, it provides an optimal combination of seven solutions which can be implemented simultaneously, working in tandem to limit and otherwise accommodate the harmful effects of climate change. Previous legislation and deployment techniques are also discussed as guides for future endeavors. ",2022,climate change,4.5733395,6.7164736,2020-,6,#5E0897
242,Man-made climate change:Facts and fiction,"Man-made climate change:Facts and fiction  Important issues about climate change are summarized and discussed:   A large body of evidence shows that the world climate is getting warmer. Climate models give a consistent explanation of this observation once human-made emissions of greenhouse gases are taken into account. Furthermore, the main source of greenhouse gases comes from the burning of oil, gas and coal, mainly in the industrialized countries. Without any change of behaviour, the possible predicted consequences of this climate change for the coming decades are very disturbing. Today's (in)action's will have long-term consequences for the entire biosphere and the living conditions of many future generations.   The combination of the various points related to the climate change leads to a final question: ""For how long will Humanity continue to bury its head in the sand?"" ",2004,climate change,4.831626,7.14783,2001-2005,2,#FFE135
243,Analysis of Greenhouse Gases,"Analysis of Greenhouse Gases  Climate change is a result of a complex system of interactions of greenhouse gases (GHG), the ocean, land, ice, and clouds. Large climate change models use several computers and solve several equations to predict the future climate. The equations may include simple polynomials to partial differential equations. Because of the uptake mechanism of the land and ocean, greenhouse gas emissions can take a while to affect the climate. The IPCC has published reports on how greenhouse gas emissions may affect the average temperature of the troposphere and the predictions show that by the end of the century, we can expect a temperature increase from 0.8 C to 5 C. In this article, I use Linear Regression (LM), Quadratic Regression and Gaussian Process Regression (GPR) on monthly GHG data going back several years and try to predict the temperature anomalies based on extrapolation. The results are quite similar to the IPCC reports. ",2020,climate change,5.112345,6.8036957,2020-,6,#5E0897
244,On the Evolution of U.S. Temperature Dynamics,"On the Evolution of U.S. Temperature Dynamics  Climate change is a massive multidimensional shift. Temperature shifts, in particular, have important implications for urbanization, agriculture, health, productivity, and poverty, among other things. While much research has documented rising mean temperature \emph{levels}, we also examine range-based measures of daily temperature \emph{volatility}. Specifically, using data for select U.S. cities over the past half-century, we compare the evolving time series dynamics of the average temperature level, AVG, and the diurnal temperature range, DTR (the difference between the daily maximum and minimum temperatures). We characterize trend and seasonality in these two series using linear models with time-varying coefficients. These straightforward yet flexible approximations provide evidence of evolving DTR seasonality and stable AVG seasonality. ",2019,climate change,4.84382,6.641134,2016-2020,5,#7402B1
245,"Climate Change and Its Causes, A Discussion About Some Key Issues","Climate Change and Its Causes, A Discussion About Some Key Issues  This article discusses the limits of the Anthropogenic Global Warming Theory advocated by the Intergovernmental Panel on Climate Change. A phenomenological theory of climate change based on the physical properties of the data themselves is proposed. At least 60% of the warming of the Earth observed since 1970 appears to be induced by natural cycles which are present in the solar system. A climatic stabilization or cooling until 2030-2040 is forecast by the phenomenological model. ",2010,climate change,5.1091814,6.5816927,2006-2010,3,#FEF65C
246,Climate Change: Sources of Warming in the Late 20th Century,"Climate Change: Sources of Warming in the Late 20th Century  The role of the North Atlantic Oscillation, the Pacific Decadal Oscillation, volcanic and other aerosols, as well as the extraordinary solar activity of the late 20th century are discussed in the context of the warming since the mid-1970s. Much of that warming is found to be due to natural causes. ",2009,climate change,5.11431,6.6312213,2006-2010,3,#FEF65C
247,Pathways to Sustainable Planetary Science,"Pathways to Sustainable Planetary Science  Climate change is a major impending threat to the future of humanity. According to the International Panel on Climate Change (IPCC), our emissions are estimated to have caused 0.8 deg C-1.2 deg C of anthropogenic global warming (AGW) above pre-industrial levels. AGW is likely to reach 1.5 degrees C between 2030 and 2052 if it continues to increase at the current rate. As the climate change is driven by the release of carbon dioxide and other greenhouse gases (GHG) into the atmosphere, there is a broad consensus that the mitigation of climate change requires transition to low GHG emission energy sources, technologies and practices. Implementing such changes systematically from individual to community-wide scales together with the resulting cultural changes and leadership towards environmental consciousness and responsibility are crucial to mitigate the looming damage of AGW. Given planetary scientists' wide recognition of the realities of climate change, and the need for us to maintain credibility by leading by example, it is appropriate to make own professional behavior more environmentally responsible. While scientists are few in numbers, and planetary scientists far fewer, high volumes of academic travel to conferences, panels, colloquia, and research collaboration visits together with extensive use of large, energetically demanding infrastructures make the ""carbon footprint"" of scientists much higher than that of an average citizen. This White Paper focuses on how modifying our activities, particularly associated with academic travel, can affect the carbon footprint of the planetary science community, and it makes recommendations on how the community and the funding agencies could best participate in the cultural change required to mitigate the damage that AGW will cause. ",2020,climate change,5.0824533,7.2806826,2020-,6,#5E0897
248,"Climate Change Policies for the XXIst Century: Mechanisms, Predictions  and Recommendations","Climate Change Policies for the XXIst Century: Mechanisms, Predictions   and Recommendations  Recent experimental works demonstrated that the Anthropogenic Global Warming (AGW) hypothesis, embodied in a series of Intergovernmental Panel on Climate Change (IPCC) global climate models, is erroneous. These works prove that atmospheric carbon dioxide contributes only very moderately to the observed warming, and that there is no climatic catastrophe in the making, independent on whether or not carbon dioxide emissions will be reduced. In view of these developments, we discuss climate predictions for the XXIst century. Based on the solar activity tendencies, a new Little Ice Age is predicted by the middle of this century, with significantly lower global temperatures. We also show that IPCC climate models can't produce any information regarding future climate, due to essential physical phenomena lacking in those, and that the current budget deficit in many EU countries is mainly caused by the policies promoting renewable energies and other AGW-motivated measures. In absence of any predictable adverse climate consequences of carbon dioxide emissions, and with no predictable shortage of fossil fuels, we argue for recalling of all policies aimed at reducing carbon dioxide emissions and usage of expensive renewable energy sources. The concepts of carbon credits, green energy and green fuels should be abandoned in favor of productive, economically viable and morally acceptable solutions. ",2011,climate change,5.0110135,6.910985,2011-2015,4,#9100CF
249,Climate Prediction through Statistical Methods,"Climate Prediction through Statistical Methods  Climate change is a reality of today. Paleoclimatic proxies and climate predictions based on coupled atmosphere-ocean general circulation models provide us with temperature data. Using Detrended Fluctuation Analysis, we are investigating the statistical connection between the climate types of the present and these local temperatures. We are relating this issue to some well-known historic climate shifts. Our main result is that the temperature fluctuations with or without a temperature scale attached to them, can be used to classify climates in the absence of other indicators such as pan evaporation and precipitation. ",2008,climate change,4.505062,6.826329,2006-2010,3,#FEF65C
250,A National Research Agenda for Intelligent Infrastructure,"A National Research Agenda for Intelligent Infrastructure  Our infrastructure touches the day-to-day life of each of our fellow citizens, and its capabilities, integrity and sustainability are crucial to the overall competitiveness and prosperity of our country. Unfortunately, the current state of U.S. infrastructure is not good: the American Society of Civil Engineers' latest report on America's infrastructure ranked it at a D+ -- in need of $3.9 trillion in new investments. This dire situation constrains the growth of our economy, threatens our quality of life, and puts our global leadership at risk. The ASCE report called out three actions that need to be taken to address our infrastructure problem: 1) investment and planning in the system; 2) bold leadership by elected officials at the local and federal state; and 3) planning sustainability and resiliency in our infrastructure.   While our immediate infrastructure needs are critical, it would be shortsighted to simply replicate more of what we have today. By doing so, we miss the opportunity to create Intelligent Infrastructure that will provide the foundation for increased safety and resilience, improved efficiencies and civic services, and broader economic opportunities and job growth. Indeed, our challenge is to proactively engage the declining, incumbent national infrastructure system and not merely repair it, but to enhance it; to create an internationally competitive cyber-physical system that provides an immediate opportunity for better services for citizens and that acts as a platform for a 21st century, high-tech economy and beyond. ",2017,infrastructure,2.0277364,7.4408607,2016-2020,5,#7402B1
251,Reliability of Critical Infrastructure Networks: Challenges,"Reliability of Critical Infrastructure Networks: Challenges  Critical infrastructures form a technological skeleton of our world by providing us with water, food, electricity, gas, transportation, communication, banking, and finance. Moreover, as urban population increases, the role of infrastructures become more vital. In this paper, we adopt a network perspective and discuss the ever growing need for fundamental interdisciplinary study of critical infrastructure networks, efficient methods for estimating their reliability, and cost-effective strategies for enhancing their resiliency. We also highlight some of the main challenges arising on this way, including cascading failures, feedback loops, and cross-sector interdependencies. ",2017,infrastructure,1.8057305,6.5135207,2016-2020,5,#7402B1
252,Spatial Data Infrastructures,"Spatial Data Infrastructures  Spatial data infrastructure (SDI) is the infrastructure that facilitates the discovery, access, management, distribution, reuse, and preservation of digital geospatial resources. These resources may include maps, data, geospatial services, and tools. As cyberinfrastructures, SDIs are similar to other infrastructures, such as water supplies and transportation networks, since they play fundamental roles in many aspects of the society. These roles have become even more significant in today's big data age, when a large volume of geospatial data and Web services are available. From a technological perspective, SDIs mainly consist of data, hardware, and software. However, a truly functional SDI also needs the efforts of people, supports from organizations, government policies, data and software standards, and many others. In this chapter, we will present the concepts and values of SDIs, as well as a brief history of SDI development in the U.S. We will also discuss the components of a typical SDI, and will specifically focus on three key components: geoportals, metadata, and search functions. Examples of the existing SDI implementations will also be discussed. ",2017,infrastructure,1.4667996,6.367065,2016-2020,5,#7402B1
253,The Web is missing an essential part of infrastructure: an Open Web  Index,The Web is missing an essential part of infrastructure: an Open Web   Index  A proposal for building an index of the Web that separates the infrastructure part of the search engine - the index - from the services part that will form the basis for myriad search engines and other services utilizing Web data on top of a public infrastructure open to everyone. ,2019,infrastructure,1.7612165,8.145815,2016-2020,5,#7402B1
254,Modelling interdependencies between the electricity and information  infrastructures,"Modelling interdependencies between the electricity and information   infrastructures  The aim of this paper is to provide qualitative models characterizing interdependencies related failures of two critical infrastructures: the electricity infrastructure and the associated information infrastructure. The interdependencies of these two infrastructures are increasing due to a growing connection of the power grid networks to the global information infrastructure, as a consequence of market deregulation and opening. These interdependencies increase the risk of failures. We focus on cascading, escalating and common-cause failures, which correspond to the main causes of failures due to interdependencies. We address failures in the electricity infrastructure, in combination with accidental failures in the information infrastructure, then we show briefly how malicious attacks in the information infrastructure can be addressed. ",2008,infrastructure,1.6826227,6.842671,2006-2010,3,#FEF65C
255,"Information Systems Playground - The Target Infrastructure, Scaling  Astro-WISE into the Petabyte range","Information Systems Playground - The Target Infrastructure, Scaling   Astro-WISE into the Petabyte range  The Target infrastructure has been specially built as a storage and compute infrastructure for the information systems derived from Astro-WISE. This infrastructure will be used by several applications that collaborate in the area of information systems within the Target project. It currently consists of 10 PB of storage and thousands of computational cores. The infrastructure has been constructed based on the requirements of the applications. The storage is controlled by the Global Parallel File System of IBM. This file system takes care of the required flexibility by combining storage hardware with different characteristics into a single file system. It is also very scalable, which allows the system to be extended into the future, while replacing old hardware with new technology. ",2011,infrastructure,1.7896938,6.898378,2011-2015,4,#9100CF
256,A National Research Agenda for Intelligent Infrastructure: 2021 Update,"A National Research Agenda for Intelligent Infrastructure: 2021 Update  Strategic, sustained Federal investments in intelligent infrastructure will increase safety and resilience, improve efficiencies and civic services, and broaden employment opportunities and job growth nationwide. The technologies that comprise intelligent infrastructure can also provide keys to solving some of the most vexing challenges we face today, including confronting future pandemics and natural disasters, achieving sustainability and energy efficiency goals, and advancing social justice. Enabling those technologies effectively will require investment in the associated computing research as well, beyond and in concert with the basic building projects. In 2017, the Computing Community Consortium (CCC) produced a series of intelligent infrastructure whitepapers, and in 2020 CCC issued a set of companion whitepapers on closely related topics. Here we briefly survey those earlier works, and then highlight four themes of rising national prominence where intelligent infrastructure can also play an enabling role, driven by experiences with the COVID-19 pandemic and the social justice movement. We conclude with recommendations for the necessary research investments. ",2021,infrastructure,1.621204,6.8862534,2020-,6,#5E0897
257,MOBILITY21: Strategic Investments for Transportation Infrastructure &  Technology,"MOBILITY21: Strategic Investments for Transportation Infrastructure &   Technology  America's transportation infrastructure is the backbone of our economy. A strong infrastructure means a strong America - an America that competes globally, supports local and regional economic development, and creates jobs. Strategic investments in our transportation infrastructure are vital to our national security, economic growth, transportation safety and our technology leadership. This document outlines critical needs for our transportation infrastructure, identifies new technology drivers and proposes strategic investments for safe and efficient air, ground, rail and marine mobility of people and goods. ",2017,infrastructure,2.0665338,6.6123805,2016-2020,5,#7402B1
258,Survey and Analysis of Production Distributed Computing Infrastructures,"Survey and Analysis of Production Distributed Computing Infrastructures  This report has two objectives. First, we describe a set of the production distributed infrastructures currently available, so that the reader has a basic understanding of them. This includes explaining why each infrastructure was created and made available and how it has succeeded and failed. The set is not complete, but we believe it is representative.   Second, we describe the infrastructures in terms of their use, which is a combination of how they were designed to be used and how users have found ways to use them. Applications are often designed and created with specific infrastructures in mind, with both an appreciation of the existing capabilities provided by those infrastructures and an anticipation of their future capabilities. Here, the infrastructures we discuss were often designed and created with specific applications in mind, or at least specific types of applications. The reader should understand how the interplay between the infrastructure providers and the users leads to such usages, which we call usage modalities. These usage modalities are really abstractions that exist between the infrastructures and the applications; they influence the infrastructures by representing the applications, and they influence the ap- plications by representing the infrastructures. ",2012,infrastructure,1.7374033,6.9679823,2011-2015,4,#9100CF
259,Delusion and Deception in Large Infrastructure Projects: Two Models for  Explaining and Preventing Executive Disaster,"Delusion and Deception in Large Infrastructure Projects: Two Models for   Explaining and Preventing Executive Disaster  The Economist recently reported that infrastructure spending is the largest it is ever been as a share of world GDP. With $22 trillion in projected investments over the next ten years in emerging economies alone, the magazine calls it the ""biggest investment boom in history."" The efficiency of infrastructure planning and execution is therefore particularly important at present. Unfortunately, the private sector, the public sector and private/public sector partnerships have a dismal record of delivering on large infrastructure cost and performance promises. This paper explains why and how to solve the problem. ",2013,infrastructure,1.9153028,7.1031146,2011-2015,4,#9100CF
260,An Emulation Framework for Fire Front Spread,"An Emulation Framework for Fire Front Spread  Forecasting bushfire spread is an important element in fire prevention and response efforts. Empirical observations of bushfire spread can be used to estimate fire response under certain conditions. These observations form rate-of-spread models, which can be used to generate simulations. We use machine learning to drive the emulation approach for bushfires and show that emulation has the capacity to closely reproduce simulated fire-front data. We present a preliminary emulator approach with the capacity for fast emulation of complex simulations. Large numbers of predictions can then be generated as part of ensemble estimation techniques, which provide more robust and reliable forecasts of stochastic systems. ",2022,bush fire,3.9372816,6.679739,2020-,6,#5E0897
261,An Optimised Satellite Constellation for Forest Fire Detection through  Edge Computing,"An Optimised Satellite Constellation for Forest Fire Detection through   Edge Computing  The end of 2019 marked a bushfire crisis for Australia that affected more than 100000km2 of land and destroyed more than 2000 houses. Here, we propose a method of in-orbit bushfire detection with high efficiency to prevent a repetition of this disaster. An LEO satellite constellation is first developed through NSGA-II (Nondominated Sorting Genetic Algorithm II), optimising for coverage over Australia. Then edge computing is adopted to run a bushfire detection algorithm using several constellation satellites as edge nodes to reduce fire detection time. A geostationary satellite is used for inter-satellite communications, such that an image taken by a satellite can be distributed among several satellites for processing. The geostationary satellite also maintains a constant link to the ground, so that a bushfire detection can be reported back without any significant delay. Overall, this system is able to detect fires that span more than 5m in length, and can make detections in 1.39s per image processed. This is faster than any currently available bushfire detection method. ",2022,bush fire,7.5351067,8.509588,2020-,6,#5E0897
262,A Clustering Algorithm to Organize Satellite Hotspot Data for the  Purpose of Tracking Bushfires Remotely,"A Clustering Algorithm to Organize Satellite Hotspot Data for the   Purpose of Tracking Bushfires Remotely  This paper proposes a spatiotemporal clustering algorithm and its implementation in the R package spotoroo. This work is motivated by the catastrophic bushfires in Australia throughout the summer of 2019-2020 and made possible by the availability of satellite hotspot data. The algorithm is inspired by two existing spatiotemporal clustering algorithms but makes enhancements to cluster points spatially in conjunction with their movement across consecutive time periods. It also allows for the adjustment of key parameters, if required, for different locations and satellite data sources. Bushfire data from Victoria, Australia, is used to illustrate the algorithm and its use within the package. ",2023,bush fire,4.115301,7.1370935,2020-,6,#5E0897
263,What will they do? Modelling self-evacuation archetypes,"What will they do? Modelling self-evacuation archetypes  A decade on from the devastating Black Saturday bushfires in Victoria, Australia, we are at a point where computer simulations of community evacuations are starting to be used within the emergency services. While fire progression modelling is embedded in strategic and operational settings at all levels of government across Victoria, modelling of community response to such fires is only just starting to be evaluated in earnest. For community response models to become integral to bushfire planning and preparedness, the key question to be addressed is: when faced with a bushfire, what will a community really do? Typically this understanding has come from local experience and expertise within the community and services, however the trend is to move towards more informed data driven approaches. In this paper we report on the latest work within the emergency sector in this space. Particularly, we discuss the application of Strahan et al.'s self-evacuation archetypes to an agent-based model of community evacuation in regional Victoria. This work is part of the consolidated bushfire evacuation modelling collaboration between several emergency management stakeholders. ",2021,bush fire,4.4331856,6.8735843,2020-,6,#5E0897
264,The emergence of scale-free fires in Australia,"The emergence of scale-free fires in Australia  Between 2019 and 2020, during the country's hottest and driest year on record, Australia experienced a dramatic bushfire season, with catastrophic ecological and environmental consequences. Several studies highlighted how such abrupt changes in fire regimes may have been in large part a consequence of climate change and other anthropogenic transformations. Here, we analyze the monthly evolution of the burned area in Australia from 2000 to 2020, obtained via satellite imaging through the MODIS platform. We find that the 2019-2020 peak is associated with signatures typically found near critical points. We introduce a modeling framework based on forest-fire models to study the properties of these emergent fire outbreaks, showing that the behavior observed during the 2019-2020 fire season matches the one of a percolation transition, where system-size outbreaks appear. Our model also highlights the existence of an absorbing phase transition that might be eventually crossed, after which the vegetation cannot recover. ",2021,bush fire,4.8888693,6.655587,2020-,6,#5E0897
265,"#ArsonEmergency and Australia's ""Black Summer"": Polarisation and  misinformation on social media","#ArsonEmergency and Australia's ""Black Summer"": Polarisation and   misinformation on social media  During the summer of 2019-20, while Australia suffered unprecedented bushfires across the country, false narratives regarding arson and limited backburning spread quickly on Twitter, particularly using the hashtag #ArsonEmergency. Misinformation and bot- and troll-like behaviour were detected and reported by social media researchers and the news soon reached mainstream media. This paper examines the communication and behaviour of two polarised online communities before and after news of the misinformation became public knowledge. Specifically, the Supporter community actively engaged with others to spread the hashtag, using a variety of news sources pushing the arson narrative, while the Opposer community engaged less, retweeted more, and focused its use of URLs to link to mainstream sources, debunking the narratives and exposing the anomalous behaviour. This influenced the content of the broader discussion. Bot analysis revealed the active accounts were predominantly human, but behavioural and content analysis suggests Supporters engaged in trolling, though both communities used aggressive language. ",2020,bush fire,1.5726633,8.058057,2020-,6,#5E0897
266,Phases and Time-Scales of Ignition and Burning of Live Fuels,"Phases and Time-Scales of Ignition and Burning of Live Fuels  Wildland fires impact ecosystems and communities worldwide. Many wildfires burn in living or a mixture of living and senescent vegetation. Therefore, it is necessary to understand the burning behavior of living fuels, in contrast to just dead or dried fuels, to more effectively support fire management decisions. In this study, the ignition and burning behaviors of needles placed in convective heat flux were evaluated. The species included longleaf pine (Pinus palustris), Douglas-fir (Pseudotsuga menziesii), western red cedar (Thuja plicata), ponderosa pine (Pinus ponderosa), western larch (Larix occidentalis), pacific yew (Taxus brevifolia), white spruce (Picea glauca), and sagebrush (Artemisia tridentate). The ignition and burning behaviors were related to live fuel moisture content (LFMC), pilot flame temperatures, and convective heat fluxes. The different phases of ignition and burning were captured using high-speed imaging. In general, four burning phases can be observed: droplet ejection and burning, a transition phase, flaming combustion, and smoldering combustion. Ejection and subsequent burning of droplets can occur prior to sustained flaming ignition only in live fuels. For some species (e.g., longleaf pine, ponderosa pine, white spruce) droplet ejection and burning can reduce ignition times relative to dried fuel with lower LFMC. In general, the transition phase tends to take longer than the flaming and droplet phases (when these occur). During the transition phase, the fuels are heated and pyrolysis occurs. Time-scales to ignition and the different phases of ignition and burning vary more among live fuels than dead and dried fuels. This conclusion indicates that other parameters, such as chemical composition and structural morphology of the fuel, can significantly influence the burning of live fuels. ",2022,bush fire,6.6198373,6.1808443,2020-,6,#5E0897
267,Effects of West Coast forest fire emissions on atmospheric environment:  A coupled satellite and ground-based assessment,"Effects of West Coast forest fire emissions on atmospheric environment:   A coupled satellite and ground-based assessment  Forest fires have a profound impact on the atmospheric environment and air quality across the ecosystems. The recent west coast forest fire in the United States of America (USA) has broken all the past records and caused severe environmental and public health burdens. As of middle September, nearly 6 million acres forest area were burned, and more than 25 casualties were reported so far. In this study, both satellite and in-situ air pollution data were utilized to examine the effects of this unprecedented wildfire on the atmospheric environment. The spatiotemporal concentrations of total six air pollutants, i.e. carbon monoxide (CO), nitrogen dioxide (NO2), sulfur dioxide (SO2), ozone (O3), particulate matter (PM2.5 and PM10), and aerosol index (AI), were measured for the periods of 15 August to 15 September for 2020 (fire year) and 2019 (reference year). The in-situ data-led measurements show that the highest increases in CO (ppm), PM2.5, and PM10 concentrations ({\mu}g/m3) were clustered around the west coastal fire-prone states, during the 15 August - 15 September period. The average CO concentration (ppm) was increased most significantly in Oregon (1147.10), followed by Washington (812.76), and California (13.17). Meanwhile, the concentration ({\mu}g/m3) in particulate matter (both PM2.5 and PM10), was increased in all three states affected severely by wildfires. Changes (positive) in both PM2.5 and PM10 were measured highest in Washington (45.83 and 88.47 for PM2.5 and PM10), followed by Oregon (41.99 and 62.75 for PM2.5 and PM10), and California (31.27 and 35.04 for PM2.5 and PM10). The average level of exposure to CO, PM2.5, and PM10 was also measured for all the three fire-prone states. The results of the exposure assessment revealed a strong tradeoff association between wildland fire and local/regional air quality standard. ",2020,bush fire,9.471775,8.771712,2020-,6,#5E0897
268,Self-Organized Criticality and Synchronization in the Forest-Fire Model,"Self-Organized Criticality and Synchronization in the Forest-Fire Model  Depending on the rule for tree growth, the forest-fire model shows either self-organized criticality with rule-dependent exponents, or synchronization, or an intermediate behavior. This is shown analytically for the one-dimensional system, but holds evidently also in higher dimensions. ",1995,bush fire,4.649082,6.3678274,1990-1995,0,#B2AB10
269,Generalizations of forest fires with ignition at origin,"Generalizations of forest fires with ignition at origin  We study generalizations of the Forest Fire model introduced in [van den Berg, J., and J\'arai, A. A. ""On the asymptotic density in a one-dimensional self-organized critical forest-fire model"". Comm. Math. Phys. 253 (2005)] and [Volkov, Stanislav. ""Forest fires on $\mathbb{Z}_+$ with ignition only at 0"". ALEA 6 (2009)] by allowing the rates at which the tree grow to depend on their location, introducing long-range burning, as well as continuous-space generalization of the model. We establish that in all the models in consideration the time required to reach site at distance $x$ from the origin is of order at most $(\log x)^{(\log 2)^{-1}+\delta}$ for any $\delta>0$. ",2019,bush fire,5.961733,5.896315,2016-2020,5,#7402B1
270,Poster: Communication in Open-Source Projects--End of the E-mail Era?,"Poster: Communication in Open-Source Projects--End of the E-mail Era?  Communication is essential in software engineering. Especially in distributed open-source teams, communication needs to be supported by channels including mailing lists, forums, issue trackers, and chat systems. Yet, we do not have a clear understanding of which communication channels stakeholders in open-source projects use. In this study, we fill the knowledge gap by investigating a statistically representative sample of 400 GitHub projects. We discover the used communication channels by regular expressions on project data. We show that (1) half of the GitHub projects use observable communication channels; (2) GitHub Issues, e-mail addresses, and the modern chat system Gitter are the most common channels; (3) mailing lists are only in place five and have a lower market share than all modern chat systems combined. ",2018,communications,1.6308311,7.29593,2016-2020,5,#7402B1
271,My Experience in Physical Layer Communications,"My Experience in Physical Layer Communications  I feel that I have been very lucky since I have experienced the most dynamic 30 years on electronics in the past. I think that the most visible change in our daily life over the past 30 years is communications. From computer modems, to internet, and to smart phones, people now feel much less lonely or bored since they are always connected. In this article, I would like to share with you on my own experience working on communications in the past decades. ",2021,communications,1.568032,6.3859944,2020-,6,#5E0897
272,Silence,"Silence  The cost of communication is a substantial factor affecting the scalability of many distributed applications. Every message sent can incur a cost in storage, computation, energy and bandwidth. Consequently, reducing the communication costs of distributed applications is highly desirable. The best way to reduce message costs is by communicating without sending any messages whatsoever. This paper initiates a rigorous investigation into the use of silence in synchronous settings, in which processes can fail. We formalize sufficient conditions for information transfer using silence, as well as necessary conditions for particular cases of interest. This allows us to identify message patterns that enable communication through silence. In particular, a pattern called a {\em silent choir} is identified, and shown to be central to information transfer via silence in failure-prone systems. The power of the new framework is demonstrated on the {\em atomic commitment} problem (AC). A complete characterization of the tradeoff between message complexity and round complexity in the synchronous model with crash failures is provided, in terms of lower bounds and matching protocols. In particular, a new message-optimal AC protocol is designed using silence, in which processes decide in~3 rounds in the common case. This significantly improves on the best previously known message-optimal AC protocol, in which decisions were performed in $\Theta(n)$ rounds. ",2018,communications,1.4729227,5.732134,2016-2020,5,#7402B1
273,Covert Wireless Communication in Presence of a Multi-Antenna Adversary  and Delay Constraints,"Covert Wireless Communication in Presence of a Multi-Antenna Adversary   and Delay Constraints  Covert communication hides the transmission of a message from a watchful adversary while ensuring reliable information decoding at the receiver, providing enhanced security in wireless communications. In this letter, covert communication in the presence of a multi-antenna adversary and under delay constraints is considered. Under the assumption of quasi-static wireless fading channels, we analyze the effect of increasing the number of antennas employed at the adversary on the achievable throughput of covert communication. It is shown that in contrast to a single-antenna adversary, a slight increase in the number of adversary's antennas drastically reduces the covert throughput, even for relaxed covertness requirements. ",2019,communications,1.0682428,5.9835205,2016-2020,5,#7402B1
274,Communication and Interference Coordination,"Communication and Interference Coordination  We study the problem of controlling the interference created to an external observer by a communication processes. We model the interference in terms of its type (empirical distribution), and we analyze the consequences of placing constraints on the admissible type. Considering a single interfering link, we characterize the communication-interference capacity region. Then, we look at a scenario where the interference is jointly created by two users allowed to coordinate their actions prior to transmission. In this case, the trade-off involves communication and interference as well as coordination. We establish an achievable communication-interference region and show that efficiency is significantly improved by coordination. ",2014,communications,7.6369805,4.744304,2011-2015,4,#9100CF
275,Communication by means of Modulated Johnson Noise,"Communication by means of Modulated Johnson Noise  We present the design of a new passive wireless communication system that does not rely on ambient or generated RF sources. Instead, we exploit the Johnson (thermal) noise generated by a resistor to transmit information bits wirelessly. By switching the load connected to an antenna between a resistor and open circuit, we can achieve data rates of up to 26bps and distances of up to 7.3 meters. This communication method is orders of magnitude less power consuming than conventional communication schemes and presents the opportunity to enable wireless communication in areas with a complete lack of connectivity. ",2021,communications,1.3880965,5.809365,2020-,6,#5E0897
276,Communicating Lists Over a Noisy Channel,"Communicating Lists Over a Noisy Channel  This work considers a communication scenario where the transmitter chooses a list of size K from a total of M messages to send over a noisy communication channel, the receiver generates a list of size L and communication is considered successful if the intersection of the lists at two terminals has cardinality greater than a threshold T. In traditional communication systems K=L=T=1. The fundamental limits of this setup in terms of K, L, T and the Shannon capacity of the channel between the terminals are examined. Specifically, necessary and/or sufficient conditions for asymptotically error free communication are provided. ",2014,communications,1.4599173,5.4659543,2011-2015,4,#9100CF
277,Vehicle infrastructure -- communication to support driver assistance  systems,"Vehicle infrastructure -- communication to support driver assistance   systems  The area of vehicle-to-infrastructure (C2I) communication has become an increasingly important area in the field of C2X communication in recent years. It is on the same level as vehicle-to-vehicle (C2C) communication and uses the same technologies and protocols. In today's research environment, the consensus is that IEEE standard 802.11p is used for communication. Based on this, the Car-2-Car Communication Consortium is developing a unified protocol stack for effective and efficient C2X communication. Roadside units (RSU) can be used to support driver assistance systems. Through targeted information processing and information distribution, they are intended to expand the vehicle's capabilities through additional data and also provide information that could not be made available through C2C communication alone. ",2021,communications,1.6367025,6.4937944,2020-,6,#5E0897
278,On decoding of digital data sent over a noisy MIMO channel,"On decoding of digital data sent over a noisy MIMO channel  The transmission of digital data is one of the principal tasks in modern wireless communication. Classically, the communication channel consists of one transmitter and one receiver; however, due to the constantly increasing demand in higher transmission rates, the popularity of using several receivers and transmitters has been rapidly growing.   In this paper, we combine a number of fairly standard techniques from numerical linear algebra and probability to develop several (apparently novel) randomized schemes for the decoding of digital messages sent over a noisy multivariate Gaussian channel.   We use a popular mathematical model for such channels to illustrate the performance of our schemes via numerical experiments. ",2015,communications,1.2527865,5.4433436,2016-2020,5,#7402B1
279,Danger Aware Vehicular Networking,"Danger Aware Vehicular Networking  IEEE 802.11p is one of the key technologies that enable Dedicated Short-Range Communications (DSRC) in intelligent transportation system (ITS) for safety on the road. The main challenge in vehicular communication is the large amount of data to be processed. As vehicle density and velocity increases, the data to be transmitted also increases. We proposed a protocol that reduces the number of messages transmitted at a vehicle according to the level of danger that the vehicle experiences. The proposed protocol measures inter-vehicle distance, as the representative of the danger of a vehicle, to determine the priority for transmission. Our results show that this prioritization of transmissions directly reduces the number of transmitters at a time, and hence results in higher performance in terms of key metrics--i.e., PDR, throughput, delay, probabilities of channel busy and collision. ",2020,communications,1.4241315,6.0790386,2020-,6,#5E0897
280,Bilevel Optimization for Traffic Mitigation in Optimal Transport  Networks,"Bilevel Optimization for Traffic Mitigation in Optimal Transport   Networks  Global infrastructure robustness and local transport efficiency are critical requirements for transportation networks. However, since passengers often travel greedily to maximize their own benefit and trigger traffic jams, overall transportation performance can be heavily disrupted. We develop adaptation rules that leverage Optimal Transport theory to effectively route passengers along their shortest paths while also strategically tuning edge weights to optimize traffic. As a result, we enforce both global and local optimality of transport. We prove the efficacy of our approach on synthetic networks and on real data. Our findings on the International European highways reveal that our method results in an effective strategy to lower car-produced carbon emissions. ",2023,infrastructure mitigation,2.5768554,6.288075,2020-,6,#5E0897
281,Advanced Metering Infrastructures: Security Risks and Mitigation,"Advanced Metering Infrastructures: Security Risks and Mitigation  Energy providers are moving to the smart meter era, encouraging consumers to install, free of charge, these devices in their homes, automating consumption readings submission and making consumers life easier. However, the increased deployment of such smart devices brings a lot of security and privacy risks. In order to overcome such risks, Intrusion Detection Systems are presented as pertinent tools that can provide network-level protection for smart devices deployed in home environments. In this context, this paper is exploring the problems of Advanced Metering Infrastructures (AMI) and proposing a novel Machine Learning (ML) Intrusion Prevention System (IPS) to get optimal decisions based on a variety of factors and graphical security models able to tackle zero-day attacks. ",2021,infrastructure mitigation,1.3503861,6.9624605,2020-,6,#5E0897
282,The Flood Mitigation Problem in a Road Network,"The Flood Mitigation Problem in a Road Network  Natural disasters are highly complex and unpredictable. However, long-term planning and preparedness activities can help to mitigate the consequences and reduce the damage. For example, in cities with a high risk of flooding, appropriate roadway mitigation can help reduce the impact of floods or high waters on transportation systems. Such communities could benefit from a comprehensive assessment of mitigation on road networks and identification of the best subset of roads to mitigate. In this study, we address a pre-disaster planning problem that seeks to strengthen a road network against flooding. We develop a network design problem that maximizes the improvement in accessibility and travel times between population centers and healthcare facilities subject to a given budget. We provide techniques for reducing the problem size to help make the problem tractable. We use cities in the state of Iowa in our computational experiments. ",2023,infrastructure mitigation,2.4267225,6.480196,2020-,6,#5E0897
283,Mitigating the Performance Impact of Network Failures in Public Clouds,"Mitigating the Performance Impact of Network Failures in Public Clouds  Some faults in data center networks require hours to days to repair because they may need reboots, re-imaging, or manual work by technicians. To reduce traffic impact, cloud providers \textit{mitigate} the effect of faults, for example, by steering traffic to alternate paths. The state-of-art in automatic network mitigations uses simple safety checks and proxy metrics to determine mitigations. SWARM, the approach described in this paper, can pick orders of magnitude better mitigations by estimating end-to-end connection-level performance (CLP) metrics. At its core is a scalable CLP estimator that quickly ranks mitigations with high fidelity and, on failures observed at a large cloud provider, outperforms the state-of-the-art by over 700$\times$ in some cases. ",2023,infrastructure mitigation,1.597851,6.405994,2020-,6,#5E0897
284,Intelligent Transportation Systems to Mitigate Road Traffic Congestion,"Intelligent Transportation Systems to Mitigate Road Traffic Congestion  Intelligent transport systems have efficiently and effectively proved themselves in settling up the problem of traffic congestion around the world. The multi-agent based transportation system is one of the most important intelligent transport systems, which represents an interaction among the neighbouring vehicles, drivers, roads, infrastructure and vehicles. In this paper, two traffic management models have been created to mitigate congestion and to ensure that emergency vehicles arrive as quickly as possible. A tool-chain SUMO-JADE is employed to create a microscopic simulation symbolizing the interactions of traffic. The simulation model has showed a significant reduction of at least 50% in the average time delay and thus a real improvement in the entire journey time. ",2021,infrastructure mitigation,2.5827296,6.604828,2020-,6,#5E0897
285,On Auxiliary Entity Allocation Problem in Multi-layered Interdependent  Critical Infrastructures,"On Auxiliary Entity Allocation Problem in Multi-layered Interdependent   Critical Infrastructures  Operation of critical infrastructures are highly interdependent on each other. Such dependencies causes failure in these infrastructures to cascade on an initial failure event. Owing to this vulnerability it is imperative to incorporate efficient strategies for their protection. Modifying dependencies by adding additional dependency implications using entities (termed as \emph{auxiliary entities}) is shown to mitigate this issue to a certain extent. With this finding, in this article we introduce the Auxiliary Entity Allocation problem. The objective is to maximize protection in Power and Communication infrastructures using a budget in number of dependency modifications using the auxiliary entities. The problem is proved to be NP-complete in general case. We provide an optimal solution using Integer Linear program and a heuristic for a restricted case. The efficacy of heuristic with respect to the optimal is judged through experimentation using real world data sets with heuristic deviating $6.75 \%$ from optimal on average. ",2017,infrastructure mitigation,1.8435776,5.8219104,2016-2020,5,#7402B1
286,Critical Infrastructure Protection: having SIEM technology cope with  network heterogeneity,"Critical Infrastructure Protection: having SIEM technology cope with   network heterogeneity  Coordinated and targeted cyber-attacks to Critical Infrastructures (CIs) are becoming more and more frequent and sophisticated. This is due to: i) the recent technology shift towards Commercial Off-The-Shelf (COTS) products, and ii) new economical and socio-political motivations. In this paper, we discuss some of the most relevant security issues resulting from the adoption in CIs of heterogeneous network infrastructures (specifically combining wireless and IP trunks), and suggest techniques to detect, as well as to counter/mitigate attacks. We claim that techniques such as those we propose here should be integrated in future SIEM (Security Information and Event Management) solutions, and we discuss how we have done so in the EC-funded MASSIF project, with respect to a real-world CI scenario, specifically a distributed system for power grid monitoring. ",2014,infrastructure mitigation,1.3001301,6.859961,2011-2015,4,#9100CF
287,Implementing RFI mitigation in Radio Science,"Implementing RFI mitigation in Radio Science  This paper presents an overview of methods for mitigating radio frequency interference (RFI) in radio science data. The primary purpose of mitigation is to assist observatories to take useful data outside frequency bands allocated to the Science Services (RAS and EESS): mitigation should not be needed within Passive bands. Mitigation methods may be introduced at a variety of points within the data acquisition system in order to lessen the RFI intensity and to limit the damage it does. These methods range from proactive methods to change the local RFI environment by means of regulatory manners, to pre- and post-detection methods, to various pre-processing methods, and to methods applied at or post-processing. ",2023,infrastructure mitigation,1.3120593,5.7706885,2020-,6,#5E0897
288,Interdependent Infrastructure System Risk and Resilience to Natural  Hazards,"Interdependent Infrastructure System Risk and Resilience to Natural   Hazards  Complex, interdependent systems are necessary to the delivery of goods and services critical to societal function. Here we demonstrate how interdependent systems respond to disruptions. Specifically, we change the spatial arrangement of a disruption in infrastructure and show that -- while controlling for the size -- changes in the spatial pattern of a disruption induce significant changes in the way interdependent systems fail and recover. This work demonstrates the potential to improve characterizations of hazard disruption to infrastructure by incorporating additional information about the impact of disruptions on interdependent systems. ",2019,infrastructure mitigation,2.254706,6.614321,2016-2020,5,#7402B1
289,Delusion and Deception in Large Infrastructure Projects: Two Models for  Explaining and Preventing Executive Disaster,"Delusion and Deception in Large Infrastructure Projects: Two Models for   Explaining and Preventing Executive Disaster  The Economist recently reported that infrastructure spending is the largest it is ever been as a share of world GDP. With $22 trillion in projected investments over the next ten years in emerging economies alone, the magazine calls it the ""biggest investment boom in history."" The efficiency of infrastructure planning and execution is therefore particularly important at present. Unfortunately, the private sector, the public sector and private/public sector partnerships have a dismal record of delivering on large infrastructure cost and performance promises. This paper explains why and how to solve the problem. ",2013,infrastructure mitigation,1.9153028,7.1031146,2011-2015,4,#9100CF
290,Learning and correcting non-Gaussian model errors,"Learning and correcting non-Gaussian model errors  All discretized numerical models contain modelling errors - this reality is amplified when reduced-order models are used. The ability to accurately approximate modelling errors informs statistics on model confidence and improves quantitative results from frameworks using numerical models in prediction, tomography, and signal processing. Further to this, the compensation of highly nonlinear and non-Gaussian modelling errors, arising in many ill-conditioned systems aiming to capture complex physics, is a historically difficult task. In this work, we address this challenge by proposing a neural network approach capable of accurately approximating and compensating for such modelling errors in augmented direct and inverse problems. The viability of the approach is demonstrated using simulated and experimental data arising from differing physical direct and inverse problems. ",2020,error modeling ,3.7585695,6.402737,2020-,6,#5E0897
291,Embedded Model Error Representation for Bayesian Model Calibration,"Embedded Model Error Representation for Bayesian Model Calibration  Model error estimation remains one of the key challenges in uncertainty quantification and predictive science. For computational models of complex physical systems, model error, also known as structural error or model inadequacy, is often the largest contributor to the overall predictive uncertainty. This work builds on a recently developed framework of embedded, internal model correction, in order to represent and quantify structural errors, together with model parameters, within a Bayesian inference context. We focus specifically on a Polynomial Chaos representation with additive modification of existing model parameters, enabling a non-intrusive procedure for efficient approximate likelihood construction, model error estimation, and disambiguation of model and data errors' contributions to predictive uncertainty. The framework is demonstrated on several synthetic examples, as well as on a chemical ignition problem. ",2018,error modeling ,4.025739,6.2540793,2016-2020,5,#7402B1
292,Bayesian Measurement Error Models Using Finite Mixtures of Scale  Mixtures of Skew-Normal Distributions,"Bayesian Measurement Error Models Using Finite Mixtures of Scale   Mixtures of Skew-Normal Distributions  We present a proposal to deal with the non-normality issue in the context of regression models with measurement errors when both the response and the explanatory variable are observed with error. We extend the normal model by jointly modeling the unobserved covariate and the random errors by a finite mixture of scale mixture of skew-normal distributions. This approach allows us to model data with great flexibility, accommodating skewness, heavy tails, and multi-modality. ",2020,error modeling ,4.1117454,6.040341,2020-,6,#5E0897
293,On Error Estimation for Reduced-order Modeling of Linear Non-parametric  and Parametric Systems,"On Error Estimation for Reduced-order Modeling of Linear Non-parametric   and Parametric Systems  Motivated by a recently proposed error estimator for the transfer function of the reduced-order model of a given linear dynamical system, we further develop more theoretical results in this work. Furthermore, we propose several variants of the error estimator, and compare those variants with the existing ones both theoretically and numerically. It has been shown that some of the proposed error estimators perform better than or equally well as the existing ones. All the error estimators considered can be easily extended to estimate output error of reduced-order modeling for steady linear parametric systems. ",2020,error modeling ,4.1877055,5.767117,2020-,6,#5E0897
294,An analytical error model for quantum computer simulation,"An analytical error model for quantum computer simulation  Quantum computers (QCs) must implement quantum error correcting codes (QECCs) to protect their logical qubits from errors, and modeling the effectiveness of QECCs on QCs is an important problem for evaluating the QC architecture. The previously developed Monte Carlo (MC) error models may take days or weeks of execution to produce an accurate result due to their random sampling approach. We present an alternative analytical error model that generates, over the course of executing the quantum program, a probability tree of the QC's error states. By calculating the fidelity of the quantum program directly, this error model has the potential for enormous speedups over the MC model when applied to small yet useful problem sizes. We observe a speedup on the order of 1,000X when accuracy is required, and we evaluate the scaling properties of this new analytical error model. ",2007,error modeling ,6.657974,3.6102524,2006-2010,3,#FEF65C
295,iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models,"iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models  Error analysis in NLP models is essential to successful model development and deployment. One common approach for diagnosing errors is to identify subpopulations in the dataset where the model produces the most errors. However, existing approaches typically define subpopulations based on pre-defined features, which requires users to form hypotheses of errors in advance. To complement these approaches, we propose iSEA, an Interactive Pipeline for Semantic Error Analysis in NLP Models, which automatically discovers semantically-grounded subpopulations with high error rates in the context of a human-in-the-loop interactive system. iSEA enables model developers to learn more about their model errors through discovered subpopulations, validate the sources of errors through interactive analysis on the discovered subpopulations, and test hypotheses about model errors by defining custom subpopulations. The tool supports semantic descriptions of error-prone subpopulations at the token and concept level, as well as pre-defined higher-level features. Through use cases and expert interviews, we demonstrate how iSEA can assist error understanding and analysis. ",2022,error modeling ,2.6672556,7.9217553,2020-,6,#5E0897
296,An Error Analysis Framework for Neural Network Modeling of Dynamical  Systems,"An Error Analysis Framework for Neural Network Modeling of Dynamical   Systems  We propose a theoretical framework for investigating a modeling error caused by numerical integration in the learning process of dynamics. Recently, learning equations of motion to describe dynamics from data using neural networks has been attracting attention. During such training, numerical integration is used to compare the data with the solution of the neural network model; however, discretization errors due to numerical integration prevent the model from being trained correctly. In this study, we formulate the modeling error using the Dahlquist test equation that is commonly used in the analysis of numerical methods and apply it to some of the Runge--Kutta methods. ",2021,error modeling ,4.303096,5.8244343,2020-,6,#5E0897
297,Error estimation in astronomy: A guide,"Error estimation in astronomy: A guide  Estimating errors is a crucial part of any scientific analysis. Whenever a parameter is estimated (model-based or not), an error estimate is necessary. Any parameter estimate that is given without an error estimate is meaningless. Nevertheless, many (undergraduate or graduate) students have to teach such methods for error estimation to themselves when working scientifically for the first time. This manuscript presents an easy-to-understand overview of different methods for error estimation that are applicable to both model-based and model-independent parameter estimates. These methods are not discussed in detail, but their basics are briefly outlined and their assumptions carefully noted. In particular, the methods for error estimation discussed are grid search, varying $\chi^2$, the Fisher matrix, Monte-Carlo methods, error propagation, data resampling, and bootstrapping. Finally, a method is outlined how to propagate measurement errors through complex data-reduction pipelines. ",2010,error modeling ,4.135244,6.1940603,2006-2010,3,#FEF65C
298,A Bayesian Approach to Identifying Representational Errors,"A Bayesian Approach to Identifying Representational Errors  Trained AI systems and expert decision makers can make errors that are often difficult to identify and understand. Determining the root cause for these errors can improve future decisions. This work presents Generative Error Model (GEM), a generative model for inferring representational errors based on observations of an actor's behavior (either simulated agent, robot, or human). The model considers two sources of error: those that occur due to representational limitations -- ""blind spots"" -- and non-representational errors, such as those caused by noise in execution or systematic errors present in the actor's policy. Disambiguating these two error types allows for targeted refinement of the actor's policy (i.e., representational errors require perceptual augmentation, while other errors can be reduced through methods such as improved training or attention support). We present a Bayesian inference algorithm for GEM and evaluate its utility in recovering representational errors on multiple domains. Results show that our approach can recover blind spots of both reinforcement learning agents as well as human users. ",2021,error modeling ,3.0300527,7.131792,2020-,6,#5E0897
299,"Model error and its estimation, with particular application to loss  reserving","Model error and its estimation, with particular application to loss   reserving  This paper is concerned with forecast error, particularly in relation to loss reserving. This is generally regarded as consisting of three components, namely parameter, process and model errors. The first two of these components, and their estimation, are well understood, but less so model error. Model error itself is considered in two parts: one part that is capable of estimation from past data (internal model error), and another part that is not (external model error). Attention is focused here on internal model error. Estimation of this error component is approached by means of Bayesian model averaging, using the Bayesian interpretation of the LASSO. This is used to generate a set of admissible models, each with its prior probability and the likelihood of observed data. A posterior on the model set, conditional on the data, results, and an estimate of model error (contained in a loss reserve) is obtained as the variance of the loss reserve according to this posterior. The population of models entering materially into the support of the posterior may turn out to be thinner than desired, and bootstrapping of the LASSO is used to gain bulk. This provides the bonus of an estimate of parameter error also. It turns out that the estimates of parameter and model errors are entangled, and dissociation of them is at least difficult, and possibly not even meaningful. These matters are discussed. The majority of the discussion applies to forecasting generally, but numerical illustration of the concepts is given in relation to insurance data and the problem of insurance loss reserving. ",2022,error modeling ,4.042855,6.1728716,2020-,6,#5E0897
300,eXplainable Artificial Intelligence (XAI) in aging clock models,"eXplainable Artificial Intelligence (XAI) in aging clock models  eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of machine learning, aiming to unravel the predictions of complex models. XAI is especially required in sensitive applications, e.g. in health care, when diagnosis, recommendations and treatment choices might rely on the decisions made by artificial intelligence systems. AI approaches have become widely used in aging research as well, in particular, in developing biological clock models and identifying biomarkers of aging and age-related diseases. However, the potential of XAI here awaits to be fully appreciated. We discuss the application of XAI for developing the ""aging clocks"" and present a comprehensive analysis of the literature categorized by the focus on particular physiological systems. ",2023,xai,1.9443382,7.6198072,2020-,6,#5E0897
301,Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and  Future Opportunities,"Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and   Future Opportunities  The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that identified challenges and potential research directions in XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey for challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions in XAI and (2) challenges and research directions in XAI based on machine learning life cycle's phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area. ",2021,xai,1.9302995,7.594084,2020-,6,#5E0897
302,Explainable AI: current status and future directions,"Explainable AI: current status and future directions  Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other ""wh"" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided. ",2021,xai,1.9757098,7.718235,2020-,6,#5E0897
303,Explaining and visualizing black-box models through counterfactual paths,"Explaining and visualizing black-box models through counterfactual paths  Explainable AI (XAI) is an increasingly important area of machine learning research, which aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses the so-called counterfactual paths generated by conditional permutations of features. The algorithm measures feature importance by identifying sequential permutations of features that most influence changes in model predictions. It is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs incorporating domain knowledge. Counterfactual paths introduce an additional graph dimension to current XAI methods in both explaining and visualizing black-box models. Experiments with synthetic and medical data demonstrate the practical applicability of our approach. ",2023,xai,2.6386173,7.7981896,2020-,6,#5E0897
304,A Survey on Explainable Artificial Intelligence for Cybersecurity,"A Survey on Explainable Artificial Intelligence for Cybersecurity  The black-box nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of network cybersecurity, XAI has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of network-driven cybersecurity threats and issues. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research. ",2023,xai,1.3375114,7.2340093,2020-,6,#5E0897
305,Explainable artificial intelligence (XAI) in deep learning-based medical  image analysis,"Explainable artificial intelligence (XAI) in deep learning-based medical   image analysis  With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of eXplainable Artificial Intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis. ",2021,xai,3.167099,8.217123,2020-,6,#5E0897
306,Beyond Explaining: Opportunities and Challenges of XAI-Based Model  Improvement,"Beyond Explaining: Opportunities and Challenges of XAI-Based Model   Improvement  Explainable Artificial Intelligence (XAI) is an emerging research field bringing transparency to highly complex and opaque machine learning (ML) models. Despite the development of a multitude of methods to explain the decisions of black-box classifiers in recent years, these tools are seldomly used beyond visualization purposes. Only recently, researchers have started to employ explanations in practice to actually improve models. This paper offers a comprehensive overview over techniques that apply XAI practically for improving various properties of ML models, and systematically categorizes these approaches, comparing their respective strengths and weaknesses. We provide a theoretical perspective on these methods, and show empirically through experiments on toy and realistic settings how explanations can help improve properties such as model generalization ability or reasoning, among others. We further discuss potential caveats and drawbacks of these methods. We conclude that while model improvement based on XAI can have significant beneficial effects even on complex and not easily quantifyable model properties, these methods need to be applied carefully, since their success can vary depending on a multitude of factors, such as the model and dataset used, or the employed explanation method. ",2022,xai,2.8105485,7.3863583,2020-,6,#5E0897
307,Reviewing the Need for Explainable Artificial Intelligence (xAI),"Reviewing the Need for Explainable Artificial Intelligence (xAI)  The diffusion of artificial intelligence (AI) applications in organizations and society has fueled research on explaining AI decisions. The explainable AI (xAI) field is rapidly expanding with numerous ways of extracting information and visualizing the output of AI technologies (e.g. deep neural networks). Yet, we have a limited understanding of how xAI research addresses the need for explainable AI. We conduct a systematic review of xAI literature on the topic and identify four thematic debates central to how xAI addresses the black-box problem. Based on this critical analysis of the xAI scholarship we synthesize the findings into a future research agenda to further the xAI body of knowledge. ",2020,xai,1.7989643,7.599778,2020-,6,#5E0897
308,XAIR: A Framework of Explainable AI in Augmented Reality,"XAIR: A Framework of Explainable AI in Augmented Reality  Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses ""when"", ""what"", and ""how"" to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users' preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR's utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR. ",2023,xai,1.9266292,7.339458,2020-,6,#5E0897
309,Strategies to exploit XAI to improve classification systems,"Strategies to exploit XAI to improve classification systems  Explainable Artificial Intelligence (XAI) aims to provide insights into the decision-making process of AI models, allowing users to understand their results beyond their decisions. A significant goal of XAI is to improve the performance of AI models by providing explanations for their decision-making processes. However, most XAI literature focuses on how to explain an AI system, while less attention has been given to how XAI methods can be exploited to improve an AI system. In this work, a set of well-known XAI methods typically used with Machine Learning (ML) classification tasks are investigated to verify if they can be exploited, not just to provide explanations but also to improve the performance of the model itself. To this aim, two strategies to use the explanation to improve a classification system are reported and empirically evaluated on three datasets: Fashion-MNIST, CIFAR10, and STL10. Results suggest that explanations built by Integrated Gradients highlight input features that can be effectively used to improve classification performance. ",2023,xai,2.5484736,7.5284495,2020-,6,#5E0897
310,Power-Consumption Outage Challenge in Next-Generation Cellular Networks,"Power-Consumption Outage Challenge in Next-Generation Cellular Networks  The conventional outage in wireless communication systems is caused by the deterioration of the wireless communication link, i.e., the received signal power is less than the minimum received signal power. Is there a possibility that the outage occurs in wireless communication systems with a good channel state? Based on both communication and heat transfer theories, a power-consumption outage in the wireless communication between millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) base stations (BSs) and smartphones has been modeled and analyzed. Moreover, the total transmission time model with respect to the number of power-consumption outages is derived for mmWave massive MIMO communication systems. Simulation results indicate that the total transmission time is extended by the power-consumption outage, which deteriorates the average transmission rate of mmWave massive MIMO BSs. ",2019,power outages,1.4665294,5.8205833,2016-2020,5,#7402B1
311,Down for Failure: Active Power Status Monitoring,"Down for Failure: Active Power Status Monitoring  Despite society's strong dependence on electricity, power outages remain prevalent. Standard methods for directly measuring power availability are complex, often inaccurate, and are prone to attack. This paper explores an alternative approach to identifying power outages through intelligent monitoring of IP address availability. In finding these outages, we explore the trade-off between the accuracy of detection and false alarms.   We begin by experimentally demonstrating that static, residential Internet connections serve as good indicators of power, as they are mostly active unless power fails and rarely have battery backups. We construct metrics that dynamically score the reliability of each residential IP, where a higher score indicates a higher correlation between that IP's availability and its regional power. We monitor specifically selected subsets of residential IPs and evaluate the accuracy with which they can indicate current county power status.   Using data gathered during the power outages caused by Hurricane Florence, we demonstrate that we can track power outages at different granularities, state and county, in both sparse and dense regions. By comparing our detection with the reports gathered from power utility companies, we achieve an average detection accuracy of $90\%$, where we also show some of our false alarms and missed outage events could be due to imperfect ground truth data. Therefore, our method can be used as a complementary technique of power outage detection. ",2019,power outages,2.3435533,6.1040893,2016-2020,5,#7402B1
312,U.S. Power Resilience for 2002--2019,"U.S. Power Resilience for 2002--2019  Prolonged power outages debilitate the economy and threaten public health. Existing research is generally limited in its scope to a single event, an outage cause, or a region. Here, we provide one of the most comprehensive analyses of U.S. power outages for 2002--2019. We categorized all outage data collected under U.S. federal mandates into four outage causes and computed industry-standard reliability metrics. Our spatiotemporal analysis reveals six of the most resilient U.S. states since 2010, improvement of power resilience against natural hazards in the south and northeast regions, and a disproportionately large number of human attacks for its population in the Western Electricity Coordinating Council region. Our regression analysis identifies several statistically significant predictors and hypotheses for power resilience. Furthermore, we propose a novel framework for analyzing outage data using differential weighting and influential points to better understand power resilience. We share curated data and code as Supplementary Materials. ",2021,power outages,2.2144825,5.956851,2020-,6,#5E0897
313,Load Dependence of Power Outage Statistics,"Load Dependence of Power Outage Statistics  The size distributions of power outages are shown to depend on the stress, or the proximity of the load of an electrical grid to complete breakdown. Using the data for the U.S. between 2002-2017, we show that the outage statistics are dependent on the usage levels during different hours of the day and months of the year. At higher load, not only are more failures likely, but the distribution of failure sizes shifts, to favor larger events. At a finer spatial scale, different regions within the U.S. can be shown to respond differently in terms of the outage statistics to variations in the usage (load). The response, in turn, corresponds to the respective bias towards larger or smaller failures in those regions. We provide a simple model, using realistic grid topologies, which can nonetheless demonstrate biases as a function of the applied load, as in the data. Given sufficient data of small scale events, the method can be used to identify vulnerable regions in power grids prior to major blackouts. ",2018,power outages,2.1373205,6.0256867,2016-2020,5,#7402B1
314,Outage Probability of Power-based Non-Orthogonal Multiple Access (NOMA)  on the Uplink of a 5G Cell,"Outage Probability of Power-based Non-Orthogonal Multiple Access (NOMA)   on the Uplink of a 5G Cell  This letter puts forth an analytical approach to evaluate the outage probability of power-based NOMA on the uplink of a 5G cell, the outage being defined as the event where the receiver fails to successfully decode all the simultaneously received signals. In the examined scenario, Successive Interference Cancellation (SIC) is considered and an arbitrary number of superimposed signals is present. For the Rayleigh fading case, the outage probability is provided in closed-form, clearly outlining its dependency on the signal-to-noise ratio of the users that are simultaneously transmitting, as well as on their distance from the receiver. ",2017,power outages,1.2670482,5.6297,2016-2020,5,#7402B1
315,Blackout Resilient Optical Core Network,"Blackout Resilient Optical Core Network  A disaster may not necessarily demolish the telecommunications infrastructure, but instead it might affect the national grid and cause blackouts, consequently disrupting the network operation unless there is an alternative power source(s). In this paper, power outages are considered, and the telecommunication network performance is evaluated during a blackout. Two approaches are presented to minimize the impact of power outage and maximize the survival time of the blackout node. A mixed integer linear programming (MILP) model is developed to evaluate the network performance under a single node blackout scenario. The model is used to evaluate the network under the two proposed scenarios. The results show that the proposed approach succeeds in extending the network life time while minimizing the required amount of backup energy. ",2020,power outages,1.6105255,5.900001,2020-,6,#5E0897
316,Lassoing Line Outages in the Smart Power Grid,"Lassoing Line Outages in the Smart Power Grid  Fast and accurate unveiling of power line outages is of paramount importance not only for preventing faults that may lead to blackouts, but also for routine monitoring and control tasks of the smart grid, including state estimation and optimal power flow. Existing approaches are either challenged by the \emph{combinatorial complexity} issues involved, and are thus limited to identifying single- and double-line outages; or, they invoke less pragmatic assumptions such as \emph{conditionally independent} phasor angle measurements available across the grid. Using only a subset of voltage phasor angle data, the present paper develops a near real-time algorithm for identifying multiple line outages at the affordable complexity of solving a quadratic program via block coordinate descent iterations. The novel approach relies on reformulating the DC linear power flow model as a \emph{sparse} overcomplete expansion, and leveraging contemporary advances in compressive sampling and variable selection using the least-absolute shrinkage and selection operator (Lasso). Analysis and simulated tests on the standard IEEE 118-bus system confirm the effectiveness of lassoing line changes in the smart power grid. ",2011,power outages,1.6415634,5.652924,2011-2015,4,#9100CF
317,Outing Power Outages: Real-time and Predictive Socio-demographic  Analytics for New York City,"Outing Power Outages: Real-time and Predictive Socio-demographic   Analytics for New York City  Electrical outages continue to occur despite technological innovations and improvements to electric power distribution infrastructure. In this paper, we describe a tool that was designed to acquire and collect data on electric power outages in New York City since July 2020. The electrical outages are then displayed on a front-end application, which is publicly available. We use the collected outage data to analyze these outages and their socio-economic impacts on electricity vulnerable population groups. We determined that there was a slightly negative linear relationship between income and number of outages. Finally, a Markov Influence Graph was created to better understand the spatial and temporal relationships between outages. ",2022,power outages,3.3329515,7.118257,2020-,6,#5E0897
318,Power Off! Challenges in Planning and Executing Power Isolations on  Shared-Use Electrified Railways,"Power Off! Challenges in Planning and Executing Power Isolations on   Shared-Use Electrified Railways  Electric railways are fast, clean, and safe, but complex to operate and maintain. Electric traction infrastructure includes signal power and feeder lines that remain live during isolations and complicate maintenance processes. Stakeholders involved in power outage planning include contractors, linemen, groundmen, power directors, dispatchers, conductor-flag, and support personnel. Weekly planning processes for track time requires many contingencies due to large number of moving parts and factors not known in advance, like personnel availability. Electrical and mechanical environments faced by crews working in adjacent areas may be entirely different and require a ""bespoke"" circuit configuration to de-energize catenary, which must be planned meticulously. Although recent automation improved real-time ""plate order"" communications between power directors and dispatchers, each outage still requires many manual switching operations. Net impact of this isolation process reduces available construction work windows nightly from a nominal 7 hours to 2 hrs 39 mins. We recommend joint design of electrical and civil infrastructure, cross-training between disciplines, limiting maximum number of concurrent outages, formal study of maintenance outage capacity, and further automation in power switching. ",2021,power outages,1.9055198,5.958092,2020-,6,#5E0897
319,Comparing Generator Unavailability Models with Empirical Distributions  from Open Energy Datasets,"Comparing Generator Unavailability Models with Empirical Distributions   from Open Energy Datasets  The modelling of power station outages is an integral part of power system planning. In this work, models of the unavailability of the fleets of eight countries in Northwest Europe are constructed and subsequently compared against empirical distributions derived using data from the open-access ENTSO-e Transparency Platform. Summary statistics of non-sequential models highlight limitations with the empirical modelling, with very variable results across countries. Additionally, analysis of time sequential models suggests a clear need for fleet-specific analytic model parameters. Despite a number of challenges and ambiguities associated with the empirical distributions, it is suggested that a range of valuable qualitative and quantitative insights can be gained by comparing these two complementary approaches for modelling and understanding generator unavailabilities. ",2022,power outages,2.1760094,5.8893986,2020-,6,#5E0897
320,Early warning via transitions in latent stochastic dynamical systems,"Early warning via transitions in latent stochastic dynamical systems  Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics. ",2023,early warning,4.0157785,6.681107,2020-,6,#5E0897
321,A Civil Protection Early Warning System to Improve the Resilience of  Adriatic-Ionian Territories to Natural and Man-made Risk,"A Civil Protection Early Warning System to Improve the Resilience of   Adriatic-Ionian Territories to Natural and Man-made Risk  We are currently witnessing an increased occurrence of extreme weather events, causing a great deal of disruption and distress across the globe. In this setting, the importance and utility of Early Warning Systems is becoming increasingly obvious. In this work, we present the design of an early warning system called TransCPEarlyWarning, aimed at seven countries in the Adriatic-Ionian area in Europe. The overall objective is to increase the level of cooperation among national civil protection institutions in these countries, addressing natural and man-made risks from the early warning stage and improving the intervention capabilities of civil protection mechanisms. The system utilizes an innovative approach with a lever effect, while also aiming to support the whole system of Civil Protection. ",2022,early warning,3.143402,7.0104017,2020-,6,#5E0897
322,The transformer earthquake alerting model: A new versatile approach to  earthquake early warning,"The transformer earthquake alerting model: A new versatile approach to   earthquake early warning  Earthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance notice of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts, and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer from apparent aleatoric uncertainties due to simplified modelling or short warning times. Here we propose a novel early warning method, the deep-learning based transformer earthquake alerting model (TEAM), to mitigate these limitations. TEAM analyzes raw, strong motion waveforms of an arbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing seismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard, Japan and Italy, that are complementary in their seismicity. On both datasets TEAM outperforms existing early warning methods considerably, offering accurate and timely warnings. Using domain adaptation, TEAM even provides reliable alerts for events larger than any in the training data, a property of highest importance as records from very large events are rare in many regions. ",2020,early warning,5.2530956,6.5263486,2020-,6,#5E0897
323,"""Shaking in 5 seconds!"" A Voluntary Smartphone-based Earthquake Early  Warning System","""Shaking in 5 seconds!"" A Voluntary Smartphone-based Earthquake Early   Warning System  Public earthquake early warning systems have the potential to reduce individual risk by warning people of an incoming tremor but their development has been hampered by costly infrastructure. Furthermore, users' understanding of such a service and their reactions to warnings remains poorly studied. The smartphone app of the Earthquake Network initiative turns users' smartphones into motion detectors and provides the first example of purely smartphone-based earthquake early warnings, without the need for dedicated seismic station infrastructure and operating in multiple countries. We demonstrate here that early warnings have been emitted in multiple countries even for damaging shaking levels and so this offers an alternative in the many regions unlikely to be covered by conventional early warning systems in the foreseeable future. We also show that although warnings are understood and appreciated by users, notably to get psychologically prepared, only a fraction take protective actions such as ""drop, cover and hold"". ",2021,early warning,5.255776,6.4506927,2020-,6,#5E0897
324,Sample-Efficient Safety Assurances using Conformal Prediction,"Sample-Efficient Safety Assurances using Conformal Prediction  When deploying machine learning models in high-stakes robotics applications, the ability to detect unsafe situations is crucial. Early warning systems can provide alerts when an unsafe situation is imminent (in the absence of corrective action). To reliably improve safety, these warning systems should have a provable false negative rate; i.e. of the situations that are unsafe, fewer than $\epsilon$ will occur without an alert. In this work, we present a framework that combines a statistical inference technique known as conformal prediction with a simulator of robot/environment dynamics, in order to tune warning systems to provably achieve an $\epsilon$ false negative rate using as few as $1/\epsilon$ data points. We apply our framework to a driver warning system and a robotic grasping application, and empirically demonstrate guaranteed false negative rate while also observing low false detection (positive) rate. ",2021,early warning,2.8281233,6.3522635,2020-,6,#5E0897
325,Early-warning indicators in the dynamic regime,Early-warning indicators in the dynamic regime  Early-warning indicators (increase of autocorrelation and variance) are commonly applied to time series data to try and detect tipping points of real-world systems. The theory behind these indicators originates from approximating the fluctuations around an equilibrium observed in time series data by a linear stationary (Ornstein-Uhlenbeck) process. Then for the approach of a bifurcation-type tipping point the formulas for the autocorrelation and variance of an Ornstein-Uhlenbeck process detect the phenomenon `critical slowing down'. The assumption of stationarity and linearity introduces two sources of error in the early-warning indicators. We investigate the difference between the theoretical and observed values for the early-warning indicators for the saddle-node normal form bifurcation with linear drift. ,2016,early warning,4.0666556,5.6237397,2016-2020,5,#7402B1
326,Early warning of pedestrians and cyclists,Early warning of pedestrians and cyclists  State-of-the-art motor vehicles are able to break for pedestrians in an emergency. We investigate what it would take to issue an early warning to the driver so he/she has time to react. We have identified that predicting the intention of a pedestrian reliably by position is a particularly hard challenge. This paper describes an early pedestrian warning demonstration system. ,2021,early warning,2.8978007,7.392022,2020-,6,#5E0897
327,Early warning signal for interior crises in excitable systems,"Early warning signal for interior crises in excitable systems  The ability to reliably predict critical transitions in dynamical systems is a long-standing goal of diverse scientific communities. Previous work focused on early warning signals related to local bifurcations (critical slowing down) and non-bifurcation type transitions. We extend this toolbox and report on a characteristic scaling behavior (critical attractor growth) which is indicative of an impending global bifurcation, an interior crisis in excitable systems. We demonstrate our early warning signal in a conceptual climate model as well as in a model of coupled neurons known to exhibit extreme events. We observed critical attractor growth prior to interior crises of chaotic as well as strange-nonchaotic attractors. These observations promise to extend the classes of transitions that can be predicted via early warning signals. ",2017,early warning,5.277716,5.9195886,2016-2020,5,#7402B1
328,Crowd-Funded Earthquake Early-Warning System,"Crowd-Funded Earthquake Early-Warning System  Earthquake early warning systems has been proven to save countless lives in Japan, Mexico, and Chile, where earthquake warnings are often broadcast live on TV up to a minute before residents experience shaking.   Unfortunately, traditional early warning systems require extensive capital investment. The high cost of traditional earthquake early-warning systems and limited budgets prevent earthquake-prone developing countries like the Philippines, Indonesia, Afghanistan, India, Burma, Ghana, Nigeria, Columbia, Venezuela, and Bolivia from building traditional earthquake warning systems.   This project describes repurposing old Android smartphones into affordable dedicated seismometers to detect tremors. These smartphones have become disposable items and are continuously ""upgraded"" and replaced. Yet every one of these devices includes everything needed to act as a dedicated seismometer: Wi-Fi capability, GPS, and an accelerometer. The software developed for this project converts these smartphones into dedicated seismometers and uses existing web technologies for telemetry services. This system would also trigger alerts to all devices that have the software installed whenever a tremor is detected, effectively making each seismic detection station double as an earthquake early-warning alarm.   A large network of these seismic detection stations will effectively create an affordable earthquake early-warning system that can be rapidly implemented at an extremely low cost. It would provide developing nations an affordable life-saving alternative to expensive traditional earthquake early-warning systems. This solution is cheap, keeps old smartphones from landfills, and will save lives. ",2022,early warning,4.82484,6.8256793,2020-,6,#5E0897
329,"Real-time Earthquake Early Warning with Deep Learning: Application to  the 2016 Central Apennines, Italy Earthquake Sequence","Real-time Earthquake Early Warning with Deep Learning: Application to   the 2016 Central Apennines, Italy Earthquake Sequence  Earthquake early warning systems are required to report earthquake locations and magnitudes as quickly as possible before the damaging S wave arrival to mitigate seismic hazards. Deep learning techniques provide potential for extracting earthquake source information from full seismic waveforms instead of seismic phase picks. We developed a novel deep learning earthquake early warning system that utilizes fully convolutional networks to simultaneously detect earthquakes and estimate their source parameters from continuous seismic waveform streams. The system determines earthquake location and magnitude as soon as one station receives earthquake signals and evolutionarily improves the solutions by receiving continuous data. We apply the system to the 2016 Mw 6.0 earthquake in Central Apennines, Italy and its subsequent sequence. Earthquake locations and magnitudes can be reliably determined as early as four seconds after the earliest P phase, with mean error ranges of 6.8-3.7 km and 0.31-0.23, respectively. ",2020,early warning,5.2871943,6.511113,2020-,6,#5E0897
330,Elastic Collision Based Dynamic Partitioning Scheme for Hybrid  Simulations,"Elastic Collision Based Dynamic Partitioning Scheme for Hybrid   Simulations  The scattering-adapted flexible inner region ensemble separator (SAFIRES) is a partitioning scheme designed to divide a simulation cell into two regions to be treated with different computational methodologies. SAFIRES prevents particles from crossing between regions and resolves boundary events through elastic collisions of the particles mediated by the boundary, conserving energy and momenta. A multiple-time-step propagation algorithm is introduced where the time step is scaled automatically to identify the moment a collision occurs. If the length of the time step is kept constant, the new propagator reduces to a regular algorithm for Langevin dynamics, and to the velocity Verlet algorithm for classical dynamics if the friction coefficient is set to zero. SAFIRES constitutes the exact limit of the premise behind boundary-based methods such as FIRES, BEST, and BCC which take advantage of the indistinguishability of molecules on opposite sides of the separator. It gives correct average ensemble statistics despite the introduction of an ensemble separator. SAFIRES is tested in simulations where the molecules on the two sides are treated in the same way, for a Lennard-Jones (LJ) liquid and a LJ liquid in contact with a surface, as well as for liquid modelling simulations using the TIP4P force field. Simulations using SAFIRES are shown to reproduce the unconstrained reference simulations without significant deviations. ",2021,fires,6.4346504,5.8978705,2020-,6,#5E0897
331,Algorithm FIRE -- Feynman Integral REduction,"Algorithm FIRE -- Feynman Integral REduction  The recently developed algorithm FIRE performs the reduction of Feynman integrals to master integrals. It is based on a number of strategies, such as applying the Laporta algorithm, the s-bases algorithm, region-bases and integrating explicitly over loop momenta when possible. Currently it is being used in complicated three-loop calculations. ",2008,fires,5.526381,2.7457774,2006-2010,3,#FEF65C
332,FIRE6: Feynman Integral REduction with Modular Arithmetic,FIRE6: Feynman Integral REduction with Modular Arithmetic  FIRE is a program performing reduction of Feynman integrals to master integrals. The C++ version of FIRE was presented in 2014. There have been multiple changes and upgrades since then including the possibility to use multiple computers for one reduction task and to perform reduction with modular arithmetic. The goal of this paper is to present the current version of FIRE. ,2019,fires,5.677642,2.615154,2016-2020,5,#7402B1
333,The Spitzer Archival Far-InfraRed Extragalactic Survey,"The Spitzer Archival Far-InfraRed Extragalactic Survey  We present the Spitzer Archival Far-InfraRed Extragalactic Survey (SAFIRES). This program produces refined mosaics and source lists for all far-infrared extragalactic data taken during the more than six years of the cryogenic operation of the Spitzer Space Telescope. The SAFIRES products consist of far-infrared data in two wavelength bands (70 um and 160 um) across approximately 180 square degrees of sky, with source lists containing far-infrared fluxes for almost 40,000 extragalactic point sources. Thus, SAFIRES provides a large, robust archival far-infrared data set suitable for many scientific goals. ",2015,fires,9.791563,8.719283,2016-2020,5,#7402B1
334,Wildfire Prediction to Inform Fire Management: Statistical Science  Challenges,"Wildfire Prediction to Inform Fire Management: Statistical Science   Challenges  Wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. A variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. Statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. Predictive models have exploited several sources of data describing fire phenomena. Experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. Fires are rare events at many scales. The data describing fire phenomena can be zero-heavy and nonstationary over both space and time. Users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. We focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales. ",2013,fires,4.313369,7.0253096,2011-2015,4,#9100CF
335,Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns  Captured by Unmanned Aerial Systems,"Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns   Captured by Unmanned Aerial Systems  This research paper addresses the challenge of detecting obscured wildfires (when the fire flames are covered by trees, smoke, clouds, and other natural barriers) in real-time using drones equipped only with RGB cameras. We propose a novel methodology that employs semantic segmentation based on the temporal analysis of smoke patterns in video sequences. Our approach utilizes an encoder-decoder architecture based on deep convolutional neural network architecture with a pre-trained CNN encoder and 3D convolutions for decoding while using sequential stacking of features to exploit temporal variations. The predicted fire locations can assist drones in effectively combating forest fires and pinpoint fire retardant chemical drop on exact flame locations. We applied our method to a curated dataset derived from the FLAME2 dataset that includes RGB video along with IR video to determine the ground truth. Our proposed method has a unique property of detecting obscured fire and achieves a Dice score of 85.88%, while achieving a high precision of 92.47% and classification accuracy of 90.67% on test data showing promising results when inspected visually. Indeed, our method outperforms other methods by a significant margin in terms of video-level fire classification as we obtained about 100% accuracy using MobileNet+CBAM as the encoder backbone. ",2023,fires,4.155745,7.8144374,2020-,6,#5E0897
336,Phases and Time-Scales of Ignition and Burning of Live Fuels,"Phases and Time-Scales of Ignition and Burning of Live Fuels  Wildland fires impact ecosystems and communities worldwide. Many wildfires burn in living or a mixture of living and senescent vegetation. Therefore, it is necessary to understand the burning behavior of living fuels, in contrast to just dead or dried fuels, to more effectively support fire management decisions. In this study, the ignition and burning behaviors of needles placed in convective heat flux were evaluated. The species included longleaf pine (Pinus palustris), Douglas-fir (Pseudotsuga menziesii), western red cedar (Thuja plicata), ponderosa pine (Pinus ponderosa), western larch (Larix occidentalis), pacific yew (Taxus brevifolia), white spruce (Picea glauca), and sagebrush (Artemisia tridentate). The ignition and burning behaviors were related to live fuel moisture content (LFMC), pilot flame temperatures, and convective heat fluxes. The different phases of ignition and burning were captured using high-speed imaging. In general, four burning phases can be observed: droplet ejection and burning, a transition phase, flaming combustion, and smoldering combustion. Ejection and subsequent burning of droplets can occur prior to sustained flaming ignition only in live fuels. For some species (e.g., longleaf pine, ponderosa pine, white spruce) droplet ejection and burning can reduce ignition times relative to dried fuel with lower LFMC. In general, the transition phase tends to take longer than the flaming and droplet phases (when these occur). During the transition phase, the fuels are heated and pyrolysis occurs. Time-scales to ignition and the different phases of ignition and burning vary more among live fuels than dead and dried fuels. This conclusion indicates that other parameters, such as chemical composition and structural morphology of the fuel, can significantly influence the burning of live fuels. ",2022,fires,6.6198373,6.1808443,2020-,6,#5E0897
337,Improved Active Fire Detection using Operational U-Nets,"Improved Active Fire Detection using Operational U-Nets  As a consequence of global warming and climate change, the risk and extent of wildfires have been increasing in many areas worldwide. Warmer temperatures and drier conditions can cause quickly spreading fires and make them harder to control; therefore, early detection and accurate locating of active fires are crucial in environmental monitoring. Using satellite imagery to monitor and detect active fires has been critical for managing forests and public land. Many traditional statistical-based methods and more recent deep-learning techniques have been proposed for active fire detection. In this study, we propose a novel approach called Operational U-Nets for the improved early detection of active fires. The proposed approach utilizes Self-Organized Operational Neural Network (Self-ONN) layers in a compact U-Net architecture. The preliminary experimental results demonstrate that Operational U-Nets not only achieve superior detection performance but can also significantly reduce computational complexity. ",2023,fires,3.58547,8.306624,2020-,6,#5E0897
338,Fires on large recursive trees,"Fires on large recursive trees  We consider random dynamics on a uniform random recursive tree with $n$ vertices. Successively, in a uniform random order, each edge is either set on fire with some probability $p_n$ or fireproof with probability $1-p_n$. Fires propagate in the tree and are only stopped by fireproof edges. We first consider the proportion of burnt and fireproof vertices as $n\to\infty$, and prove a phase transition when $p_n$ is of order $\ln n/n$. We then study the connectivity of the fireproof forest, more precisely the existence of a giant component. We finally investigate the sizes of the burnt subtrees. ",2014,fires,3.1198444,3.6623635,2011-2015,4,#9100CF
339,Upper Atmosphere Smoke Injection from Large Areal Fires,"Upper Atmosphere Smoke Injection from Large Areal Fires  Large fires can inject smoke into the upper troposphere and lower stratosphere. Detailed fire simulations allow for assessment of how local weather interacts with these fires and affects smoke lofting. In this study, we employ the fire simulation package in the Weather Research and Forecasting model (WRF-Fire), Version 4.0.1, to explore how smoke lofting from a fire burning a homogeneous fuel bed changes with varying local winds, relative humidity, and atmospheric boundary-layer stability for two different-sized areal fires. We find that the presence of moisture has the greatest influence on the results by raising the altitude of lofting, while faster winds speeds dampen lofting and lower the injection height. Stably stratified conditions further inhibit plume propagation compared with neutrally stratified conditions. ",2020,fires,6.619396,6.2981224,2020-,6,#5E0897
340,Detecting and Explaining Crisis,"Detecting and Explaining Crisis  Individuals on social media may reveal themselves to be in various states of crisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis from social media text automatically and accurately can have profound consequences. However, detecting a general state of crisis without explaining why has limited applications. An explanation in this context is a coherent, concise subset of the text that rationalizes the crisis detection. We explore several methods to detect and explain crisis using a combination of neural and non-neural techniques. We evaluate these techniques on a unique data set obtained from Koko, an anonymous emotional support network available through various messaging applications. We annotate a small subset of the samples labeled with crisis with corresponding explanations. Our best technique significantly outperforms the baseline for detection and explanation. ",2017,crisis,1.6052089,7.6993957,2016-2020,5,#7402B1
341,"Bank Panics and Fire Sales, Insolvency and Illiquidity","Bank Panics and Fire Sales, Insolvency and Illiquidity  Banking system crises are complex events that in a short span of time can inflict extensive damage to banks themselves and to the external economy. The crisis literature has so far identified a number of distinct effects or channels that can propagate distress contagiously both directly within the banking network itself and indirectly, between the network and the external economy. These contagious effects, and the potential events that trigger these effects, can explain most aspects of past crises, and are thought to be likely to dominate future financial crises. Since the current international financial regulatory regime based on the Basel III Accord does a good job of ensuring that banks are resilient to such contagion effects taken one at a time, systemic risk theorists increasingly understand that future crises are likely to be dominated by the spillovers between distinct contagion channels. The present paper aims to provide a model for systemic risk that is comprehensive enough to include the important contagion channels identified in the literature. In such a model one can hope to understand the dangerous spillover effects that are expected to dominate future crises. To rein in the number and complexity of the modelling assumptions, two requirements are imposed, neither of which is yet well-known or established in the main stream of systemic risk research. The first, called stock-flow consistency, demands that the financial system follows a rigorous set of rules based on accounting principles. The second requirement, called Asset-Liability symmetry, implies that every proposed contagion channel has a dual channel obtained by interchanging assets and liabilities, and that these dual channel pairs have a symmetric mathematical representation. ",2017,crisis,3.2657177,5.96159,2016-2020,5,#7402B1
342,CrisisBERT: a Robust Transformer for Crisis Classification and  Contextual Crisis Embedding,"CrisisBERT: a Robust Transformer for Crisis Classification and   Contextual Crisis Embedding  Classification of crisis events, such as natural disasters, terrorist attacks and pandemics, is a crucial task to create early signals and inform relevant parties for spontaneous actions to reduce overall damage. Despite crisis such as natural disasters can be predicted by professional institutions, certain events are first signaled by civilians, such as the recent COVID-19 pandemics. Social media platforms such as Twitter often exposes firsthand signals on such crises through high volume information exchange over half a billion tweets posted daily. Prior works proposed various crisis embeddings and classification using conventional Machine Learning and Neural Network models. However, none of the works perform crisis embedding and classification using state of the art attention-based deep neural networks models, such as Transformers and document-level contextual embeddings. This work proposes CrisisBERT, an end-to-end transformer-based model for two crisis classification tasks, namely crisis detection and crisis recognition, which shows promising results across accuracy and f1 scores. The proposed model also demonstrates superior robustness over benchmark, as it shows marginal performance compromise while extending from 6 to 36 events with only 51.4% additional data points. We also proposed Crisis2Vec, an attention-based, document-level contextual embedding architecture for crisis embedding, which achieve better performance than conventional crisis embedding methods such as Word2Vec and GloVe. To the best of our knowledge, our works are first to propose using transformer-based crisis classification and document-level contextual crisis embedding in the literature. ",2020,crisis,1.5047755,7.6426253,2020-,6,#5E0897
343,Financial Crisis in the Framework of Non-zero Temperature Balance Theory,"Financial Crisis in the Framework of Non-zero Temperature Balance Theory  Financial crises are known as crashes that result in a sudden loss of value of financial assets in large part and they continue to occur from time to time surprisingly. In order to discover features of the financial network, the pairwise interaction of stocks has been considered in many research, but the existence of the strong correlation of stocks and their collective behavior in crisis made us address higher-order interactions. Hence, in this study, we investigate financial networks by triplet interaction in the framework of balance theory. Due to detecting the contribution of higher-order interactions in understanding the complex behavior of stocks we take the advantage of the orders parameters of the higher-order interactions. Looking at real data of financial market obtained from $S\&P500$ through the lens of balance theory for the quest of network structure in different periods of time near and far from crisis reveals the existence of a structural difference of the network that corresponds to different periods of time. Here, we address two well-known crises the Great regression (2008) and the Covid-19 recession (2020). Results show an ordered structure forms on-crisis in the financial network while stocks behave independently far from a crisis. The formation of the ordered structure of stocks in crisis makes the network resistant against disorder. The resistance of the ordered structure against applying a disorder (temperature) can measure the crisis strength and determine the temperature at which the network transits. There is a critical temperature, $T_{c}$, in the language of statistical mechanics and mean-field approach which above, the ordered structure destroys abruptly and a first-order phase transition occurs. The stronger the crisis, the higher the critical temperature. ",2022,crisis,3.3836641,6.022148,2020-,6,#5E0897
344,Crises Do Not Cause Lower Short-Term Growth,"Crises Do Not Cause Lower Short-Term Growth  It is commonly believed that financial crises ""lead to"" lower growth of a country during the two-year recession period, which can be reflected by their post-crisis GDP growth. However, by contrasting a causal model with a standard prediction model, this paper argues that such a belief is non-causal. To make causal inferences, we design a two-stage staggered difference-in-differences model to estimate the average treatment effects. Interpreting the residuals as the contribution of each crisis to the treatment effects, we astonishingly conclude that cross-sectional crises are often limited to providing relevant causal information to policymakers. ",2022,crisis,3.9514418,6.6873364,2020-,6,#5E0897
345,CrisisBench: Benchmarking Crisis-related Social Media Datasets for  Humanitarian Information Processing,"CrisisBench: Benchmarking Crisis-related Social Media Datasets for   Humanitarian Information Processing  Time-critical analysis of social media streams is important for humanitarian organizations for planing rapid response during disasters. The \textit{crisis informatics} research community has developed several techniques and systems for processing and classifying big crisis-related data posted on social media. However, due to the dispersed nature of the datasets used in the literature (e.g., for training models), it is not possible to compare the results and measure the progress made towards building better models for crisis informatics tasks. In this work, we attempt to bridge this gap by combining various existing crisis-related datasets. We consolidate eight human-annotated datasets and provide 166.1k and 141.5k tweets for \textit{informativeness} and \textit{humanitarian} classification tasks, respectively. We believe that the consolidated dataset will help train more sophisticated models. Moreover, we provide benchmarks for both binary and multiclass classification tasks using several deep learning architecrures including, CNN, fastText, and transformers. We make the dataset and scripts available at: https://crisisnlp.qcri.org/crisis_datasets_benchmarks.html ",2020,crisis,1.931009,8.234576,2020-,6,#5E0897
346,Remarks on criticality and crisis in pure exchange economies,"Remarks on criticality and crisis in pure exchange economies  In the framework of the theory of Equilibrium Manifold of a Pure Exchange Economy, introduced by Balasko, we give a rigorous definition of a crisis transition and establish some infinitesimal criteria which distinguish unavoidable crises from other critical equilibria. Our approach builds on the relationship between the mathematical notions of branching, envelopes, and the intrinsic derivative. ",2021,crisis,3.3016806,5.7129107,2020-,6,#5E0897
347,Crisis Analytics: Big Data Driven Crisis Response,"Crisis Analytics: Big Data Driven Crisis Response  Disasters have long been a scourge for humanity. With the advances in technology (in terms of computing, communications, and the ability to process and analyze big data), our ability to respond to disasters is at an inflection point. There is great optimism that big data tools can be leveraged to process the large amounts of crisis-related data (in the form of user generated data in addition to the traditional humanitarian data) to provide an insight into the fast-changing situation and help drive an effective disaster response. This article introduces the history and the future of big crisis data analytics, along with a discussion on its promise, challenges, and pitfalls. ",2016,crisis,2.4952407,7.4440966,2016-2020,5,#7402B1
348,Predicting economic market crises using measures of collective panic,"Predicting economic market crises using measures of collective panic  Predicting panic is of critical importance in many areas of human and animal behavior, notably in the context of economics. The recent financial crisis is a case in point. Panic may be due to a specific external threat, or self-generated nervousness. Here we show that the recent economic crisis and earlier large single-day panics were preceded by extended periods of high levels of market mimicry --- direct evidence of uncertainty and nervousness, and of the comparatively weak influence of external news. High levels of mimicry can be a quite general indicator of the potential for self-organized crises. ",2011,crisis,3.3508625,6.029434,2011-2015,4,#9100CF
349,Worldwide spreading of economic crisis,"Worldwide spreading of economic crisis  We model the spreading of a crisis by constructing a global economic network and applying the Susceptible-Infected-Recovered (SIR) epidemic model with a variable probability of infection. The probability of infection depends on the strength of economic relations between the pair of countries, and the strength of the target country. It is expected that a crisis which originates in a large country, such as the USA, has the potential to spread globally, like the recent crisis. Surprisingly we show that also countries with much lower GDP, such as Belgium, are able to initiate a global crisis. Using the {\it k}-shell decomposition method to quantify the spreading power (of a node), we obtain a measure of ``centrality'' as a spreader of each country in the economic network. We thus rank the different countries according to the shell they belong to, and find the 12 most central countries. These countries are the most likely to spread a crisis globally. Of these 12 only six are large economies, while the other six are medium/small ones, a result that could not have been otherwise anticipated. Furthermore, we use our model to predict the crisis spreading potential of countries belonging to different shells according to the crisis magnitude. ",2010,crisis,4.6467113,7.0010834,2006-2010,3,#FEF65C
350,Aerodynamic Models For Hurricanes I. Model description and horizontal  motion of hurricane,"Aerodynamic Models For Hurricanes I. Model description and horizontal   motion of hurricane  Aerodynamic models are developed to describe coherent structures and transport processes in hurricanes moving over open seas. The models consist of the lower boundary layer and upper adiabatic layer. Except friction at the air/sea interface,proposed modeling avoids the common turbulent approximations while using explicitly or implicitly basic stability constraints. The models analyze dynamics of upper hurricane adiabatic layer, dynamics and transport processes in hurricane boundary layer, and genesis and maturing of hurricane. The proposed modeling provides a rude enough but consistent analytical description of basic processes in hurricanes. The present paper qualitatively describes the model of mature hurricane, briefly discusses the basic thermodynamic relations and aerodynamic equations, and establishes the principles of horizontal motion for mature hurricane. ",2008,hurricane,5.816909,5.2118206,2006-2010,3,#FEF65C
351,Predicting Hurricane Trajectories using a Recurrent Neural Network,"Predicting Hurricane Trajectories using a Recurrent Neural Network  Hurricanes are cyclones circulating about a defined center whose closed wind speeds exceed 75 mph originating over tropical and subtropical waters. At landfall, hurricanes can result in severe disasters. The accuracy of predicting their trajectory paths is critical to reduce economic loss and save human lives. Given the complexity and nonlinearity of weather data, a recurrent neural network (RNN) could be beneficial in modeling hurricane behavior. We propose the application of a fully connected RNN to predict the trajectory of hurricanes. We employed the RNN over a fine grid to reduce typical truncation errors. We utilized their latitude, longitude, wind speed, and pressure publicly provided by the National Hurricane Center (NHC) to predict the trajectory of a hurricane at 6-hour intervals. Results show that this proposed technique is competitive to methods currently employed by the NHC and can predict up to approximately 120 hours of hurricane path. ",2018,hurricane,3.5019484,6.8155384,2016-2020,5,#7402B1
352,Hurricanes and hashtags: Characterizing online collective attention for  natural disasters,"Hurricanes and hashtags: Characterizing online collective attention for   natural disasters  We study collective attention paid towards hurricanes through the lens of $n$-grams on Twitter, a social media platform with global reach. Using hurricane name mentions as a proxy for awareness, we find that the exogenous temporal dynamics are remarkably similar across storms, but that overall collective attention varies widely even among storms causing comparable deaths and damage. We construct `hurricane attention maps' and observe that hurricanes causing deaths on (or economic damage to) the continental United States generate substantially more attention in English language tweets than those that do not. We find that a hurricane's Saffir-Simpson wind scale category assignment is strongly associated with the amount of attention it receives. Higher category storms receive higher proportional increases of attention per proportional increases in number of deaths or dollars of damage, than lower category storms. The most damaging and deadly storms of the 2010s, Hurricanes Harvey and Maria, generated the most attention and were remembered the longest, respectively. On average, a category 5 storm receives 4.6 times more attention than a category 1 storm causing the same number of deaths and economic damage. ",2020,hurricane,1.8027179,8.088126,2020-,6,#5E0897
353,Can a Hurricane Be Managed,"Can a Hurricane Be Managed  Based on realistic estimates of geophysical conditions it is demonstrated that by practical means; (1) the intensity of a hurricane can be diminished before making landfall; (2) and other circumstances, a potential hurricane might be quenched before achieving critical strength. Under the first strategy, cold deep ocean water cools the hurricane track to weaken it. Analyses are facilitated by derivation of a novel exact solution that models a tropical depression, a mesoscale eddy, and the eye of a hurricane. Under the second strategy, it is shown. the threat of a tropical depression can be diminished, in a timely manner. Estimates of the power needed to perform timely ocean mixing show that this might be accomplished by high-performance submarines, and an exceptional coefficient of performance, O(10^4). The destructive power of a hurricane is functionally related to its maximal wind speed, Vm. It is shown that a 20 percent reduction in maximal wind speed produces a 50 percent reduction in destructive costs. Complementary deliberations show the potential for producing rainfall in relief of drought. Shown also is the opportunity to significantly modify vessel architecture to maximize the desired goals. It is the contention of this paper that a practical framework exists to pursue means by which to reduce the tragedy and devastation caused by hurricanes. ",2020,hurricane,8.647595,8.110922,2020-,6,#5E0897
354,The Transient Responses of An Axisymmetric Tropical Cyclone to  Instantaneous Surface Roughening and Drying. Part I: Numerical Experiments,"The Transient Responses of An Axisymmetric Tropical Cyclone to   Instantaneous Surface Roughening and Drying. Part I: Numerical Experiments  Inland tropical cyclone (TC) impacts due to high winds and rainfall-induced flooding depend strongly on the evolution of the wind field and precipitation distribution after landfall. However, research has yet to test the detailed response of a mature TC and its hazards to changes in surface forcing in idealized settings. This work tests the transient response of an idealized hurricane to instantaneous transitions in two key surface properties associated with landfall: surface roughening and drying. Simplified axisymmetric experiments are performed in CM1 where surface drag coefficient and evaporative fraction are each systematically modified beneath a mature hurricane. Surface drying stabilizes the eyewall and consequently weakens the overturning circulation, thereby reducing inward angular momentum transport that slowly decays the wind field only within the inner-core. In contrast, surface roughening initially ($\sim$12 hours) rapidly weakens the entire low-level wind field and enhances the overturning circulation dynamically despite the concurrent thermodynamic stabilization of the eyewall; thereafter the storm gradually decays similar to drying. As a result, total precipitation temporarily increases with roughening but uniformly decreases with drying. Storm size decreases monotonically and rapidly with surface roughening, while the radius of maximum wind can increase with moderate surface drying. Overall, this work provides a mechanistic foundation for understanding the inland evolution of real storms in nature. ",2020,hurricane,8.934197,8.27718,2020-,6,#5E0897
355,An Analytical Theory for the Early Stage of the Development of  Hurricanes: Part I,"An Analytical Theory for the Early Stage of the Development of   Hurricanes: Part I  A theoretical formulation for the early stage (Tropical Storm stage) of hurricane development is proposed. These solutions are not only consistent with observations but also offer some new insights into hurricane properties. This is the first time a theoretical model for the early growing of hurricanes is proposed for which analytical solutions have been found, based on an assumption of a positive feedback of a self-induced developing system. ",2004,hurricane,8.422434,8.660018,2001-2005,2,#FFE135
356,Spatiotemporal Impact Assessment of Hurricanes on Electric Power Systems,"Spatiotemporal Impact Assessment of Hurricanes on Electric Power Systems  Almost 90% of the major power outages in the US are caused due to hurricanes. Due to the highly uncertain nature of hurricanes in both spatial and temporal dimensions, it is essential to quantify the effect of such hurricanes on a power grid. In this paper, we provide a Monte-Carlo-based framework in which several hurricane scenarios and their impact on a power grid are analyzed in spatiotemporal dimensions. The hurricane simulations are performed using samples from previously occurred hurricanes in the US whereas probabilistic assessment of the transmission lines is performed through line fragility model. Finally, a loss metric based on the amount of load disconnected due to hurricanes traveling inland is calculated for each time step. The simulation is performed on ACTIVSg2000: 2000-bus synthetic Texas grid while mapping the transmission lines of the test case on the geographical footprint of Texas. The simulation results show that the loss increases significantly for a few time steps when the wind field of a hurricane is intense and almost saturates when the intensity of the hurricane decays while traversing further. The proposed analysis can provide some insights for proactive planning strategies on improving the resilience of the power grid. ",2021,hurricane,6.6025667,7.63277,2020-,6,#5E0897
357,A Universal Hurricane Frequency Function,"A Universal Hurricane Frequency Function  Evidence is provided that the global distribution of tropical hurricanes is principally determined by a universal function H of a single variable z that in turn is expressible in terms of the local sea surface temperature and latitude. The data-driven model presented here carries stark implications for the large increased numbers of hurricanes which it predicts for a warmer world. Moreover, the rise in recent decades in the numbers of hurricanes in the Atlantic, but not the Pacific basin, is shown to have a simple explanation in terms of the specific form of H(z), which yields larger percentage increases when a fixed increase in sea surface temperature occurs at higher latitudes and lower temperatures. ",2010,hurricane,9.52783,7.9972053,2006-2010,3,#FEF65C
358,On the molecular dynamics in the hurricane interactions with its  environment,"On the molecular dynamics in the hurricane interactions with its   environment  By resorting to the Burgers model for hurricanes, we study the molecular motion involved in the hurricane dynamics. We show that the Lagrangian canonical formalism requires the inclusion of the environment degrees of freedom. This also allows the description of the motion of charged particles. In view of the role played by moist convection, cumulus and cloud water droplets in the hurricane dynamics, we discuss on the basis of symmetry considerations the role played by the molecular electrical dipoles and the formation of topologically non-trivial structures. The mechanism of energy storage and dissipation, the non-stationary time dependent Ginzburg-Landau equation and the vortex equation are studied. Finally, we discuss the fractal self-similarity properties of hurricanes. ",2018,hurricane,7.243613,4.176007,2016-2020,5,#7402B1
359,Changes in number and intensity of world-wide tropical cyclones,"Changes in number and intensity of world-wide tropical cyclones  Bayesian statistical models were developed for the number of tropical cyclones and the rate at which these cyclones became hurricanes in the North Atlantic, North and South Indian, and East and West Pacific Oceans. We find that there is small probability that the number of cyclones has increased in the past thirty years. The rate at which these storms become hurricanes appears to be constant. The rate at which hurricanes evolve into category 4 and higher major storms does appear to have increased. We also investigate storm intensity by measuring the distribution of individual storm lifetime in days, storm track length, and Emanuel's power dissiptation index. We find little evidence that, overall, the mean of the distribution of individual storm intensity is changing through time, but the variability of the distribution has increased. The cold tongue index and the North Atlantic oscillation index were found to be strongly associated with storm quality in the Western, and to a smaller extent, the Eastern Pacific oceans. The North Atlantic oscillation index was strongly associated with the increase in the rate of strong storms evolving. ",2007,hurricane,5.1791635,6.7266607,2006-2010,3,#FEF65C
360,Acceleration of Tropical Cyclones As a Proxy For Extratropical  Interactions: Synoptic-Scale Patterns and Long-Term Trends,"Acceleration of Tropical Cyclones As a Proxy For Extratropical   Interactions: Synoptic-Scale Patterns and Long-Term Trends  It is well known that rapid changes in tropical cyclone motion occur during interaction with extratropical waves. While the translation speed has received much attention in the published literature, acceleration has not. Using a large data sample of Atlantic tropical cyclones, we formally examine the composite synoptic-scale patterns associated with tangential and \curvature components of their acceleration. During periods of rapid tangential acceleration, the composite tropical cyclone moves poleward between an upstream trough and downstream ridge of a developing extratropical wavepacket. The two systems subsequently merge in a manner that is consistent with extratropical transition. During rapid curvature acceleration, a prominent downstream ridge promotes recurvature of the tropical cyclone. In contrast, during rapid tangential or curvature deceleration, a ridge is located directly poleward of the tropical cyclone. Locally, this arrangement takes the form of a cyclone-anticyclone vortex pair somewhat akin to a dipole block. On average, the tangential acceleration peaks 18 hours prior to extratropical transition while the curvature acceleration peaks at recurvature. These findings confirm that rapid acceleration of tropical cyclones is mediated by interaction with extratropical baroclinic waves. Furthermore, The tails of the distribution of acceleration and translation speed show a robust reduction over the past 5 decades. We speculate that these trends may reflect the poleward shift and weakening of extratropical Rossby waves. ",2021,cyclone,8.729773,8.316493,2020-,6,#5E0897
361,Tropical cyclone intensity estimations over the Indian ocean using  Machine Learning,"Tropical cyclone intensity estimations over the Indian ocean using   Machine Learning  Tropical cyclones are one of the most powerful and destructive natural phenomena on earth. Tropical storms and heavy rains can cause floods, which lead to human lives and economic loss. Devastating winds accompanying cyclones heavily affect not only the coastal regions, even distant areas. Our study focuses on the intensity estimation, particularly cyclone grade and maximum sustained surface wind speed (MSWS) of a tropical cyclone over the North Indian Ocean. We use various machine learning algorithms to estimate cyclone grade and MSWS. We have used the basin of origin, date, time, latitude, longitude, estimated central pressure, and pressure drop as attributes of our models. We use multi-class classification models for the categorical outcome variable, cyclone grade, and regression models for MSWS as it is a continuous variable. Using the best track data of 28 years over the North Indian Ocean, we estimate grade with an accuracy of 88% and MSWS with a root mean square error (RMSE) of 2.3. For higher grade categories (5-7), accuracy improves to an average of 98.84%. We tested our model with two recent tropical cyclones in the North Indian Ocean, Vayu and Fani. For grade, we obtained an accuracy of 93.22% and 95.23% respectively, while for MSWS, we obtained RMSE of 2.2 and 3.4 and $R^2$ of 0.99 and 0.99, respectively. ",2021,cyclone,4.4187307,6.8357844,2020-,6,#5E0897
362,Tropical Cyclone Track Forecasting using Fused Deep Learning from  Aligned Reanalysis Data,"Tropical Cyclone Track Forecasting using Fused Deep Learning from   Aligned Reanalysis Data  The forecast of tropical cyclone trajectories is crucial for the protection of people and property. Although forecast dynamical models can provide high-precision short-term forecasts, they are computationally demanding, and current statistical forecasting models have much room for improvement given that the database of past hurricanes is constantly growing. Machine learning methods, that can capture non-linearities and complex relations, have only been scarcely tested for this application. We propose a neural network model fusing past trajectory data and reanalysis atmospheric images (wind and pressure 3D fields). We use a moving frame of reference that follows the storm center for the 24h tracking forecast. The network is trained to estimate the longitude and latitude displacement of tropical cyclones and depressions from a large database from both hemispheres (more than 3000 storms since 1979, sampled at a 6 hour frequency). The advantage of the fused network is demonstrated and a comparison with current forecast models shows that deep learning methods could provide a valuable and complementary prediction. Moreover, our method can give a forecast for a new storm in a few seconds, which is an important asset for real-time forecasts compared to traditional forecasts. ",2019,cyclone,3.8437421,6.937892,2016-2020,5,#7402B1
363,"Prediction of Landfall Intensity, Location, and Time of a Tropical  Cyclone","Prediction of Landfall Intensity, Location, and Time of a Tropical   Cyclone  The prediction of the intensity, location and time of the landfall of a tropical cyclone well advance in time and with high accuracy can reduce human and material loss immensely. In this article, we develop a Long Short-Term memory based Recurrent Neural network model to predict intensity (in terms of maximum sustained surface wind speed), location (latitude and longitude), and time (in hours after the observation period) of the landfall of a tropical cyclone which originates in the North Indian ocean. The model takes as input the best track data of cyclone consisting of its location, pressure, sea surface temperature, and intensity for certain hours (from 12 to 36 hours) anytime during the course of the cyclone as a time series and then provide predictions with high accuracy. For example, using 24 hours data of a cyclone anytime during its course, the model provides state-of-the-art results by predicting landfall intensity, time, latitude, and longitude with a mean absolute error of 4.24 knots, 4.5 hours, 0.24 degree, and 0.37 degree respectively, which resulted in a distance error of 51.7 kilometers from the landfall location. We further check the efficacy of the model on three recent devastating cyclones Bulbul, Fani, and Gaja, and achieved better results than the test dataset. ",2021,cyclone,3.9207907,7.0357275,2020-,6,#5E0897
364,The Transient Responses of An Axisymmetric Tropical Cyclone to  Instantaneous Surface Roughening and Drying. Part I: Numerical Experiments,"The Transient Responses of An Axisymmetric Tropical Cyclone to   Instantaneous Surface Roughening and Drying. Part I: Numerical Experiments  Inland tropical cyclone (TC) impacts due to high winds and rainfall-induced flooding depend strongly on the evolution of the wind field and precipitation distribution after landfall. However, research has yet to test the detailed response of a mature TC and its hazards to changes in surface forcing in idealized settings. This work tests the transient response of an idealized hurricane to instantaneous transitions in two key surface properties associated with landfall: surface roughening and drying. Simplified axisymmetric experiments are performed in CM1 where surface drag coefficient and evaporative fraction are each systematically modified beneath a mature hurricane. Surface drying stabilizes the eyewall and consequently weakens the overturning circulation, thereby reducing inward angular momentum transport that slowly decays the wind field only within the inner-core. In contrast, surface roughening initially ($\sim$12 hours) rapidly weakens the entire low-level wind field and enhances the overturning circulation dynamically despite the concurrent thermodynamic stabilization of the eyewall; thereafter the storm gradually decays similar to drying. As a result, total precipitation temporarily increases with roughening but uniformly decreases with drying. Storm size decreases monotonically and rapidly with surface roughening, while the radius of maximum wind can increase with moderate surface drying. Overall, this work provides a mechanistic foundation for understanding the inland evolution of real storms in nature. ",2020,cyclone,8.934197,8.27718,2020-,6,#5E0897
365,Numerical Simulation of a Quasi-Tropical Cyclone over the Black Sea,"Numerical Simulation of a Quasi-Tropical Cyclone over the Black Sea  The paper describes results of numerical experiments on the simulation of a mesoscale quasi-tropical cyclone, a rare event for the Black Sea, with the MM5 regional atmospheric circulation model. General characteristics of the cyclone and its evolution and physical formation mechanisms are discussed. The balances of the momentum components have been estimated, and sensitivity experiments have been performed. It is shown that, according to its main physical properties and energy supply mechanisms, the cyclone can be related to quasi-tropical cyclones. ",2009,cyclone,5.873124,5.070653,2006-2010,3,#FEF65C
366,Cyclone Codes,"Cyclone Codes  We introduce Cyclone codes which are rateless erasure resilient codes. They combine Pair codes with Luby Transform (LT) codes by computing a code symbol from a random set of data symbols using bitwise XOR and cyclic shift operations. The number of data symbols is chosen according to the Robust Soliton distribution. XOR and cyclic shift operations establish a unitary commutative ring if data symbols have a length of $p-1$ bits, for some prime number $p$. We consider the graph given by code symbols combining two data symbols. If $n/2$ such random pairs are given for $n$ data symbols, then a giant component appears, which can be resolved in linear time. We can extend Cyclone codes to data symbols of arbitrary even length, provided the Goldbach conjecture holds.   Applying results for this giant component, it follows that Cyclone codes have the same encoding and decoding time complexity as LT codes, while the overhead is upper-bounded by those of LT codes. Simulations indicate that Cyclone codes significantly decreases the overhead of extra coding symbols. ",2016,cyclone,1.6450534,4.796993,2016-2020,5,#7402B1
367,Cyclone-anticyclone asymmetry in rotating thin fluid layers,"Cyclone-anticyclone asymmetry in rotating thin fluid layers  We report of a series of laboratory experiments and numerical simulations of freely-decaying rotating turbulent flows confined in domains with variable height. We show that the vertical confinement has important effects on the formation of large-scale columnar vortices, the hallmark of rotating turbulence, and in particular delays the development of the cyclone-anticyclone asymmetry. We compare the experimental and numerical results face-to-face, showing the robustness of the results. ",2020,cyclone,6.0805864,5.0245576,2020-,6,#5E0897
368,Forecasting formation of a Tropical Cyclone Using Reanalysis Data,"Forecasting formation of a Tropical Cyclone Using Reanalysis Data  The tropical cyclone formation process is one of the most complex natural phenomena which is governed by various atmospheric, oceanographic, and geographic factors that varies with time and space. Despite several years of research, accurately predicting tropical cyclone formation remains a challenging task. While the existing numerical models have inherent limitations, the machine learning models fail to capture the spatial and temporal dimensions of the causal factors behind TC formation. In this study, a deep learning model has been proposed that can forecast the formation of a tropical cyclone with a lead time of up to 60 hours with high accuracy. The model uses the high-resolution reanalysis data ERA5 (ECMWF reanalysis 5th generation), and best track data IBTrACS (International Best Track Archive for Climate Stewardship) to forecast tropical cyclone formation in six ocean basins of the world. For 60 hours lead time the models achieve an accuracy in the range of 86.9% - 92.9% across the six ocean basins. The model takes about 5-15 minutes of training time depending on the ocean basin, and the amount of data used and can predict within seconds, thereby making it suitable for real-life usage. ",2022,cyclone,3.893421,6.7236814,2020-,6,#5E0897
369,Coupling Onset of Cyclone Upward and Rotation Flows in a Little Bottle,"Coupling Onset of Cyclone Upward and Rotation Flows in a Little Bottle  A coupling onset of the cyclone upward and rotation flows is experimentally demonstrated in a little bottle. The rotating flow provides a pressure increase in the outer part of the rotating flow by its centrifugal force. When a gradient of the fluid rotation appears along the rotation axis, the higher-pressure area is localized and pushes the fluid in a low pressure. Then the fluid staying in the central area of the rotation is pushed up along the rotation axis, and the upward wind is enhanced. In this coupling mechanism the rotation gradient is the key; the coupling of the rotation and the upward fluid flow is essentially important for a cyclone buildup, and is well explained experimentally and theoretically. ",2012,cyclone,5.9941416,5.3024187,2011-2015,4,#9100CF
370,Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian  Optimization with Spatiotemporal feature fusion,"Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian   Optimization with Spatiotemporal feature fusion  Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were 0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall input conditions. Additionally, the processing speed was significantly improved, with an inference time of 1.158s (approximately 1/125 of the traditional computation time) compared to the physically-based models. ",2023,best machine learning model for flood forecasting,4.2357583,6.795878,2020-,6,#5E0897
371,MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak  Inundation Depth And Decoding Influencing Features,"MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak   Inundation Depth And Decoding Influencing Features  Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively. ",2023,best machine learning model for flood forecasting,4.25608,6.8919473,2020-,6,#5E0897
372,Flood forecasting with machine learning models in an operational  framework,"Flood forecasting with machine learning models in an operational   framework  The operational flood forecasting system by Google was developed to provide accurate real-time flood warnings to agencies and the public, with a focus on riverine floods in large, gauged rivers. It became operational in 2018 and has since expanded geographically. This forecasting system consists of four subsystems: data validation, stage forecasting, inundation modeling, and alert distribution. Machine learning is used for two of the subsystems. Stage forecasting is modeled with the Long Short-Term Memory (LSTM) networks and the Linear models. Flood inundation is computed with the Thresholding and the Manifold models, where the former computes inundation extent and the latter computes both inundation extent and depth. The Manifold model, presented here for the first time, provides a machine-learning alternative to hydraulic modeling of flood inundation. When evaluated on historical data, all models achieve sufficiently high-performance metrics for operational use. The LSTM showed higher skills than the Linear model, while the Thresholding and Manifold models achieved similar performance metrics for modeling inundation extent. During the 2021 monsoon season, the flood warning system was operational in India and Bangladesh, covering flood-prone regions around rivers with a total area of 287,000 km2, home to more than 350M people. More than 100M flood alerts were sent to affected populations, to relevant authorities, and to emergency organizations. Current and future work on the system includes extending coverage to additional flood-prone locations, as well as improving modeling capabilities and accuracy. ",2021,best machine learning model for flood forecasting,4.0636454,7.06618,2020-,6,#5E0897
373,Flood Prediction Using Machine Learning Models,"Flood Prediction Using Machine Learning Models  Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy. ",2022,best machine learning model for flood forecasting,4.1629286,6.9495845,2020-,6,#5E0897
374,Flood Prediction Using Machine Learning Models: Literature Review,"Flood Prediction Using Machine Learning Models: Literature Review  Floods are among the most destructive natural disasters, which are highly complex to model. The research on the advancement of flood prediction models contributed to risk reduction, policy suggestion, minimization of the loss of human life, and reduction the property damage associated with floods. To mimic the complex mathematical expressions of physical processes of floods, during the past two decades, machine learning (ML) methods contributed highly in the advancement of prediction systems providing better performance and cost-effective solutions. Due to the vast benefits and potential of ML, its popularity dramatically increased among hydrologists. Researchers through introducing novel ML methods and hybridizing of the existing ones aim at discovering more accurate and efficient prediction models. The main contribution of this paper is to demonstrate the state of the art of ML models in flood prediction and to give insight into the most suitable models. In this paper, the literature where ML models were benchmarked through a qualitative analysis of robustness, accuracy, effectiveness, and speed are particularly investigated to provide an extensive overview on the various ML algorithms used in the field. The performance comparison of ML models presents an in-depth understanding of the different techniques within the framework of a comprehensive evaluation and discussion. As a result, this paper introduces the most promising prediction methods for both long-term and short-term floods. Furthermore, the major trends in improving the quality of the flood prediction models are investigated. Among them, hybridization, data decomposition, algorithm ensemble, and model optimization are reported as the most effective strategies for the improvement of ML methods. ",2019,best machine learning model for flood forecasting,3.8797252,7.0247035,2016-2020,5,#7402B1
375,Global Flood Prediction: a Multimodal Machine Learning Approach,"Global Flood Prediction: a Multimodal Machine Learning Approach  Flooding is one of the most destructive and costly natural disasters, and climate changes would further increase risks globally. This work presents a novel multimodal machine learning approach for multi-year global flood risk prediction, combining geographical information and historical natural disaster dataset. Our multimodal framework employs state-of-the-art processing techniques to extract embeddings from each data modality, including text-based geographical data and tabular-based time-series data. Experiments demonstrate that a multimodal approach, that is combining text and statistical data, outperforms a single-modality approach. Our most advanced architecture, employing embeddings extracted using transfer learning upon DistilBert model, achieves 75\%-77\% ROCAUC score in predicting the next 1-5 year flooding event in historically flooded locations. This work demonstrates the potentials of using machine learning for long-term planning in natural disaster management. ",2023,best machine learning model for flood forecasting,4.0601277,6.979293,2020-,6,#5E0897
376,Rapid Flood Inundation Forecast Using Fourier Neural Operator,"Rapid Flood Inundation Forecast Using Fourier Neural Operator  Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting strong generalization skill. ",2023,best machine learning model for flood forecasting,4.2761974,6.9030023,2020-,6,#5E0897
377,Machine Learning for Generalizable Prediction of Flood Susceptibility,"Machine Learning for Generalizable Prediction of Flood Susceptibility  Flooding is a destructive and dangerous hazard and climate change appears to be increasing the frequency of catastrophic flooding events around the world. Physics-based flood models are costly to calibrate and are rarely generalizable across different river basins, as model outputs are sensitive to site-specific parameters and human-regulated infrastructure. In contrast, statistical models implicitly account for such factors through the data on which they are trained. Such models trained primarily from remotely-sensed Earth observation data could reduce the need for extensive in-situ measurements. In this work, we develop generalizable, multi-basin models of river flooding susceptibility using geographically-distributed data from the USGS stream gauge network. Machine learning models are trained in a supervised framework to predict two measures of flood susceptibility from a mix of river basin attributes, impervious surface cover information derived from satellite imagery, and historical records of rainfall and stream height. We report prediction performance of multiple models using precision-recall curves, and compare with performance of naive baselines. This work on multi-basin flood prediction represents a step in the direction of making flood prediction accessible to all at-risk communities. ",2019,best machine learning model for flood forecasting,4.067657,6.8755326,2016-2020,5,#7402B1
378,ML for Flood Forecasting at Scale,"ML for Flood Forecasting at Scale  Effective riverine flood forecasting at scale is hindered by a multitude of factors, most notably the need to rely on human calibration in current methodology, the limited amount of data for a specific location, and the computational difficulty of building continent/global level models that are sufficiently accurate. Machine learning (ML) is primed to be useful in this scenario: learned models often surpass human experts in complex high-dimensional scenarios, and the framework of transfer or multitask learning is an appealing solution for leveraging local signals to achieve improved global performance. We propose to build on these strengths and develop ML systems for timely and accurate riverine flood prediction. ",2019,best machine learning model for flood forecasting,3.8501947,7.049657,2016-2020,5,#7402B1
379,"KNN, An Underestimated Model for Regional Rainfall Forecasting","KNN, An Underestimated Model for Regional Rainfall Forecasting  Regional rainfall forecasting is an important issue in hydrology and meteorology. This paper aims to design an integrated tool by applying various machine learning algorithms, especially the state-of-the-art deep learning algorithms including Deep Neural Network, Wide Neural Network, Deep and Wide Neural Network, Reservoir Computing, Long Short Term Memory, Support Vector Machine, K-Nearest Neighbor for forecasting regional precipitations over different catchments in Upstate New York. Through the experimental results and the comparison among machine learning models including classification and regression, we find that KNN is an outstanding model over other models to handle the uncertainty in the precipitation data. The data normalization methods such as ZScore and MinMax are also evaluated and discussed. ",2021,best machine learning model for flood forecasting,4.290997,6.8792467,2020-,6,#5E0897
380,Burning Two Worlds: Algorithms for Burning Dense and Tree-like Graphs,"Burning Two Worlds: Algorithms for Burning Dense and Tree-like Graphs  Graph burning is a simple model for the spread of social influence in networks. The objective is to measure how quickly a fire (e.g., a piece of fake news) can be spread in a network. The burning process takes place in discrete rounds. In each round, a new fire breaks out at a selected vertex and burns it. Meanwhile, the old fires extend to their neighbours and burn them. A burning schedule selects where the new fire breaks out in each round, and the burning problem asks for a schedule that burns all vertices in a minimum number of rounds, termed the burning number of the graph. The burning problem is known to be NP-hard even when the graph is a tree or a disjoint set of paths. For connected graphs, it has been conjectured that burning takes at most $\lceil \sqrt{n} \rceil$ rounds.   We approach the algorithmic study of graph burning from two directions. First, we consider graphs with minimum degree $\delta$. We present an algorithm that burns any graph of size $n$ in at most $\sqrt{\frac{24n}{\delta+1}}$ rounds. In particular, for dense graphs with $\delta \in \Theta(n)$, all vertices are burned in a constant number of rounds. More interestingly, even when $\delta$ is a constant that is independent of the graph size, our algorithm answers the graph-burning conjecture in the affirmative by burning the graph in at most $\lceil \sqrt{n} \rceil$ rounds. Next, we consider burning graphs with bounded path-length or tree-length. These include many graph families including connected interval graphs and connected chordal graphs. We show that any graph with path-length $pl$ and diameter $d$ can be burned in $\lceil \sqrt{d-1} \rceil + pl$ rounds. Our algorithm ensures an approximation ratio of $1+o(1)$ for graphs of bounded path-length. We introduce another algorithm that achieves an approximation ratio of $2+o(1)$ for burning graphs of bounded tree-length. ",2019,forrest fires,2.7252438,3.5165823,2016-2020,5,#7402B1
381,Generalizations of forest fires with ignition at origin,"Generalizations of forest fires with ignition at origin  We study generalizations of the Forest Fire model introduced in [van den Berg, J., and J\'arai, A. A. ""On the asymptotic density in a one-dimensional self-organized critical forest-fire model"". Comm. Math. Phys. 253 (2005)] and [Volkov, Stanislav. ""Forest fires on $\mathbb{Z}_+$ with ignition only at 0"". ALEA 6 (2009)] by allowing the rates at which the tree grow to depend on their location, introducing long-range burning, as well as continuous-space generalization of the model. We establish that in all the models in consideration the time required to reach site at distance $x$ from the origin is of order at most $(\log x)^{(\log 2)^{-1}+\delta}$ for any $\delta>0$. ",2019,forrest fires,5.961733,5.896315,2016-2020,5,#7402B1
382,The Bak-Chen-Tang Forest Fire Model Revisited,"The Bak-Chen-Tang Forest Fire Model Revisited  We reconsider a model introduced by Bak, Chen, and Tang (Phys. Rev. A 38, 364 (1988)) as a supposedly self-organized critical model for forest fires. We verify again that the model is not critical in 2 dimensions, as found also by previous authors. But we find that the model does show anomalous scaling (i.e., is critical in the sense of statistical mechanics) in 3 and 4 dimensions. We relate these results to recent claims by A. Johansen. ",1997,forrest fires,4.6375175,6.3445005,1996-2000,1,#B2AB10
383,FORFIS: A forest fire firefighting simulation tool for education and  research,"FORFIS: A forest fire firefighting simulation tool for education and   research  We present a forest fire firefighting simulation tool named FORFIS that is implemented in Python. Unlike other existing software, we focus on a user-friendly software interface with an easy-to-modify software engine. Our tool is published under GNU GPLv3 license and comes with a GUI as well as additional output functionality. The used wildfire model is based on the well-established approach by cellular automata in two variants - a rectangular and a hexagonal cell decomposition of the wildfire area. The model takes wind into account. In addition, our tool allows the user to easily include a customized firefighting strategy for the firefighting agents. ",2023,forrest fires,3.7800984,7.115713,2020-,6,#5E0897
384,Fuzzy-based forest fire prevention and detection by wireless sensor  networks,"Fuzzy-based forest fire prevention and detection by wireless sensor   networks  Forest fires may cause considerable damages both in ecosystems and lives. This proposal describes the application of Internet of Things and wireless sensor networks jointly with multi-hop routing through a real time and dynamic monitoring system for forest fire prevention. It is based on gathering and analyzing information related to meteorological conditions, concentrations of polluting gases and oxygen level around particular interesting forest areas. Unusual measurements of these environmental variables may help to prevent wildfire incidents and make their detection more efficient. A forest fire risk controller based on fuzzy logic has been implemented in order to activate environmental risk alerts through a Web service and a mobile application. For this purpose, security mechanisms have been proposed for ensuring integrity and confidentiality in the transmission of measured environmental information. Lamport's signature and a block cipher algorithm are used to achieve this objective. ",2022,forrest fires,1.6141669,6.7084227,2020-,6,#5E0897
385,Phases and Time-Scales of Ignition and Burning of Live Fuels,"Phases and Time-Scales of Ignition and Burning of Live Fuels  Wildland fires impact ecosystems and communities worldwide. Many wildfires burn in living or a mixture of living and senescent vegetation. Therefore, it is necessary to understand the burning behavior of living fuels, in contrast to just dead or dried fuels, to more effectively support fire management decisions. In this study, the ignition and burning behaviors of needles placed in convective heat flux were evaluated. The species included longleaf pine (Pinus palustris), Douglas-fir (Pseudotsuga menziesii), western red cedar (Thuja plicata), ponderosa pine (Pinus ponderosa), western larch (Larix occidentalis), pacific yew (Taxus brevifolia), white spruce (Picea glauca), and sagebrush (Artemisia tridentate). The ignition and burning behaviors were related to live fuel moisture content (LFMC), pilot flame temperatures, and convective heat fluxes. The different phases of ignition and burning were captured using high-speed imaging. In general, four burning phases can be observed: droplet ejection and burning, a transition phase, flaming combustion, and smoldering combustion. Ejection and subsequent burning of droplets can occur prior to sustained flaming ignition only in live fuels. For some species (e.g., longleaf pine, ponderosa pine, white spruce) droplet ejection and burning can reduce ignition times relative to dried fuel with lower LFMC. In general, the transition phase tends to take longer than the flaming and droplet phases (when these occur). During the transition phase, the fuels are heated and pyrolysis occurs. Time-scales to ignition and the different phases of ignition and burning vary more among live fuels than dead and dried fuels. This conclusion indicates that other parameters, such as chemical composition and structural morphology of the fuel, can significantly influence the burning of live fuels. ",2022,forrest fires,6.6198373,6.1808443,2020-,6,#5E0897
386,Seven-dimensional forest fires,"Seven-dimensional forest fires  We show that in high dimensional Bernoulli percolation, removing from a thin infinite cluster a much thinner infinite cluster leaves an infinite component. This observation has implications for the van den Berg-Brouwer forest fire process, also known as self-destructive percolation, for dimension high enough. ",2013,forrest fires,5.572144,5.0373154,2011-2015,4,#9100CF
387,Forest fire spreading: a nonlinear stochastic model continuous in space  and time,"Forest fire spreading: a nonlinear stochastic model continuous in space   and time  Forest fire spreading is a complex phenomenon characterized by a stochastic behavior. Nowadays, the enormous quantity of georeferenced data and the availability of powerful techniques for their analysis can provide a very careful picture of forest fires opening the way to more realistic models. We propose a stochastic spreading model continuous in space and time that is able to use such data in their full power. The state of the forest fire is described by the subprobability densities of the green trees and of the trees on fire that can be estimated thanks to data coming from satellites and earth detectors. The fire dynamics is encoded into a density probability kernel which can take into account wind conditions, land slope, spotting phenomena and so on, bringing to a system of integro-differential equations for the probability densities. Existence and uniqueness of the solutions is proved by using Banach's fixed point theorem. The asymptotic behavior of the model is analyzed as well. Stochastic models based on cellular automata can be considered as particular cases of the present model from which they can be derived by space and/or time discretization. Suggesting a particular structure for the kernel, we obtain numerical simulations of the fire spreading under different conditions. For example, in the case of a forest fire evolving towards a river, the simulations show that the probability density of the trees on fire is different from zero beyond the river due to the spotting phenomenon. Firefighters interventions and weather changes can be easily introduced into the model. ",2023,forrest fires,4.989074,6.0083675,2020-,6,#5E0897
388,Are Forest Fires Predictable?,"Are Forest Fires Predictable?  Dynamic mean field theory is applied to the problem of forest fires. The starting point is the Monte Carlo simulation in a lattice of million cells. The statistics of the clusters is obtained by means of the Hoshen--Kopelman algorithm. We get the map $p_n\to p_{n+1}$, where $p_n$ is the probability of finding a tree in a cell, and $n$ is the discrete time. We demonstrate that the time evolution of $p$ is chaotic. The arguments are provided by the calculation of the bifurcation diagram and the Lyapunov exponent. The bifurcation diagram reveals several windows of stability, including periodic orbits of length three, five and seven. For smaller lattices, the results of the iteration are in qualitative agreement with the statistics of the forest fires in Canada in years 1970--2000. ",2002,forrest fires,5.3638177,4.8190675,2001-2005,2,#FFE135
389,Siberian forest fires: anomalies and trends from satellite data  (2000-2019),"Siberian forest fires: anomalies and trends from satellite data   (2000-2019)  The forest fires characteristics in Siberia detected by satellite data (MODIS instruments on Aqua and Terra platforms) for the period 2000-2019 are analyzed. Regional statistical data and distribution functions of wildfire characteristics are presented. Differences in spatio-temporal changes of forest fires of various intensity were revealed. An analysis of trends of the wildfire characteristics over the past two decades indicates an increase in the proportion of intense forest fires during the fire hazardous seasons, as well as an increase in the intensity of average Siberian wildfire in summer. ",2022,forrest fires,4.528334,7.093381,2020-,6,#5E0897
390,Flood forecasting with machine learning models in an operational  framework,"Flood forecasting with machine learning models in an operational   framework  The operational flood forecasting system by Google was developed to provide accurate real-time flood warnings to agencies and the public, with a focus on riverine floods in large, gauged rivers. It became operational in 2018 and has since expanded geographically. This forecasting system consists of four subsystems: data validation, stage forecasting, inundation modeling, and alert distribution. Machine learning is used for two of the subsystems. Stage forecasting is modeled with the Long Short-Term Memory (LSTM) networks and the Linear models. Flood inundation is computed with the Thresholding and the Manifold models, where the former computes inundation extent and the latter computes both inundation extent and depth. The Manifold model, presented here for the first time, provides a machine-learning alternative to hydraulic modeling of flood inundation. When evaluated on historical data, all models achieve sufficiently high-performance metrics for operational use. The LSTM showed higher skills than the Linear model, while the Thresholding and Manifold models achieved similar performance metrics for modeling inundation extent. During the 2021 monsoon season, the flood warning system was operational in India and Bangladesh, covering flood-prone regions around rivers with a total area of 287,000 km2, home to more than 350M people. More than 100M flood alerts were sent to affected populations, to relevant authorities, and to emergency organizations. Current and future work on the system includes extending coverage to additional flood-prone locations, as well as improving modeling capabilities and accuracy. ",2021,best machine learning model for short-term flood forecasting,4.0636454,7.06618,2020-,6,#5E0897
391,MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak  Inundation Depth And Decoding Influencing Features,"MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak   Inundation Depth And Decoding Influencing Features  Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively. ",2023,best machine learning model for short-term flood forecasting,4.25608,6.8919473,2020-,6,#5E0897
392,Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian  Optimization with Spatiotemporal feature fusion,"Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian   Optimization with Spatiotemporal feature fusion  Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were 0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall input conditions. Additionally, the processing speed was significantly improved, with an inference time of 1.158s (approximately 1/125 of the traditional computation time) compared to the physically-based models. ",2023,best machine learning model for short-term flood forecasting,4.2357583,6.795878,2020-,6,#5E0897
393,Flood Prediction Using Machine Learning Models: Literature Review,"Flood Prediction Using Machine Learning Models: Literature Review  Floods are among the most destructive natural disasters, which are highly complex to model. The research on the advancement of flood prediction models contributed to risk reduction, policy suggestion, minimization of the loss of human life, and reduction the property damage associated with floods. To mimic the complex mathematical expressions of physical processes of floods, during the past two decades, machine learning (ML) methods contributed highly in the advancement of prediction systems providing better performance and cost-effective solutions. Due to the vast benefits and potential of ML, its popularity dramatically increased among hydrologists. Researchers through introducing novel ML methods and hybridizing of the existing ones aim at discovering more accurate and efficient prediction models. The main contribution of this paper is to demonstrate the state of the art of ML models in flood prediction and to give insight into the most suitable models. In this paper, the literature where ML models were benchmarked through a qualitative analysis of robustness, accuracy, effectiveness, and speed are particularly investigated to provide an extensive overview on the various ML algorithms used in the field. The performance comparison of ML models presents an in-depth understanding of the different techniques within the framework of a comprehensive evaluation and discussion. As a result, this paper introduces the most promising prediction methods for both long-term and short-term floods. Furthermore, the major trends in improving the quality of the flood prediction models are investigated. Among them, hybridization, data decomposition, algorithm ensemble, and model optimization are reported as the most effective strategies for the improvement of ML methods. ",2019,best machine learning model for short-term flood forecasting,3.8797252,7.0247035,2016-2020,5,#7402B1
394,Global Flood Prediction: a Multimodal Machine Learning Approach,"Global Flood Prediction: a Multimodal Machine Learning Approach  Flooding is one of the most destructive and costly natural disasters, and climate changes would further increase risks globally. This work presents a novel multimodal machine learning approach for multi-year global flood risk prediction, combining geographical information and historical natural disaster dataset. Our multimodal framework employs state-of-the-art processing techniques to extract embeddings from each data modality, including text-based geographical data and tabular-based time-series data. Experiments demonstrate that a multimodal approach, that is combining text and statistical data, outperforms a single-modality approach. Our most advanced architecture, employing embeddings extracted using transfer learning upon DistilBert model, achieves 75\%-77\% ROCAUC score in predicting the next 1-5 year flooding event in historically flooded locations. This work demonstrates the potentials of using machine learning for long-term planning in natural disaster management. ",2023,best machine learning model for short-term flood forecasting,4.0601277,6.979293,2020-,6,#5E0897
395,"KNN, An Underestimated Model for Regional Rainfall Forecasting","KNN, An Underestimated Model for Regional Rainfall Forecasting  Regional rainfall forecasting is an important issue in hydrology and meteorology. This paper aims to design an integrated tool by applying various machine learning algorithms, especially the state-of-the-art deep learning algorithms including Deep Neural Network, Wide Neural Network, Deep and Wide Neural Network, Reservoir Computing, Long Short Term Memory, Support Vector Machine, K-Nearest Neighbor for forecasting regional precipitations over different catchments in Upstate New York. Through the experimental results and the comparison among machine learning models including classification and regression, we find that KNN is an outstanding model over other models to handle the uncertainty in the precipitation data. The data normalization methods such as ZScore and MinMax are also evaluated and discussed. ",2021,best machine learning model for short-term flood forecasting,4.290997,6.8792467,2020-,6,#5E0897
396,Flood Prediction Using Machine Learning Models,"Flood Prediction Using Machine Learning Models  Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy. ",2022,best machine learning model for short-term flood forecasting,4.1629286,6.9495845,2020-,6,#5E0897
397,"A comparison of machine learning surrogate models of street-scale  flooding in Norfolk, Virginia","A comparison of machine learning surrogate models of street-scale   flooding in Norfolk, Virginia  Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features. ",2023,best machine learning model for short-term flood forecasting,4.078527,6.3830633,2020-,6,#5E0897
398,Machine Learning for Precipitation Nowcasting from Radar Images,"Machine Learning for Precipitation Nowcasting from Radar Images  High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction. ",2019,best machine learning model for short-term flood forecasting,4.10131,7.0356936,2016-2020,5,#7402B1
399,Rapid Flood Inundation Forecast Using Fourier Neural Operator,"Rapid Flood Inundation Forecast Using Fourier Neural Operator  Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting strong generalization skill. ",2023,best machine learning model for short-term flood forecasting,4.2761974,6.9030023,2020-,6,#5E0897
400,Saying Hello World with UML-RSDS - A Solution to the 2011 Instructive  Case,"Saying Hello World with UML-RSDS - A Solution to the 2011 Instructive   Case  In this paper we apply the UML-RSDS notation and tools to the ""Hello World"" case studies and explain the underlying development process for this model transformation approach. ",2011,hello world,2.416996,8.323054,2011-2015,4,#9100CF
401,Saying Hello World with VIATRA2 - A Solution to the TTC 2011 Instructive  Case,Saying Hello World with VIATRA2 - A Solution to the TTC 2011 Instructive   Case  The paper presents a solution of the Hello World! An Instructive Case for the Transformation Tool Contest using the VIATRA2 model transformation tool. ,2011,hello world,2.327388,8.363274,2011-2015,4,#9100CF
402,Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive  Case,Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive   Case  This paper describes the solution of Hello World transformations in MOLA transformation language. Transformations implementing the task are relatively straightforward and easily inferable from the task specification. The required additional steps related to model import and export are also described. ,2011,hello world,2.1882155,8.285063,2011-2015,4,#9100CF
403,Saying Hello World with GrGen.NET - A Solution to the TTC 2011  Instructive Case,Saying Hello World with GrGen.NET - A Solution to the TTC 2011   Instructive Case  We introduce the graph transformation tool GrGen.NET (www.grgen.net) by solving the Hello World Case of the Transformation Tool Contest 2011 which consists of a collection of small transformation tasks; for each task a section is given explaining our implementation. ,2011,hello world,2.4919093,8.39294,2011-2015,4,#9100CF
404,Saying Hello World with Henshin - A Solution to the TTC 2011 Instructive  Case,"Saying Hello World with Henshin - A Solution to the TTC 2011 Instructive   Case  This paper gives an overview of the Henshin solution to the Hello World case study of the Transformation Tool Contest 2011, intended to show basic language concepts and constructs. ",2011,hello world,1.9739178,8.506879,2011-2015,4,#9100CF
405,Saying Hello World with GReTL - A Solution to the TTC 2011 Instructive  Case,Saying Hello World with GReTL - A Solution to the TTC 2011 Instructive   Case  This paper discusses the GReTL solution of the TTC 2011 Hello World case. The submitted solution covers all tasks including the optional ones. ,2011,hello world,4.522969,4.446618,2011-2015,4,#9100CF
406,Saying Hello World with Edapt - A Solution to the TTC 2011 Instructive  Case,Saying Hello World with Edapt - A Solution to the TTC 2011 Instructive   Case  This paper gives an overview of the Edapt solution to the hello world case of the Transformation Tool Contest 2011. ,2011,hello world,1.8014113,8.300376,2011-2015,4,#9100CF
407,Saying Hello World with Epsilon - A Solution to the 2011 Instructive  Case,"Saying Hello World with Epsilon - A Solution to the 2011 Instructive   Case  Epsilon is an extensible platform of integrated and task-specific languages for model management. With solutions to the 2011 TTC Hello World case, this paper demonstrates some of the key features of the Epsilon Object Language (an extension and reworking of OCL), which is at the core of Epsilon. In addition, the paper introduces several of the task-specific languages provided by Epsilon including the Epsilon Generation Language (for model-to-text transformation), the Epsilon Validation Language (for model validation) and Epsilon Flock (for model migration). ",2011,hello world,2.1335168,8.284834,2011-2015,4,#9100CF
408,Saying Hello World with GROOVE - A Solution to the TTC 2011 Instructive  Case,Saying Hello World with GROOVE - A Solution to the TTC 2011 Instructive   Case  This report presents a solution to the Hello World case study of TTC 2011 using GROOVE. We provide and explain the grammar that we used to solve the case study. Every requested question of the case study was solved by a single rule application. ,2011,hello world,2.3805768,5.843505,2011-2015,4,#9100CF
409,The distributed Language Hello White Paper,"The distributed Language Hello White Paper  Hello is a general-purpose, object-oriented, protocol-agnostic distributed programming language. This paper explains the ideas that guided design of Hello. It shows the spirit of Hello using two brief expressive programs and provides a summary of language features. In addition, it explores historical parallels between the binary programming of early computers and the distributed programming of modern networks. ",2014,hello world,2.0099144,8.72685,2011-2015,4,#9100CF
410,On Social and Economic Spheres: An Observation of the 'gantangan'  Indonesian tradition,"On Social and Economic Spheres: An Observation of the 'gantangan'   Indonesian tradition  Indonesian traditional villagers have a tradition for the sake of their own social and economic security named 'nyumbang'. There are wide variations of the traditions across the archipelago, and we revisit an observation to one in Subang, West Java, Indonesia. The paper discusses and employs the evolutionary game theoretic insights to see the process of 'gantangan', of the intertwining social cohesion and economic expectation of the participation within the traditional activities. The current development of the gantangan tradition is approached and generalized to propose a view between the economic and social sphere surrounding modern people. While some explanations due to the current development of gantangan is drawn, some aspects related to traditional views complying the modern life with social and economic expectations is outlined. ",2015,siddha ganju,3.1376169,6.738044,2016-2020,5,#7402B1
411,GANs for Medical Image Synthesis: An Empirical Study,"GANs for Medical Image Synthesis: An Empirical Study  Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images.   Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of a medical datasets. ",2021,siddha ganju,3.8594186,9.047993,2020-,6,#5E0897
412,Applications of Generative Adversarial Networks in Neuroimaging and  Clinical Neuroscience,"Applications of Generative Adversarial Networks in Neuroimaging and   Clinical Neuroscience  Generative adversarial networks (GANs) are one powerful type of deep learning models that have been successfully utilized in numerous fields. They belong to a broader family called generative methods, which generate new data with a probabilistic model by learning sample distribution from real examples. In the clinical context, GANs have shown enhanced capabilities in capturing spatially complex, nonlinear, and potentially subtle disease effects compared to traditional generative methods. This review appraises the existing literature on the applications of GANs in imaging studies of various neurological conditions, including Alzheimer's disease, brain tumors, brain aging, and multiple sclerosis. We provide an intuitive explanation of various GAN methods for each application and further discuss the main challenges, open questions, and promising future directions of leveraging GANs in neuroimaging. We aim to bridge the gap between advanced deep learning methods and neurology research by highlighting how GANs can be leveraged to support clinical decision making and contribute to a better understanding of the structural and functional patterns of brain diseases. ",2022,siddha ganju,3.7246232,9.0734215,2020-,6,#5E0897
413,Generative Adversarial Networks: A Survey Towards Private and Secure  Applications,"Generative Adversarial Networks: A Survey Towards Private and Secure   Applications  Generative Adversarial Networks (GAN) have promoted a variety of applications in computer vision, natural language processing, etc. due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey paper to summarize those state-of-the-art works systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey paper conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this paper also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions. ",2021,siddha ganju,3.76131,9.200277,2020-,6,#5E0897
414,A review of Generative Adversarial Networks (GANs) and its applications  in a wide variety of disciplines -- From Medical to Remote Sensing,"A review of Generative Adversarial Networks (GANs) and its applications   in a wide variety of disciplines -- From Medical to Remote Sensing  We look into Generative Adversarial Network (GAN), its prevalent variants and applications in a number of sectors. GANs combine two neural networks that compete against one another using zero-sum game theory, allowing them to create much crisper and discrete outputs. GANs can be used to perform image processing, video generation and prediction, among other computer vision applications. GANs can also be utilised for a variety of science-related activities, including protein engineering, astronomical data processing, remote sensing image dehazing, and crystal structure synthesis. Other notable fields where GANs have made gains include finance, marketing, fashion design, sports, and music. Therefore in this article we provide a comprehensive overview of the applications of GANs in a wide variety of disciplines. We first cover the theory supporting GAN, GAN variants, and the metrics to evaluate GANs. Then we present how GAN and its variants can be applied in twelve domains, ranging from STEM fields, such as astronomy and biology, to business fields, such as marketing and finance, and to arts, such as music. As a result, researchers from other fields may grasp how GANs work and apply them to their own study. To the best of our knowledge, this article provides the most comprehensive survey of GAN's applications in different fields. ",2021,siddha ganju,3.7439685,9.162317,2020-,6,#5E0897
415,A Novel Measure to Evaluate Generative Adversarial Networks Based on  Direct Analysis of Generated Images,"A Novel Measure to Evaluate Generative Adversarial Networks Based on   Direct Analysis of Generated Images  The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance, e.g., Inception Score (IS) and statistical metrics, e.g., Fr\'echet Inception Distance (FID). Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, we have designed the Likeness Score (LS) to evaluate GAN performance, and have applied it to evaluate several typical GANs. We compared our proposed measure with two commonly used GAN evaluation methods: IS and FID, and four additional measures. Furthermore, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance. ",2020,siddha ganju,3.735894,9.084473,2020-,6,#5E0897
416,PPGAN: Privacy-preserving Generative Adversarial Network,"PPGAN: Privacy-preserving Generative Adversarial Network  Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget. ",2019,siddha ganju,3.7332711,9.125138,2016-2020,5,#7402B1
417,FA-GANs: Facial Attractiveness Enhancement with Generative Adversarial  Networks on Frontal Faces,"FA-GANs: Facial Attractiveness Enhancement with Generative Adversarial   Networks on Frontal Faces  Facial attractiveness enhancement has been an interesting application in Computer Vision and Graphics over these years. It aims to generate a more attractive face via manipulations on image and geometry structure while preserving face identity. In this paper, we propose the first Generative Adversarial Networks (GANs) for enhancing facial attractiveness in both geometry and appearance aspects, which we call ""FA-GANs"". FA-GANs contain two branches and enhance facial attractiveness in two perspectives: facial geometry and facial appearance. Each branch consists of individual GANs with the appearance branch adjusting the facial image and the geometry branch adjusting the facial landmarks in appearance and geometry aspects, respectively. Unlike the traditional facial manipulations learning from paired faces, which are infeasible to collect before and after enhancement of the same individual, we achieve this by learning the features of attractiveness faces through unsupervised adversarial learning. The proposed FA-GANs are able to extract attractiveness features and impose them on the enhancement results. To better enhance faces, both the geometry and appearance networks are considered to refine the facial attractiveness by adjusting the geometry layout of faces and the appearance of faces independently. To the best of our knowledge, we are the first to enhance the facial attractiveness with GANs in both geometry and appearance aspects. The experimental results suggest that our FA-GANs can generate compelling perceptual results in both geometry structure and facial appearance and outperform current state-of-the-art methods. ",2020,siddha ganju,3.6555576,9.095932,2020-,6,#5E0897
418,Differentially Private Generative Adversarial Network,"Differentially Private Generative Adversarial Network  Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level. ",2018,siddha ganju,3.7098722,9.0866,2016-2020,5,#7402B1
419,"Detection, Attribution and Localization of GAN Generated Images","Detection, Attribution and Localization of GAN Generated Images  Recent advances in Generative Adversarial Networks (GANs) have led to the creation of realistic-looking digital images that pose a major challenge to their detection by humans or computers. GANs are used in a wide range of tasks, from modifying small attributes of an image (StarGAN [14]), transferring attributes between image pairs (CycleGAN [91]), as well as generating entirely new images (ProGAN [36], StyleGAN [37], SPADE/GauGAN [64]). In this paper, we propose a novel approach to detect, attribute and localize GAN generated images that combines image features with deep learning methods. For every image, co-occurrence matrices are computed on neighborhood pixels of RGB channels in different directions (horizontal, vertical and diagonal). A deep learning network is then trained on these features to detect, attribute and localize these GAN generated/manipulated images. A large scale evaluation of our approach on 5 GAN datasets comprising over 2.76 million images (ProGAN, StarGAN, CycleGAN, StyleGAN and SPADE/GauGAN) shows promising results in detecting GAN generated images. ",2020,siddha ganju,3.7292,9.088258,2020-,6,#5E0897
420,Bust-a-Move/Puzzle Bobble is NP-Complete,"Bust-a-Move/Puzzle Bobble is NP-Complete  We prove that the classic 1994 Taito video game, known as Puzzle Bobble or Bust-a-Move, is NP-complete. Our proof applies to the perfect-information version where the bubble sequence is known in advance, and it uses just three bubble colors. ",2015,,2.6868045,5.420489,2016-2020,5,#7402B1
421,Candy Crush is NP-hard,Candy Crush is NP-hard  We prove that playing Candy Crush to achieve a given score in a fixed number of swaps is NP-hard. ,2014,,2.5067084,5.966925,2011-2015,4,#9100CF
422,SERRANT: a syntactic classifier for English Grammatical Error Types,SERRANT: a syntactic classifier for English Grammatical Error Types  SERRANT is a system and code for automatic classification of English grammatical errors that combines SErCl and ERRANT. SERRANT uses ERRANT's annotations when they are informative and those provided by SErCl otherwise. ,2021,,2.1607502,8.605667,2020-,6,#5E0897
423,Explaining Cybersecurity with Films and the Arts (Extended Abstract),Explaining Cybersecurity with Films and the Arts (Extended Abstract)  Explaining Cybersecurity with Films and the Arts ,2019,,1.3267926,7.2353396,2016-2020,5,#7402B1
424,A family of lacunary recurrences for Lucas Numbers,A family of lacunary recurrences for Lucas Numbers  We prove an infinite family of lacunary recurrences for the Lucas numbers using combinatorial means. ,2020,,4.1251884,2.0996838,2020-,6,#5E0897
425,The Chow groups and the motive of the Hilbert scheme of points on a  surface,The Chow groups and the motive of the Hilbert scheme of points on a   surface  We compute the Chow motive and the Chow groups with rational coefficients of the Hilbert scheme of points on a smooth algebraic surface ,2000,,4.0664115,1.488718,1996-2000,1,#B2AB10
426,Rejoinder: The Future of Indirect Evidence,"Rejoinder: The Future of Indirect Evidence  Rejoinder to ""The Future of Indirect Evidence"" [arXiv:1012.1161] ",2010,,5.786859,3.6085293,2006-2010,3,#FEF65C
427,Much ado about Zero,Much ado about Zero  A brief historical introduction for the enigmatic number Zero is given. The discussions are for popular consumption. ,2016,,5.510999,3.1770468,2016-2020,5,#7402B1
428,A brief introduction to amenable equivalence relations,A brief introduction to amenable equivalence relations  This is a short survey of amenable equivalence relations. ,2018,,4.722834,1.6479621,2016-2020,5,#7402B1
429,Inversion of adjunction on log canonicity,Inversion of adjunction on log canonicity  We prove inversion of adjunction on log canonicity. ,2005,,2.7265048,4.258007,2006-2010,3,#FEF65C
430,IceTop Status in 2004,"IceTop Status in 2004  IceTop is the surface component of IceCube neutrino telescope. Goals, plans and status of IceTop in 2004 are reported ",2005,status of ice caps,11.266032,6.6251545,2006-2010,3,#FEF65C
431,Status of IceCube in 2005,"Status of IceCube in 2005  IceCube is a kilometer scale neutrino observatory now in construction at the South Pole. The construction started in January 2005 with the deployment of 76 sensors on the first string and four surface detector stations. Nine strings and 32 surface detectors are in operation since February 2006. The data based on calibration measurements, muons and artificial light flashes are consistent with performance expectations. This report focuses on design, construction experience and first data from the sensors deployed in January 2005. ",2006,status of ice caps,11.051494,6.621423,2006-2010,3,#FEF65C
432,Acoustic detection of high energy neutrinos in ice: Status and results  from the South Pole Acoustic Test Setup,Acoustic detection of high energy neutrinos in ice: Status and results   from the South Pole Acoustic Test Setup  The feasibility and specific design of an acoustic neutrino detection array at the South Pole depend on the acoustic properties of the ice. The South Pole Acoustic Test Setup (SPATS) has been built to evaluate the acoustic characteristics of the ice in the 1 to 100 kHz frequency range. The most recent results of SPATS are presented. ,2009,status of ice caps,11.027573,6.501474,2006-2010,3,#FEF65C
433,Results of a Collective Awareness Platforms Investigation,"Results of a Collective Awareness Platforms Investigation  In this paper we provide two introductory analyses of CAPs, based exclusively on the analysis of documents found on the Internet. The first analysis allowed us to investigate the world of CAPs, in particular for what concerned their status (dead or alive), the scope of those platforms and the typology of users. In order to develop a more accurate model of CAPs, and to understand more deeply the motivation of the users and the type of expected payoff, we analysed those CAPs from the above list that are still alive and we used two models developed for what concerned the virtual community and the collective intelligence. ",2016,status of ice caps,2.4214463,8.180355,2016-2020,5,#7402B1
434,Freezing a rivulet,"Freezing a rivulet  We investigate experimentally the formation of the particular ice structure obtained when a capillary trickle of water flows on a cold substrate. We show that after a few minutes the water ends up flowing on a tiny ice wall whose shape is permanent. We characterize and understand quantitatively the formation dynamics and the final thickness of this ice structure. In particular, we identify two growth regimes. First, a 1D solidification diffusive regime, where ice is building independently of the flowing water. And second, once the ice is thick enough, the heat flux in the water comes into play, breaking the 1D symmetry of the problem, and the ice ends up thickening linearly downward. This linear pattern is explained by considering the confinement of the thermal boundary layer in the water by the free surface. ",2019,status of ice caps,7.0849953,5.7460175,2016-2020,5,#7402B1
435,The IceCube Project,"The IceCube Project  This talk gives a brief description of goals, expected performance and status of the Icecube project. ",2004,status of ice caps,3.5660236,7.217607,2001-2005,2,#FFE135
436,Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS  Imagery,"Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS   Imagery  Depleting lake ice is a climate change indicator, just like sea-level rise or glacial retreat. Monitoring Lake Ice Phenology (LIP) is useful because long-term freezing and thawing patterns serve as sentinels to understand regional and global climate change. We report a study for the Oberengadin region of Switzerland, where several small- and medium-sized mountain lakes are located. We observe the LIP events, such as freeze-up, break-up and ice cover duration, across two decades (2000-2020) from optical satellite images. We analyse the time series of MODIS imagery by estimating spatially resolved maps of lake ice for these Alpine lakes with supervised machine learning. To train the classifier we rely on reference data annotated manually based on webcam images. From the ice maps, we derive long-term LIP trends. Since the webcam data are only available for two winters, we cross-check our results against the operational MODIS and VIIRS snow products. We find a change in complete freeze duration of -0.76 and -0.89 days per annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe plausible correlations of the LIP trends with climate data measured at nearby meteorological stations. We notice that mean winter air temperature has a negative correlation with the freeze duration and break-up events and a positive correlation with the freeze-up events. Additionally, we observe a strong negative correlation of sunshine during the winter months with the freeze duration and break-up events. ",2021,status of ice caps,5.1097045,6.8424993,2020-,6,#5E0897
437,Ice Rule Fragility via Topological Charge Transfer in Artificial  Colloidal Ice,"Ice Rule Fragility via Topological Charge Transfer in Artificial   Colloidal Ice  Artificial particle ices are model systems of constrained, interacting particles. They have been introduced theoretically to study ice-manifolds emergent from frustration, along with domain wall and grain boundary dynamics, doping, pinning-depinning, controlled transport of topological defects, avalanches, and memory effects. Recently such particle-based ices have been experimentally realized with vortices in nano-patterned superconductors or gravitationally trapped colloids. Here we demonstrate that, although these ices are generally considered equivalent to magnetic spin ices, they can access a novel spectrum of phenomenologies that are inaccessible to the latter. With experiments, theory and simulations we demonstrate that in mixed coordination geometries, entropy-driven negative monopoles spontaneously appear at a density determined by the vertex-mixture ratio. Unlike its spin-based analogue, the colloidal system displays a ""fragile ice"" manifold, where local energetics oppose the ice rule, which is instead enforced through conservation of the global topological charge. The fragile colloidal ice, stabilized by topology, can be spontaneously broken by topological charge transfer. ",2018,status of ice caps,8.836914,4.088083,2016-2020,5,#7402B1
438,Ice XV: a new thermodynamically stable phase of ice,"Ice XV: a new thermodynamically stable phase of ice  A new phase of ice, named ice XV, has been identified and its structure determined by neutron diffraction. Ice XV is the hydrogen-ordered counterpart of ice VI and is thermodynamically stable at temperatures below ~130 K in the 0.8 to 1.5 GPa pressure range. The regions of stability in the medium pressure range of the phase diagram have thus been finally mapped, with only hydrogen-ordered phases stable at 0 K. The ordered ice XV structure is antiferroelectric, in clear disagreement with recent theoretical calculations predicting ferroelectric ordering. ",2009,status of ice caps,7.4761834,5.508566,2006-2010,3,#FEF65C
439,Skin supersolidity slipperizing ice,"Skin supersolidity slipperizing ice  Consistency between theory predictions and measurements and calculations revealed that the skin of ice, containing water molecules with fewer than four neighbours, forms a supersolid phase that is highly polarized, elastic, hydrophobic, with ultra-low density and high thermal stability. The supersolidity of skin sliperizes ice. ",2013,status of ice caps,7.106553,5.6655126,2011-2015,4,#9100CF
